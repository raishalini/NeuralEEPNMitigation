{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ded0be04-378e-4991-825b-d5769e7b9deb",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9aaffd-07a9-4fc6-96ff-00dab6fa6d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape of received symbols: (100, 1, 10000)\n",
      "Final shape of sent symbols: (100, 1, 11030)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "try:\n",
    "    import sionna\n",
    "except ImportError as e:\n",
    "    import os\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna\n",
    "\n",
    "from sionna.channel import utils\n",
    "from comsys import Transmitter, Receiver, Channel\n",
    "from phasenoise import phase_noise\n",
    "from plots import SNRVsLinewidthPlotter, PlotInputOutput, PlotNN, SNRVsLinewidthPlotterNN\n",
    "# from neural_network import MyModel\n",
    "from neural_network_new import MyModel\n",
    "\n",
    "class Transceiver:\n",
    "    def __init__(self, gpu_num=0):\n",
    "        self.set_gpu(gpu_num)\n",
    "        self.dtype = tf.complex64\n",
    "\n",
    "        # Parameters\n",
    "        self.beta = 0.1\n",
    "        self.span_in_symbols = 128\n",
    "        self.samples_per_symbol = 10\n",
    "        self.beta_2 = -21.67\n",
    "        self.t_norm = 1e-12\n",
    "        self.z_norm = 1e3\n",
    "        self.linewidth = 200e3\n",
    "        self.f_c = 193.55e12\n",
    "        self.length_sp = 4000.0\n",
    "        self.alpha = 0.046\n",
    "        self.num_bits_per_symbol = 4\n",
    "        self.batch_size = 1\n",
    "        self.num_symbols = 11030\n",
    "\n",
    "        mem = 501\n",
    "        if not mem % 2:\n",
    "            warnings.warn(\"Even number of filter taps for moving average. Expanding by 1.\")\n",
    "            mem = mem + (1 - mem % 2)\n",
    "        self.filter = tf.ones((mem, 1, 1, 1), dtype=tf.as_dtype(self.dtype).real_dtype)\n",
    "        self.mem_cut = (mem // 2) * 2\n",
    "        if not self.mem_cut:\n",
    "            self.mem_cut = None        \n",
    "\n",
    "        self.transmitter = Transmitter(self.num_bits_per_symbol, self.batch_size, self.num_symbols, self.samples_per_symbol, self.beta, self.span_in_symbols)\n",
    "        self.channel = Channel(self.alpha, self.beta_2, self.f_c, self.length_sp, self.t_norm, self.dtype)\n",
    "        self.receiver = Receiver(self.linewidth, self.t_norm, self.samples_per_symbol, self.transmitter.rcf)\n",
    "        \n",
    "        # Initialize the neural network model\n",
    "        self.nn_equalise = MyModel()\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    def set_gpu(self, gpu_num):\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "        tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "    def cal_EVM(self, x, y_cpr):\n",
    "        mse = tf.reduce_mean(tf.math.square(tf.abs(x[..., self.receiver.gardner.avglenhalf + self.receiver.gardner.flenhalf + 500: -(self.receiver.gardner.avglenhalf + \n",
    "                                                                                    self.receiver.gardner.flenhalf) ]-y_cpr)))\n",
    "        return 10*tf.math.log(mse)/tf.math.log(10.0)\n",
    "\n",
    "    def calculate_snr(self, x, y_cpr, y_cpr_wo_tr):\n",
    "        signal_power = tf.reduce_mean(tf.math.square(tf.abs(x[..., self.receiver.gardner.avglenhalf + self.receiver.gardner.flenhalf + 500: -(self.receiver.gardner.avglenhalf + \n",
    "                                                                                    self.receiver.gardner.flenhalf) ])))\n",
    "        noise_power = tf.reduce_mean(tf.math.square(tf.abs(x[..., self.receiver.gardner.avglenhalf + self.receiver.gardner.flenhalf + 500: -(self.receiver.gardner.avglenhalf + \n",
    "                                                                                    self.receiver.gardner.flenhalf) ]-y_cpr)))\n",
    "        snr = 10 * tf.math.log(signal_power / noise_power) / tf.math.log(10.0)\n",
    "            \n",
    "        signal_power_wo_tr = tf.reduce_mean(tf.math.square(tf.abs(x)))\n",
    "        noise_power_wo_tr = tf.reduce_mean(tf.math.square(tf.abs(x[..., self.mem_cut:] - y_cpr_wo_tr))) \n",
    "\n",
    "        snr_wo_tr = 10 * tf.math.log(signal_power_wo_tr / noise_power_wo_tr) / tf.math.log(10.0)\n",
    "        \n",
    "        return snr, snr_wo_tr\n",
    "\n",
    "    def evaluate_snr(self, linewidths, link_distances, its=100):\n",
    "        results = {'with_tr': {}, 'without_tr': {}}\n",
    "        for length in link_distances:\n",
    "            self.channel.length_sp = length\n",
    "            self.channel = Channel(self.alpha, self.beta_2, self.f_c, length, self.t_norm, self.dtype)\n",
    "            assert abs(float(self.channel.ss_fn._length) - float(length)) < 1e-3\n",
    "            snr_values_with_tr = []\n",
    "            snr_values_without_tr = []\n",
    "            for lw in linewidths:\n",
    "                self.receiver.linewidth = lw\n",
    "                sent_symbols = tf.TensorArray(dtype=tf.complex64, size=its, dynamic_size=False)\n",
    "                received_symbols_with_tr = tf.TensorArray(dtype=tf.complex64, size=its, dynamic_size=False)\n",
    "                received_symbols_wo_tr = tf.TensorArray(dtype=tf.complex64, size=its, dynamic_size=False)\n",
    "                for i in range(its):\n",
    "                    y_cpr_wo_tr, y_cpr, sent, received = self.run()\n",
    "                    sent_symbols = sent_symbols.write(i, sent)\n",
    "                    received_symbols_with_tr = received_symbols_with_tr.write(i, received)\n",
    "                    received_symbols_wo_tr = received_symbols_wo_tr.write(i, y_cpr_wo_tr)\n",
    "                sent_symbols = sent_symbols.stack()\n",
    "                received_symbols_with_tr = received_symbols_with_tr.stack()\n",
    "                received_symbols_wo_tr = received_symbols_wo_tr.stack()\n",
    "                snr_with_tr, snr_without_tr = self.calculate_snr(sent_symbols, received_symbols_with_tr, received_symbols_wo_tr)\n",
    "                snr_values_with_tr.append(snr_with_tr.numpy())\n",
    "                snr_values_without_tr.append(snr_without_tr.numpy())\n",
    "            results['with_tr'][length] = snr_values_with_tr\n",
    "            results['without_tr'][length] = snr_values_without_tr\n",
    "        return results\n",
    "\n",
    "    def run(self):\n",
    "        x = self.transmitter.generate_qam_symbols()\n",
    "\n",
    "        x_us = self.transmitter.upsample(x)\n",
    "\n",
    "        x_rcf = self.transmitter.apply_rcf(x_us)\n",
    "\n",
    "        x_rcf_padded, padding_left, padding_right = self.transmitter.pad_signal(x_rcf)\n",
    "\n",
    "        y = self.channel.transmit(x_rcf_padded)\n",
    "        \n",
    "        y_pn = self.receiver.add_phase_noise(y, phase_noise())\n",
    "\n",
    "        y_mf = self.receiver.matched_filter(y_pn)\n",
    "\n",
    "        y_cdc = self.channel.compensate_dispersion(y_mf)\n",
    "\n",
    "        y_lpf = self.receiver.low_pass_filter(y_cdc)\n",
    "        \n",
    "        y_ds = self.receiver.downsample(y_cdc, padding_left, padding_right)\n",
    "\n",
    "        y_normalised = self.receiver.normalize(y_ds)\n",
    "\n",
    "        y_tr, timing_errors = self.receiver.timing_recovery(y_normalised)\n",
    "\n",
    "        y_cpr = self.receiver.cpr(y_tr, x[..., self.receiver.gardner.avglenhalf + self.receiver.gardner.flenhalf: -(self.receiver.gardner.avglenhalf + self.receiver.gardner.flenhalf)], self.dtype, self.mem_cut, self.filter)\n",
    "\n",
    "        y_cpr_wo_tr = self.receiver.cpr(y_normalised, x, self.dtype, self.mem_cut, self.filter)\n",
    "        \n",
    "        y_cpr_normalised = self.receiver.normalize(y_cpr)\n",
    "        \n",
    "        return y_cpr_wo_tr, y_cpr, x, y_cpr_normalised\n",
    "        \n",
    "    def cal_mse(self, x, y):\n",
    "        mse = tf.reduce_mean(tf.math.square(tf.abs(x - y)))\n",
    "        signal_power = tf.reduce_mean(tf.math.square(tf.abs(x)))\n",
    "        snr = 10 * tf.math.log(signal_power / mse) / tf.math.log(10.0)\n",
    "        \n",
    "        return mse, snr\n",
    "\n",
    "    def train_and_test(self, iterations, linewidths, link_distances):\n",
    "        # Initialize result storage\n",
    "        test_results = {lw: {length: {\"mse\": [], \"snr\": []} for length in link_distances} for lw in linewidths}\n",
    "    \n",
    "        for idx, lw in enumerate(linewidths):\n",
    "            print(f\"\\nTraining with linewidth: {lw} Hz\")\n",
    "    \n",
    "            # Load weights if not the first linewidth\n",
    "            if idx > 0:\n",
    "                self.nn_equalise.load_weights(\"final_model_weights.h5\")\n",
    "                print(f\"Loaded model weights for continued training.\")\n",
    "            \n",
    "            for length in link_distances:\n",
    "                # Update the link distance in the channel\n",
    "                self.channel.length_sp = length\n",
    "                print(f\"\\nTraining with linewidth: {lw} Hz and Distance: {length} km\")\n",
    "                \n",
    "                # Training on the current linewidth and link distance\n",
    "                for i in range(iterations):\n",
    "                    y_cpr_wo_tr, y_cpr, tx_symbols, rx_symbols = self.run()\n",
    "                    tx_symbols_short = tx_symbols[..., self.receiver.gardner.avglenhalf + self.receiver.gardner.flenhalf + 500: -(self.receiver.gardner.avglenhalf + \n",
    "                                                                                                        self.receiver.gardner.flenhalf)]\n",
    "                    x_train = rx_symbols\n",
    "                    y_train = tx_symbols_short\n",
    "        \n",
    "                    with tf.GradientTape() as tape:\n",
    "                        network_out = self.nn_equalise(x_train, training=True)\n",
    "                        network_out = self.receiver.normalize(network_out)\n",
    "                        loss, _ = self.cal_mse(y_train, network_out)\n",
    "        \n",
    "                    gradients = tape.gradient(loss, self.nn_equalise.trainable_variables)\n",
    "                    self.optimizer.apply_gradients(zip(gradients, self.nn_equalise.trainable_variables))\n",
    "                    print(f\"Iteration {i+1}/{iterations}, Loss: {loss.numpy()}\")\n",
    "    \n",
    "                # Save the model weights after training and testing\n",
    "                self.nn_equalise.save_weights(\"final_model_weights.h5\")\n",
    "                print(f\"Model weights saved after training and testing with linewidth {lw} Hz.\\n\")\n",
    "        \n",
    "                # Test the model and store results for the current linewidth and link distance\n",
    "                print(f\"\\nTesting with linewidth: {lw} Hz and Distance: {length} km\")\n",
    "                original_mse, original_snr, nn_mse, nn_snr, tx_symbols_arr, rx_symbols_arr, rx_symbols_nn_arr = self.test(self.nn_equalise, num_symbols=100)\n",
    "                print(f\"Testing MSE - Linewidth: {lw}, Link Distance: {length}, Original: {original_mse.numpy()}, Neural Network: {nn_mse.numpy()}\")\n",
    "                    \n",
    "                # Store results for the current linewidth and link distance\n",
    "                test_results[lw][length][\"mse\"].append({\"original_mse\": original_mse.numpy(), \"nn_mse\": nn_mse.numpy()})\n",
    "                test_results[lw][length][\"snr\"].append({\"original_snr\": original_snr.numpy(), \"nn_snr\": nn_snr.numpy()})\n",
    "    \n",
    "        return test_results\n",
    "\n",
    "    \n",
    "    def test(self, model, num_symbols):\n",
    "        tx_symbols_arr = tf.TensorArray(dtype=tf.complex64, size=num_symbols)\n",
    "        rx_symbols_arr = tf.TensorArray(dtype=tf.complex64, size=num_symbols)\n",
    "        rx_symbols_nn_arr = tf.TensorArray(dtype=tf.complex64, size=num_symbols)\n",
    "        for i in range(num_symbols):\n",
    "            y_cpr_wo_tr, y_cpr, tx_symbols, rx_symbols = self.run()\n",
    "            tx_symbols_short = tx_symbols[..., self.receiver.gardner.avglenhalf + self.receiver.gardner.flenhalf + 500: -(self.receiver.gardner.avglenhalf + \n",
    "                                                                                    self.receiver.gardner.flenhalf) ]\n",
    "            network_out = model.predict(rx_symbols)\n",
    "            network_out = self.receiver.normalize(network_out)\n",
    "            # print(\"network_out : \", network_out)\n",
    "            tx_symbols_arr = tx_symbols_arr.write(i, tx_symbols_short)\n",
    "            rx_symbols_arr = rx_symbols_arr.write(i, rx_symbols)\n",
    "            rx_symbols_nn_arr = rx_symbols_nn_arr.write(i, network_out)\n",
    "\n",
    "        tx_symbols_arr = tx_symbols_arr.stack()\n",
    "        # print(\"tx_symbols_arr : \", tx_symbols_arr)\n",
    "        rx_symbols_arr = rx_symbols_arr.stack()\n",
    "        # print(\"rx_symbols_arr : \", rx_symbols_arr)\n",
    "        rx_symbols_nn_arr = rx_symbols_nn_arr.stack()\n",
    "        # print(\"rx_symbols_nn_arr : \", rx_symbols_nn_arr)\n",
    "        org_mse, org_snr = self.cal_mse(tf.reshape(tx_symbols_arr, [-1]), tf.reshape(rx_symbols_arr, [-1]))\n",
    "        nn_mse, nn_snr = self.cal_mse(tf.reshape(tx_symbols_arr, [-1]), tf.reshape(rx_symbols_nn_arr, [-1]))\n",
    "        return org_mse, org_snr, nn_mse, nn_snr, tx_symbols_arr, rx_symbols_arr, rx_symbols_nn_arr\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    pipeline = Transceiver()\n",
    "    its = 100\n",
    "    sent_symbols = tf.TensorArray(dtype=tf.complex64, size=its, dynamic_size=False)\n",
    "    received_symbols = tf.TensorArray(dtype=tf.complex64, size=its, dynamic_size=False)\n",
    "\n",
    "    for i in range(its):\n",
    "        y_cpr_wo_tr, y_cpr, sent, received = pipeline.run()\n",
    "        sent_symbols = sent_symbols.write(i, sent)\n",
    "        received_symbols = received_symbols.write(i, received)\n",
    "\n",
    "    sent_symbols = sent_symbols.stack()\n",
    "    received_symbols = received_symbols.stack()\n",
    "\n",
    "    print(\"Final shape of received symbols:\", received_symbols.shape)\n",
    "    print(\"Final shape of sent symbols:\", sent_symbols.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d8d262-35cc-4c72-93e4-fb9682ab45d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Train\n",
      "\n",
      "Training with linewidth: 100000.0 Hz\n",
      "\n",
      "Training with linewidth: 100000.0 Hz and Distance: 1000.0 km\n",
      "Iteration 1/2000, Loss: 0.0004390437970869243\n",
      "Iteration 2/2000, Loss: 0.0002537276304792613\n",
      "Iteration 3/2000, Loss: 0.00027076929109171033\n",
      "Iteration 4/2000, Loss: 0.0002160911390092224\n",
      "Iteration 5/2000, Loss: 0.0002765471872407943\n",
      "Iteration 6/2000, Loss: 0.0002891708572860807\n",
      "Iteration 7/2000, Loss: 0.00022931273269932717\n",
      "Iteration 8/2000, Loss: 0.0001633406209293753\n",
      "Iteration 9/2000, Loss: 0.00018091502715833485\n",
      "Iteration 10/2000, Loss: 0.00017868807481136173\n",
      "Iteration 11/2000, Loss: 0.00016352348029613495\n",
      "Iteration 12/2000, Loss: 0.00023644856992177665\n",
      "Iteration 13/2000, Loss: 0.00017571345961187035\n",
      "Iteration 14/2000, Loss: 0.0002465861034579575\n",
      "Iteration 15/2000, Loss: 0.00018677716434467584\n",
      "Iteration 16/2000, Loss: 0.00020323078206274658\n",
      "Iteration 17/2000, Loss: 0.00016260436677839607\n",
      "Iteration 18/2000, Loss: 0.00015705097757745534\n",
      "Iteration 19/2000, Loss: 0.0004903390654362738\n",
      "Iteration 20/2000, Loss: 0.00022493896540254354\n",
      "Iteration 21/2000, Loss: 0.00019574124598875642\n",
      "Iteration 22/2000, Loss: 0.00021863507572561502\n",
      "Iteration 23/2000, Loss: 0.0002826988638844341\n",
      "Iteration 24/2000, Loss: 0.00017679843585938215\n",
      "Iteration 25/2000, Loss: 0.00023862248053774238\n",
      "Iteration 26/2000, Loss: 0.00043630407890304923\n",
      "Iteration 27/2000, Loss: 0.0002633676049299538\n",
      "Iteration 28/2000, Loss: 0.00017810626013670117\n",
      "Iteration 29/2000, Loss: 0.00016247964231297374\n",
      "Iteration 30/2000, Loss: 0.0001549365697428584\n",
      "Iteration 31/2000, Loss: 0.0002741412026807666\n",
      "Iteration 32/2000, Loss: 0.0003154669830109924\n",
      "Iteration 33/2000, Loss: 0.00022305425954982638\n",
      "Iteration 34/2000, Loss: 0.0002587286289781332\n",
      "Iteration 35/2000, Loss: 0.00017303336062468588\n",
      "Iteration 36/2000, Loss: 0.0003158756298944354\n",
      "Iteration 37/2000, Loss: 0.00015027752669993788\n",
      "Iteration 38/2000, Loss: 0.0003052889369428158\n",
      "Iteration 39/2000, Loss: 0.000249392818659544\n",
      "Iteration 40/2000, Loss: 0.00020219289581291378\n",
      "Iteration 41/2000, Loss: 0.00022711374913342297\n",
      "Iteration 42/2000, Loss: 0.00010304548777639866\n",
      "Iteration 43/2000, Loss: 0.0001414837024640292\n",
      "Iteration 44/2000, Loss: 0.0001821596670197323\n",
      "Iteration 45/2000, Loss: 0.00017570990894455463\n",
      "Iteration 46/2000, Loss: 0.00016524385137017816\n",
      "Iteration 47/2000, Loss: 0.0002871197066269815\n",
      "Iteration 48/2000, Loss: 0.00021823871065862477\n",
      "Iteration 49/2000, Loss: 0.00018577396986074746\n",
      "Iteration 50/2000, Loss: 0.00021394739451352507\n",
      "Iteration 51/2000, Loss: 0.00024985207710415125\n",
      "Iteration 52/2000, Loss: 0.00015432407963089645\n",
      "Iteration 53/2000, Loss: 0.00016298439004458487\n",
      "Iteration 54/2000, Loss: 0.00016055168816819787\n",
      "Iteration 55/2000, Loss: 0.0003067746583838016\n",
      "Iteration 56/2000, Loss: 0.00029175024246796966\n",
      "Iteration 57/2000, Loss: 0.00019300503481645137\n",
      "Iteration 58/2000, Loss: 0.00017558679974172264\n",
      "Iteration 59/2000, Loss: 0.00033214883296750486\n",
      "Iteration 60/2000, Loss: 0.0007499334751628339\n",
      "Iteration 61/2000, Loss: 0.0002390177542110905\n",
      "Iteration 62/2000, Loss: 0.0003683467803057283\n",
      "Iteration 63/2000, Loss: 0.00019642265397123992\n",
      "Iteration 64/2000, Loss: 0.0002800443908199668\n",
      "Iteration 65/2000, Loss: 0.00026211387012153864\n",
      "Iteration 66/2000, Loss: 0.00018582100165076554\n",
      "Iteration 67/2000, Loss: 0.0002737326140049845\n",
      "Iteration 68/2000, Loss: 0.00014110842312220484\n",
      "Iteration 69/2000, Loss: 0.0003430176875554025\n",
      "Iteration 70/2000, Loss: 0.0002120418066624552\n",
      "Iteration 71/2000, Loss: 0.0001530545123387128\n",
      "Iteration 72/2000, Loss: 0.00023481411335524172\n",
      "Iteration 73/2000, Loss: 0.0001990889140870422\n",
      "Iteration 74/2000, Loss: 0.0002708630927372724\n",
      "Iteration 75/2000, Loss: 0.0001798544981284067\n",
      "Iteration 76/2000, Loss: 0.00019362611055839807\n",
      "Iteration 77/2000, Loss: 0.0002297971659572795\n",
      "Iteration 78/2000, Loss: 0.00016089665587060153\n",
      "Iteration 79/2000, Loss: 0.00016727994079701602\n",
      "Iteration 80/2000, Loss: 0.00013003993080928922\n",
      "Iteration 81/2000, Loss: 0.00010865827789530158\n",
      "Iteration 82/2000, Loss: 0.00015610875561833382\n",
      "Iteration 83/2000, Loss: 0.00030245023663155735\n",
      "Iteration 84/2000, Loss: 0.00035325007047504187\n",
      "Iteration 85/2000, Loss: 0.00014532993372995406\n",
      "Iteration 86/2000, Loss: 0.00016328852507285774\n",
      "Iteration 87/2000, Loss: 0.00016572455933783203\n",
      "Iteration 88/2000, Loss: 0.0001795957505237311\n",
      "Iteration 89/2000, Loss: 0.00014470148016698658\n",
      "Iteration 90/2000, Loss: 0.00027572509134188294\n",
      "Iteration 91/2000, Loss: 0.00022280588746070862\n",
      "Iteration 92/2000, Loss: 0.00019781631999649107\n",
      "Iteration 93/2000, Loss: 0.00021611565898638219\n",
      "Iteration 94/2000, Loss: 0.00018614220607560128\n",
      "Iteration 95/2000, Loss: 0.00013470191333908588\n",
      "Iteration 96/2000, Loss: 0.0003712081233970821\n",
      "Iteration 97/2000, Loss: 0.0001982652029255405\n",
      "Iteration 98/2000, Loss: 0.00013992500316817313\n",
      "Iteration 99/2000, Loss: 0.00020033278269693255\n",
      "Iteration 100/2000, Loss: 0.00023419295030180365\n",
      "Iteration 101/2000, Loss: 0.00030541306477971375\n",
      "Iteration 102/2000, Loss: 0.00024198612663894892\n",
      "Iteration 103/2000, Loss: 0.00026529093156568706\n",
      "Iteration 104/2000, Loss: 0.000368786248145625\n",
      "Iteration 105/2000, Loss: 0.0004362387116998434\n",
      "Iteration 106/2000, Loss: 0.0003954892454203218\n",
      "Iteration 107/2000, Loss: 0.00019211097969673574\n",
      "Iteration 108/2000, Loss: 0.0003789106267504394\n",
      "Iteration 109/2000, Loss: 0.0001708968193270266\n",
      "Iteration 110/2000, Loss: 0.00017385280807502568\n",
      "Iteration 111/2000, Loss: 0.0002149019273929298\n",
      "Iteration 112/2000, Loss: 0.00016266884631477296\n",
      "Iteration 113/2000, Loss: 0.00014954440121073276\n",
      "Iteration 114/2000, Loss: 0.00015120026364456862\n",
      "Iteration 115/2000, Loss: 0.0003466087218839675\n",
      "Iteration 116/2000, Loss: 0.00014847020793240517\n",
      "Iteration 117/2000, Loss: 0.00011942206765525043\n",
      "Iteration 118/2000, Loss: 0.00019837509898934513\n",
      "Iteration 119/2000, Loss: 0.00016639853129163384\n",
      "Iteration 120/2000, Loss: 0.00013047715765424073\n",
      "Iteration 121/2000, Loss: 0.00016256661911029369\n",
      "Iteration 122/2000, Loss: 0.00011166855983901769\n",
      "Iteration 123/2000, Loss: 0.00020709099771920592\n",
      "Iteration 124/2000, Loss: 0.0001533760951133445\n",
      "Iteration 125/2000, Loss: 0.0002773303131107241\n",
      "Iteration 126/2000, Loss: 0.00024296907940879464\n",
      "Iteration 127/2000, Loss: 0.0002017635852098465\n",
      "Iteration 128/2000, Loss: 0.00014709388779010624\n",
      "Iteration 129/2000, Loss: 0.00035685510374605656\n",
      "Iteration 130/2000, Loss: 0.00015758522204123437\n",
      "Iteration 131/2000, Loss: 0.0010371344396844506\n",
      "Iteration 132/2000, Loss: 0.00016856484580785036\n",
      "Iteration 133/2000, Loss: 0.00013973475142847747\n",
      "Iteration 134/2000, Loss: 0.00011731312406482175\n",
      "Iteration 135/2000, Loss: 0.00016875470464583486\n",
      "Iteration 136/2000, Loss: 0.00010936580656562\n",
      "Iteration 137/2000, Loss: 0.000728706712834537\n",
      "Iteration 138/2000, Loss: 0.00022393038670998067\n",
      "Iteration 139/2000, Loss: 0.00021531226229853928\n",
      "Iteration 140/2000, Loss: 0.00022860088211018592\n",
      "Iteration 141/2000, Loss: 0.00014505430590361357\n",
      "Iteration 142/2000, Loss: 0.00013533954916056246\n",
      "Iteration 143/2000, Loss: 0.0001296392729273066\n",
      "Iteration 144/2000, Loss: 0.00011059889948228374\n",
      "Iteration 145/2000, Loss: 0.00013150429003871977\n",
      "Iteration 146/2000, Loss: 0.00025012335390783846\n",
      "Iteration 147/2000, Loss: 0.00026068396982736886\n",
      "Iteration 148/2000, Loss: 0.000272652308922261\n",
      "Iteration 149/2000, Loss: 0.000253304693615064\n",
      "Iteration 150/2000, Loss: 0.00029214960522949696\n",
      "Iteration 151/2000, Loss: 0.00041097067878581583\n",
      "Iteration 152/2000, Loss: 0.00032784006907604635\n",
      "Iteration 153/2000, Loss: 0.00029331707628443837\n",
      "Iteration 154/2000, Loss: 0.0004029994015581906\n",
      "Iteration 155/2000, Loss: 0.0004079507780261338\n",
      "Iteration 156/2000, Loss: 0.0003177369071636349\n",
      "Iteration 157/2000, Loss: 0.00039436985389329493\n",
      "Iteration 158/2000, Loss: 0.0010790153173729777\n",
      "Iteration 159/2000, Loss: 0.00018034008098766208\n",
      "Iteration 160/2000, Loss: 0.0002575100807007402\n",
      "Iteration 161/2000, Loss: 0.0002554287202656269\n",
      "Iteration 162/2000, Loss: 0.00015554613491985947\n",
      "Iteration 163/2000, Loss: 0.00025065880618058145\n",
      "Iteration 164/2000, Loss: 0.00023492555192206055\n",
      "Iteration 165/2000, Loss: 0.00015227441326715052\n",
      "Iteration 166/2000, Loss: 0.0002905631554313004\n",
      "Iteration 167/2000, Loss: 0.00029582445858977735\n",
      "Iteration 168/2000, Loss: 0.00018994466518051922\n",
      "Iteration 169/2000, Loss: 0.00018212308350484818\n",
      "Iteration 170/2000, Loss: 0.0001957497006515041\n",
      "Iteration 171/2000, Loss: 0.0002221656613983214\n",
      "Iteration 172/2000, Loss: 0.00040132313733920455\n",
      "Iteration 173/2000, Loss: 0.00018409635231364518\n",
      "Iteration 174/2000, Loss: 0.0002309434930793941\n",
      "Iteration 175/2000, Loss: 0.0001907321420731023\n",
      "Iteration 176/2000, Loss: 0.00014037470100447536\n",
      "Iteration 177/2000, Loss: 0.0002181664458476007\n",
      "Iteration 178/2000, Loss: 0.00021337339421734214\n",
      "Iteration 179/2000, Loss: 0.00029383754008449614\n",
      "Iteration 180/2000, Loss: 0.00022763159358873963\n",
      "Iteration 181/2000, Loss: 0.00014639602159149945\n",
      "Iteration 182/2000, Loss: 0.00015234368038363755\n",
      "Iteration 183/2000, Loss: 0.00024079241848085076\n",
      "Iteration 184/2000, Loss: 0.00018242203805129975\n",
      "Iteration 185/2000, Loss: 0.00020973090431652963\n",
      "Iteration 186/2000, Loss: 0.0002431960601825267\n",
      "Iteration 187/2000, Loss: 0.00012874476669821888\n",
      "Iteration 188/2000, Loss: 0.00020360443159006536\n",
      "Iteration 189/2000, Loss: 0.0001843537320382893\n",
      "Iteration 190/2000, Loss: 0.0002575576363597065\n",
      "Iteration 191/2000, Loss: 0.00030255349702201784\n",
      "Iteration 192/2000, Loss: 0.0002521161222830415\n",
      "Iteration 193/2000, Loss: 0.00021455767273437232\n",
      "Iteration 194/2000, Loss: 0.00027605085051618516\n",
      "Iteration 195/2000, Loss: 0.00023622658045496792\n",
      "Iteration 196/2000, Loss: 0.00023232588137034327\n",
      "Iteration 197/2000, Loss: 0.00010480126366019249\n",
      "Iteration 198/2000, Loss: 0.00018296526104677469\n",
      "Iteration 199/2000, Loss: 0.00024275071336887777\n",
      "Iteration 200/2000, Loss: 0.0002703916688915342\n",
      "Iteration 201/2000, Loss: 0.00023327047529164702\n",
      "Iteration 202/2000, Loss: 0.00022896548034623265\n",
      "Iteration 203/2000, Loss: 0.0001727713824948296\n",
      "Iteration 204/2000, Loss: 0.000203982213861309\n",
      "Iteration 205/2000, Loss: 0.00023609623895026743\n",
      "Iteration 206/2000, Loss: 0.0004541885282378644\n",
      "Iteration 207/2000, Loss: 0.00017194497922901064\n",
      "Iteration 208/2000, Loss: 0.00020965361909475178\n",
      "Iteration 209/2000, Loss: 0.0003895734262187034\n",
      "Iteration 210/2000, Loss: 0.00023269538360182196\n",
      "Iteration 211/2000, Loss: 0.00017003703396767378\n",
      "Iteration 212/2000, Loss: 0.00029621736030094326\n",
      "Iteration 213/2000, Loss: 0.00021979112352710217\n",
      "Iteration 214/2000, Loss: 0.00021101527090650052\n",
      "Iteration 215/2000, Loss: 0.0005722576170228422\n",
      "Iteration 216/2000, Loss: 0.0004410170076880604\n",
      "Iteration 217/2000, Loss: 0.00021492881933227181\n",
      "Iteration 218/2000, Loss: 0.000269136595306918\n",
      "Iteration 219/2000, Loss: 0.0002220413734903559\n",
      "Iteration 220/2000, Loss: 0.00028797436971217394\n",
      "Iteration 221/2000, Loss: 0.00020281426259316504\n",
      "Iteration 222/2000, Loss: 0.00033310212893411517\n",
      "Iteration 223/2000, Loss: 0.0003232764429412782\n",
      "Iteration 224/2000, Loss: 0.00039334193570539355\n",
      "Iteration 225/2000, Loss: 0.00018994731362909079\n",
      "Iteration 226/2000, Loss: 0.00024626890080980957\n",
      "Iteration 227/2000, Loss: 0.00019696258823387325\n",
      "Iteration 228/2000, Loss: 0.00025548783014528453\n",
      "Iteration 229/2000, Loss: 0.00023680791491642594\n",
      "Iteration 230/2000, Loss: 0.0001709654607111588\n",
      "Iteration 231/2000, Loss: 0.0001711710065137595\n",
      "Iteration 232/2000, Loss: 0.00018707795243244618\n",
      "Iteration 233/2000, Loss: 0.00026075029745697975\n",
      "Iteration 234/2000, Loss: 0.00019829237135127187\n",
      "Iteration 235/2000, Loss: 0.00017603655578568578\n",
      "Iteration 236/2000, Loss: 0.00020132906502112746\n",
      "Iteration 237/2000, Loss: 0.00021313758043106645\n",
      "Iteration 238/2000, Loss: 0.0002885450376197696\n",
      "Iteration 239/2000, Loss: 0.00014534812362398952\n",
      "Iteration 240/2000, Loss: 0.00015260472719091922\n",
      "Iteration 241/2000, Loss: 0.0002344526583328843\n",
      "Iteration 242/2000, Loss: 0.00027019414119422436\n",
      "Iteration 243/2000, Loss: 0.0002196367277065292\n",
      "Iteration 244/2000, Loss: 0.00044135915231890976\n",
      "Iteration 245/2000, Loss: 0.0004061982617713511\n",
      "Iteration 246/2000, Loss: 0.00025640151579864323\n",
      "Iteration 247/2000, Loss: 0.00022004103811923414\n",
      "Iteration 248/2000, Loss: 0.00026277583674527705\n",
      "Iteration 249/2000, Loss: 0.00033767271088436246\n",
      "Iteration 250/2000, Loss: 0.00021003608708269894\n",
      "Iteration 251/2000, Loss: 0.00019143313693348318\n",
      "Iteration 252/2000, Loss: 0.00016487280663568527\n",
      "Iteration 253/2000, Loss: 0.00029956421349197626\n",
      "Iteration 254/2000, Loss: 0.0004237254033796489\n",
      "Iteration 255/2000, Loss: 0.0001474321907153353\n",
      "Iteration 256/2000, Loss: 0.0002136819384759292\n",
      "Iteration 257/2000, Loss: 0.00018246455874759704\n",
      "Iteration 258/2000, Loss: 0.000314439763315022\n",
      "Iteration 259/2000, Loss: 0.00013322448648978025\n",
      "Iteration 260/2000, Loss: 0.00012983972555957735\n",
      "Iteration 261/2000, Loss: 0.00016591540770605206\n",
      "Iteration 262/2000, Loss: 0.00019384613551665097\n",
      "Iteration 263/2000, Loss: 0.00016774696996435523\n",
      "Iteration 264/2000, Loss: 0.00017025494889821857\n",
      "Iteration 265/2000, Loss: 0.00035842377110384405\n",
      "Iteration 266/2000, Loss: 0.0002513685030862689\n",
      "Iteration 267/2000, Loss: 0.0001357093860860914\n",
      "Iteration 268/2000, Loss: 0.0002763743686955422\n",
      "Iteration 269/2000, Loss: 0.00027870162739418447\n",
      "Iteration 270/2000, Loss: 0.00015564165369141847\n",
      "Iteration 271/2000, Loss: 0.0002671518886927515\n",
      "Iteration 272/2000, Loss: 0.0002197308058384806\n",
      "Iteration 273/2000, Loss: 0.00015076085401233286\n",
      "Iteration 274/2000, Loss: 0.0002850190212484449\n",
      "Iteration 275/2000, Loss: 0.00024021982972044498\n",
      "Iteration 276/2000, Loss: 0.00018548710795585066\n",
      "Iteration 277/2000, Loss: 0.0002213383704656735\n",
      "Iteration 278/2000, Loss: 0.0002018806990236044\n",
      "Iteration 279/2000, Loss: 0.00014797686890233308\n",
      "Iteration 280/2000, Loss: 0.0002829114382620901\n",
      "Iteration 281/2000, Loss: 0.00034397313720546663\n",
      "Iteration 282/2000, Loss: 0.00019081216305494308\n",
      "Iteration 283/2000, Loss: 0.00022950995480641723\n",
      "Iteration 284/2000, Loss: 0.00014019494119565934\n",
      "Iteration 285/2000, Loss: 0.000275494676316157\n",
      "Iteration 286/2000, Loss: 0.00014171405928209424\n",
      "Iteration 287/2000, Loss: 0.00017341916100122035\n",
      "Iteration 288/2000, Loss: 0.00014173438830766827\n",
      "Iteration 289/2000, Loss: 0.00032423000084236264\n",
      "Iteration 290/2000, Loss: 0.00019646063446998596\n",
      "Iteration 291/2000, Loss: 0.00015608711692038924\n",
      "Iteration 292/2000, Loss: 0.0003912958491127938\n",
      "Iteration 293/2000, Loss: 0.00022954623273108155\n",
      "Iteration 294/2000, Loss: 0.000220434638322331\n",
      "Iteration 295/2000, Loss: 0.00014164240565150976\n",
      "Iteration 296/2000, Loss: 0.0001373734703520313\n",
      "Iteration 297/2000, Loss: 0.00011733844439731911\n",
      "Iteration 298/2000, Loss: 0.0002526890311855823\n",
      "Iteration 299/2000, Loss: 0.00021517959248740226\n",
      "Iteration 300/2000, Loss: 0.0002659133169800043\n",
      "Iteration 301/2000, Loss: 0.00017526360170450062\n",
      "Iteration 302/2000, Loss: 0.00028119012131355703\n",
      "Iteration 303/2000, Loss: 0.00014415214536711574\n",
      "Iteration 304/2000, Loss: 0.0003235314798075706\n",
      "Iteration 305/2000, Loss: 0.00029106324655003846\n",
      "Iteration 306/2000, Loss: 0.00021225149976089597\n",
      "Iteration 307/2000, Loss: 0.0002347233530599624\n",
      "Iteration 308/2000, Loss: 0.0001847383682616055\n",
      "Iteration 309/2000, Loss: 0.0002153304376406595\n",
      "Iteration 310/2000, Loss: 0.0006470954394899309\n",
      "Iteration 311/2000, Loss: 0.0002956890675704926\n",
      "Iteration 312/2000, Loss: 0.00028185511473566294\n",
      "Iteration 313/2000, Loss: 0.0002413403708487749\n",
      "Iteration 314/2000, Loss: 0.00022176346101332456\n",
      "Iteration 315/2000, Loss: 0.0002528006152715534\n",
      "Iteration 316/2000, Loss: 0.00023068118025548756\n",
      "Iteration 317/2000, Loss: 0.00031332045909948647\n",
      "Iteration 318/2000, Loss: 0.00012790842447429895\n",
      "Iteration 319/2000, Loss: 0.00022010273823980242\n",
      "Iteration 320/2000, Loss: 0.0003502850595396012\n",
      "Iteration 321/2000, Loss: 0.00019414252892602235\n",
      "Iteration 322/2000, Loss: 0.00016917123866733164\n",
      "Iteration 323/2000, Loss: 0.0002155704569304362\n",
      "Iteration 324/2000, Loss: 0.0004979126970283687\n",
      "Iteration 325/2000, Loss: 0.000200641414267011\n",
      "Iteration 326/2000, Loss: 0.00024582119658589363\n",
      "Iteration 327/2000, Loss: 0.00038922892417758703\n",
      "Iteration 328/2000, Loss: 0.0002632451360113919\n",
      "Iteration 329/2000, Loss: 0.0002091775822918862\n",
      "Iteration 330/2000, Loss: 0.0002124775346601382\n",
      "Iteration 331/2000, Loss: 0.00024545210180804133\n",
      "Iteration 332/2000, Loss: 0.00015785207506269217\n",
      "Iteration 333/2000, Loss: 0.00016082919319160283\n",
      "Iteration 334/2000, Loss: 0.00015540221647825092\n",
      "Iteration 335/2000, Loss: 0.0002323440567124635\n",
      "Iteration 336/2000, Loss: 0.00031749752815812826\n",
      "Iteration 337/2000, Loss: 0.00017239512817468494\n",
      "Iteration 338/2000, Loss: 0.0002564621972851455\n",
      "Iteration 339/2000, Loss: 0.00028131878934800625\n",
      "Iteration 340/2000, Loss: 0.00018410934717394412\n",
      "Iteration 341/2000, Loss: 0.0002011619071708992\n",
      "Iteration 342/2000, Loss: 0.0002483864373061806\n",
      "Iteration 343/2000, Loss: 0.00020739174215123057\n",
      "Iteration 344/2000, Loss: 0.00021613287390209734\n",
      "Iteration 345/2000, Loss: 0.0001409933902323246\n",
      "Iteration 346/2000, Loss: 0.0001653065119171515\n",
      "Iteration 347/2000, Loss: 0.00017887854482978582\n",
      "Iteration 348/2000, Loss: 0.00018074568652082235\n",
      "Iteration 349/2000, Loss: 0.0001691479264991358\n",
      "Iteration 350/2000, Loss: 0.00023019019863568246\n",
      "Iteration 351/2000, Loss: 0.00026942166732624173\n",
      "Iteration 352/2000, Loss: 0.00021027779439464211\n",
      "Iteration 353/2000, Loss: 0.00013909772678744048\n",
      "Iteration 354/2000, Loss: 0.0003132630663458258\n",
      "Iteration 355/2000, Loss: 0.00022188539151102304\n",
      "Iteration 356/2000, Loss: 0.00017854095494840294\n",
      "Iteration 357/2000, Loss: 0.0001031504143611528\n",
      "Iteration 358/2000, Loss: 0.00021747515711467713\n",
      "Iteration 359/2000, Loss: 0.00014578034461010247\n",
      "Iteration 360/2000, Loss: 0.00025278443354181945\n",
      "Iteration 361/2000, Loss: 0.0003494459670037031\n",
      "Iteration 362/2000, Loss: 0.0001707287592580542\n",
      "Iteration 363/2000, Loss: 0.00026411397266201675\n",
      "Iteration 364/2000, Loss: 0.00022354116663336754\n",
      "Iteration 365/2000, Loss: 0.00037767572212032974\n",
      "Iteration 366/2000, Loss: 0.00012890424113720655\n",
      "Iteration 367/2000, Loss: 0.00022959290072321892\n",
      "Iteration 368/2000, Loss: 0.0001573565386934206\n",
      "Iteration 369/2000, Loss: 0.00021441430726554245\n",
      "Iteration 370/2000, Loss: 0.00014252426626626402\n",
      "Iteration 371/2000, Loss: 0.0002884305140469223\n",
      "Iteration 372/2000, Loss: 0.00016667923773638904\n",
      "Iteration 373/2000, Loss: 0.0001392132689943537\n",
      "Iteration 374/2000, Loss: 0.00012236657494213432\n",
      "Iteration 375/2000, Loss: 0.00015874083328526467\n",
      "Iteration 376/2000, Loss: 0.00034683369449339807\n",
      "Iteration 377/2000, Loss: 0.0002198701840825379\n",
      "Iteration 378/2000, Loss: 0.00020735245198011398\n",
      "Iteration 379/2000, Loss: 0.0002243940398329869\n",
      "Iteration 380/2000, Loss: 0.00010837880108738318\n",
      "Iteration 381/2000, Loss: 0.00017952144844457507\n",
      "Iteration 382/2000, Loss: 0.00042335825855843723\n",
      "Iteration 383/2000, Loss: 0.00017096824012696743\n",
      "Iteration 384/2000, Loss: 0.00026925475685857236\n",
      "Iteration 385/2000, Loss: 0.00016573030734434724\n",
      "Iteration 386/2000, Loss: 0.00024282520462293178\n",
      "Iteration 387/2000, Loss: 0.0003549063694663346\n",
      "Iteration 388/2000, Loss: 0.00019166198035236448\n",
      "Iteration 389/2000, Loss: 0.00024471007054671645\n",
      "Iteration 390/2000, Loss: 0.00018485603504814208\n",
      "Iteration 391/2000, Loss: 0.0002373897732468322\n",
      "Iteration 392/2000, Loss: 0.0005804623942822218\n",
      "Iteration 393/2000, Loss: 0.0002045307628577575\n",
      "Iteration 394/2000, Loss: 0.0002629932714626193\n",
      "Iteration 395/2000, Loss: 0.00033063339651562274\n",
      "Iteration 396/2000, Loss: 0.00036136110429652035\n",
      "Iteration 397/2000, Loss: 0.00014998998085502535\n",
      "Iteration 398/2000, Loss: 0.0002483490970917046\n",
      "Iteration 399/2000, Loss: 0.000126051832921803\n",
      "Iteration 400/2000, Loss: 0.00025759730488061905\n",
      "Iteration 401/2000, Loss: 0.0001717195991659537\n",
      "Iteration 402/2000, Loss: 0.00017728310194797814\n",
      "Iteration 403/2000, Loss: 0.00019217882072553039\n",
      "Iteration 404/2000, Loss: 0.0002004466950893402\n",
      "Iteration 405/2000, Loss: 0.000293163291644305\n",
      "Iteration 406/2000, Loss: 0.00015993935812730342\n",
      "Iteration 407/2000, Loss: 0.0002488117024768144\n",
      "Iteration 408/2000, Loss: 0.00022880930919200182\n",
      "Iteration 409/2000, Loss: 0.00017769985424820334\n",
      "Iteration 410/2000, Loss: 0.0001276744151255116\n",
      "Iteration 411/2000, Loss: 0.00014956927043385804\n",
      "Iteration 412/2000, Loss: 0.00029602934955619276\n",
      "Iteration 413/2000, Loss: 0.00010884786752285436\n",
      "Iteration 414/2000, Loss: 0.00024993103579618037\n",
      "Iteration 415/2000, Loss: 0.00012960063759237528\n",
      "Iteration 416/2000, Loss: 0.00020868456340394914\n",
      "Iteration 417/2000, Loss: 0.00040006553172133863\n",
      "Iteration 418/2000, Loss: 0.0001640137779759243\n",
      "Iteration 419/2000, Loss: 0.00020205836335662752\n",
      "Iteration 420/2000, Loss: 0.00013596033386420459\n",
      "Iteration 421/2000, Loss: 0.0008268296369351447\n",
      "Iteration 422/2000, Loss: 0.00013326579937711358\n",
      "Iteration 423/2000, Loss: 0.0002476671652402729\n",
      "Iteration 424/2000, Loss: 0.00024063189630396664\n",
      "Iteration 425/2000, Loss: 0.00018230456043966115\n",
      "Iteration 426/2000, Loss: 0.00019785806944128126\n",
      "Iteration 427/2000, Loss: 0.00017808003758545965\n",
      "Iteration 428/2000, Loss: 0.00013464027142617851\n",
      "Iteration 429/2000, Loss: 0.00019156135385856032\n",
      "Iteration 430/2000, Loss: 0.00019974785391241312\n",
      "Iteration 431/2000, Loss: 0.00016105033864732832\n",
      "Iteration 432/2000, Loss: 0.00018619011098053306\n",
      "Iteration 433/2000, Loss: 0.0002973997325170785\n",
      "Iteration 434/2000, Loss: 0.00021482183365151286\n",
      "Iteration 435/2000, Loss: 0.00017581127758603543\n",
      "Iteration 436/2000, Loss: 0.0002277140156365931\n",
      "Iteration 437/2000, Loss: 0.00021816344815306365\n",
      "Iteration 438/2000, Loss: 0.00013632660557050258\n",
      "Iteration 439/2000, Loss: 0.00018154139979742467\n",
      "Iteration 440/2000, Loss: 0.0004156185023020953\n",
      "Iteration 441/2000, Loss: 0.00015177283785305917\n",
      "Iteration 442/2000, Loss: 0.0001780917664291337\n",
      "Iteration 443/2000, Loss: 0.00022613642795477062\n",
      "Iteration 444/2000, Loss: 0.00021746182756032795\n",
      "Iteration 445/2000, Loss: 0.0002297715691383928\n",
      "Iteration 446/2000, Loss: 0.00014275251305662096\n",
      "Iteration 447/2000, Loss: 0.00019217535736970603\n",
      "Iteration 448/2000, Loss: 0.00015927187632769346\n",
      "Iteration 449/2000, Loss: 0.00013466512609738857\n",
      "Iteration 450/2000, Loss: 0.0001807169319363311\n",
      "Iteration 451/2000, Loss: 0.00028492650017142296\n",
      "Iteration 452/2000, Loss: 0.00012443553714547306\n",
      "Iteration 453/2000, Loss: 0.00016742732259444892\n",
      "Iteration 454/2000, Loss: 0.0002120490971719846\n",
      "Iteration 455/2000, Loss: 0.00016306016186717898\n",
      "Iteration 456/2000, Loss: 0.00014556506357621402\n",
      "Iteration 457/2000, Loss: 0.00015058033750392497\n",
      "Iteration 458/2000, Loss: 0.0001705727627268061\n",
      "Iteration 459/2000, Loss: 0.00017638517601881176\n",
      "Iteration 460/2000, Loss: 8.963649452198297e-05\n",
      "Iteration 461/2000, Loss: 0.00014833593741059303\n",
      "Iteration 462/2000, Loss: 0.00013880719779990613\n",
      "Iteration 463/2000, Loss: 0.0002005145070143044\n",
      "Iteration 464/2000, Loss: 0.000389670894946903\n",
      "Iteration 465/2000, Loss: 0.00016763074381742626\n",
      "Iteration 466/2000, Loss: 0.00039877014933153987\n",
      "Iteration 467/2000, Loss: 0.00015328630979638547\n",
      "Iteration 468/2000, Loss: 0.0003031472151633352\n",
      "Iteration 469/2000, Loss: 0.0015841766726225615\n",
      "Iteration 470/2000, Loss: 0.00021313739125616848\n",
      "Iteration 471/2000, Loss: 0.0002190558734582737\n",
      "Iteration 472/2000, Loss: 0.00023539914400316775\n",
      "Iteration 473/2000, Loss: 0.0001718579005682841\n",
      "Iteration 474/2000, Loss: 0.0003169870760757476\n",
      "Iteration 475/2000, Loss: 0.0003000333090312779\n",
      "Iteration 476/2000, Loss: 0.0002265448565594852\n",
      "Iteration 477/2000, Loss: 0.00033955363323912024\n",
      "Iteration 478/2000, Loss: 0.00020170604693703353\n",
      "Iteration 479/2000, Loss: 0.00017203039897140115\n",
      "Iteration 480/2000, Loss: 0.00030850424082018435\n",
      "Iteration 481/2000, Loss: 0.00017354238661937416\n",
      "Iteration 482/2000, Loss: 0.0001758817961672321\n",
      "Iteration 483/2000, Loss: 0.0002487451129127294\n",
      "Iteration 484/2000, Loss: 0.00028991830185987055\n",
      "Iteration 485/2000, Loss: 0.00024114677216857672\n",
      "Iteration 486/2000, Loss: 0.00019224855350330472\n",
      "Iteration 487/2000, Loss: 0.0002583633759059012\n",
      "Iteration 488/2000, Loss: 0.00019965013780165464\n",
      "Iteration 489/2000, Loss: 0.0002189871738664806\n",
      "Iteration 490/2000, Loss: 0.00014693339471705258\n",
      "Iteration 491/2000, Loss: 0.0001372115220874548\n",
      "Iteration 492/2000, Loss: 0.00015813841309864074\n",
      "Iteration 493/2000, Loss: 0.0002427442086627707\n",
      "Iteration 494/2000, Loss: 0.00033234120928682387\n",
      "Iteration 495/2000, Loss: 0.0002588457427918911\n",
      "Iteration 496/2000, Loss: 0.00024202179338317364\n",
      "Iteration 497/2000, Loss: 0.00023272917314898223\n",
      "Iteration 498/2000, Loss: 0.00019101405632682145\n",
      "Iteration 499/2000, Loss: 0.0007033784640952945\n",
      "Iteration 500/2000, Loss: 0.0003157922183163464\n",
      "Iteration 501/2000, Loss: 0.0001577929506311193\n",
      "Iteration 502/2000, Loss: 0.00030874263029545546\n",
      "Iteration 503/2000, Loss: 0.0002772434672806412\n",
      "Iteration 504/2000, Loss: 0.00019551892182789743\n",
      "Iteration 505/2000, Loss: 0.00036869454197585583\n",
      "Iteration 506/2000, Loss: 0.00014661137538496405\n",
      "Iteration 507/2000, Loss: 0.0002449193561915308\n",
      "Iteration 508/2000, Loss: 0.00026278485893271863\n",
      "Iteration 509/2000, Loss: 0.00023975243675522506\n",
      "Iteration 510/2000, Loss: 0.0003885815676767379\n",
      "Iteration 511/2000, Loss: 0.00019129228894598782\n",
      "Iteration 512/2000, Loss: 0.00021417837706394494\n",
      "Iteration 513/2000, Loss: 0.00035691255470737815\n",
      "Iteration 514/2000, Loss: 0.00018387932504992932\n",
      "Iteration 515/2000, Loss: 0.0004760508018080145\n",
      "Iteration 516/2000, Loss: 0.00022065205848775804\n",
      "Iteration 517/2000, Loss: 0.00020402359950821847\n",
      "Iteration 518/2000, Loss: 0.00046706548891961575\n",
      "Iteration 519/2000, Loss: 0.00019337900448590517\n",
      "Iteration 520/2000, Loss: 0.0002079147525364533\n",
      "Iteration 521/2000, Loss: 0.00020144024165347219\n",
      "Iteration 522/2000, Loss: 0.000172734959051013\n",
      "Iteration 523/2000, Loss: 0.0002715087903197855\n",
      "Iteration 524/2000, Loss: 0.00032035564072430134\n",
      "Iteration 525/2000, Loss: 0.00020578519615810364\n",
      "Iteration 526/2000, Loss: 0.0003327993326820433\n",
      "Iteration 527/2000, Loss: 0.00015613474533893168\n",
      "Iteration 528/2000, Loss: 0.00031156803015619516\n",
      "Iteration 529/2000, Loss: 0.00018295148038305342\n",
      "Iteration 530/2000, Loss: 0.0005625298945233226\n",
      "Iteration 531/2000, Loss: 0.0002420462406007573\n",
      "Iteration 532/2000, Loss: 0.00025583739625290036\n",
      "Iteration 533/2000, Loss: 0.00021802556875627488\n",
      "Iteration 534/2000, Loss: 0.00024324021069332957\n",
      "Iteration 535/2000, Loss: 0.00014200821169652045\n",
      "Iteration 536/2000, Loss: 0.0004752364766318351\n",
      "Iteration 537/2000, Loss: 0.0001681405701674521\n",
      "Iteration 538/2000, Loss: 0.00018264523532707244\n",
      "Iteration 539/2000, Loss: 0.00016990360745694488\n",
      "Iteration 540/2000, Loss: 0.0001718145067570731\n",
      "Iteration 541/2000, Loss: 0.00024442412541247904\n",
      "Iteration 542/2000, Loss: 0.0002734668378252536\n",
      "Iteration 543/2000, Loss: 0.00022808976063970476\n",
      "Iteration 544/2000, Loss: 0.00048000868991948664\n",
      "Iteration 545/2000, Loss: 0.00017521290283184499\n",
      "Iteration 546/2000, Loss: 0.0003486841160338372\n",
      "Iteration 547/2000, Loss: 0.00041940115625038743\n",
      "Iteration 548/2000, Loss: 0.00013855344150215387\n",
      "Iteration 549/2000, Loss: 0.0005320507334545255\n",
      "Iteration 550/2000, Loss: 0.00018722584354691207\n",
      "Iteration 551/2000, Loss: 0.00024883326841518283\n",
      "Iteration 552/2000, Loss: 0.0004223170690238476\n",
      "Iteration 553/2000, Loss: 0.00015868223272264004\n",
      "Iteration 554/2000, Loss: 0.00037188222631812096\n",
      "Iteration 555/2000, Loss: 0.0002220849273726344\n",
      "Iteration 556/2000, Loss: 0.0004655805241782218\n",
      "Iteration 557/2000, Loss: 0.0002590903313830495\n",
      "Iteration 558/2000, Loss: 0.00017869594739750028\n",
      "Iteration 559/2000, Loss: 0.00014353478036355227\n",
      "Iteration 560/2000, Loss: 0.00025942520005628467\n",
      "Iteration 561/2000, Loss: 0.00020188694179523736\n",
      "Iteration 562/2000, Loss: 0.0003706343413796276\n",
      "Iteration 563/2000, Loss: 0.00040623568929731846\n",
      "Iteration 564/2000, Loss: 0.00035844577359966934\n",
      "Iteration 565/2000, Loss: 0.0001374652492813766\n",
      "Iteration 566/2000, Loss: 0.00017310158000327647\n",
      "Iteration 567/2000, Loss: 0.0001682817528489977\n",
      "Iteration 568/2000, Loss: 0.00025208957958966494\n",
      "Iteration 569/2000, Loss: 0.00015491181693505496\n",
      "Iteration 570/2000, Loss: 0.00014305768127087504\n",
      "Iteration 571/2000, Loss: 0.00018852933135349303\n",
      "Iteration 572/2000, Loss: 0.0002890190517064184\n",
      "Iteration 573/2000, Loss: 0.00039234632276929915\n",
      "Iteration 574/2000, Loss: 0.00013661265256814659\n",
      "Iteration 575/2000, Loss: 0.0002671174006536603\n",
      "Iteration 576/2000, Loss: 0.0002458758244756609\n",
      "Iteration 577/2000, Loss: 0.0002911857736762613\n",
      "Iteration 578/2000, Loss: 0.00020041689276695251\n",
      "Iteration 579/2000, Loss: 0.00040595923201180995\n",
      "Iteration 580/2000, Loss: 0.0002983890881296247\n",
      "Iteration 581/2000, Loss: 0.00016933253209572285\n",
      "Iteration 582/2000, Loss: 0.0005764166126027703\n",
      "Iteration 583/2000, Loss: 0.0005329171544872224\n",
      "Iteration 584/2000, Loss: 0.00018385055591352284\n",
      "Iteration 585/2000, Loss: 0.00023669363872613758\n",
      "Iteration 586/2000, Loss: 0.00033975191763602197\n",
      "Iteration 587/2000, Loss: 0.00021235656458884478\n",
      "Iteration 588/2000, Loss: 0.0002689702669158578\n",
      "Iteration 589/2000, Loss: 0.00022249421454034746\n",
      "Iteration 590/2000, Loss: 0.00019239868561271578\n",
      "Iteration 591/2000, Loss: 0.00013171501632314175\n",
      "Iteration 592/2000, Loss: 0.00013398277224041522\n",
      "Iteration 593/2000, Loss: 0.00025358531274832785\n",
      "Iteration 594/2000, Loss: 0.0001397469750372693\n",
      "Iteration 595/2000, Loss: 0.00031208127620629966\n",
      "Iteration 596/2000, Loss: 0.00033992985845543444\n",
      "Iteration 597/2000, Loss: 0.0001416625309502706\n",
      "Iteration 598/2000, Loss: 0.0003439770080149174\n",
      "Iteration 599/2000, Loss: 0.00025010458193719387\n",
      "Iteration 600/2000, Loss: 0.0002766271645668894\n",
      "Iteration 601/2000, Loss: 0.000371665257262066\n",
      "Iteration 602/2000, Loss: 0.00052842334844172\n",
      "Iteration 603/2000, Loss: 0.00022162232198752463\n",
      "Iteration 604/2000, Loss: 0.00027852391940541565\n",
      "Iteration 605/2000, Loss: 0.0002646443317644298\n",
      "Iteration 606/2000, Loss: 0.00040298348176293075\n",
      "Iteration 607/2000, Loss: 0.00030435097869485617\n",
      "Iteration 608/2000, Loss: 0.000330803421093151\n",
      "Iteration 609/2000, Loss: 0.00035507537540979683\n",
      "Iteration 610/2000, Loss: 0.00046914612175896764\n",
      "Iteration 611/2000, Loss: 0.00042413658229634166\n",
      "Iteration 612/2000, Loss: 0.0003208992420695722\n",
      "Iteration 613/2000, Loss: 0.0003940060269087553\n",
      "Iteration 614/2000, Loss: 0.0002825216797646135\n",
      "Iteration 615/2000, Loss: 0.0004138615622650832\n",
      "Iteration 616/2000, Loss: 0.0004965345142409205\n",
      "Iteration 617/2000, Loss: 0.0002470858453307301\n",
      "Iteration 618/2000, Loss: 0.00036588203511200845\n",
      "Iteration 619/2000, Loss: 0.0004922453663311899\n",
      "Iteration 620/2000, Loss: 0.00020527672313619405\n",
      "Iteration 621/2000, Loss: 0.00031040087924338877\n",
      "Iteration 622/2000, Loss: 0.0004063176747877151\n",
      "Iteration 623/2000, Loss: 0.0002510421327315271\n",
      "Iteration 624/2000, Loss: 0.000419981952290982\n",
      "Iteration 625/2000, Loss: 0.00019565384718589485\n",
      "Iteration 626/2000, Loss: 0.000573996570892632\n",
      "Iteration 627/2000, Loss: 0.00044828251702710986\n",
      "Iteration 628/2000, Loss: 0.00026553214411251247\n",
      "Iteration 629/2000, Loss: 0.0005434806807897985\n",
      "Iteration 630/2000, Loss: 0.0005099325207993388\n",
      "Iteration 631/2000, Loss: 0.0005717634921893477\n",
      "Iteration 632/2000, Loss: 0.0003944169438909739\n",
      "Iteration 633/2000, Loss: 0.000411502638598904\n",
      "Iteration 634/2000, Loss: 0.0003674811450764537\n",
      "Iteration 635/2000, Loss: 0.0003342962882015854\n",
      "Iteration 636/2000, Loss: 0.0002543024020269513\n",
      "Iteration 637/2000, Loss: 0.0002943449071608484\n",
      "Iteration 638/2000, Loss: 0.0006031515076756477\n",
      "Iteration 639/2000, Loss: 0.000517269829288125\n",
      "Iteration 640/2000, Loss: 0.00024712682352401316\n",
      "Iteration 641/2000, Loss: 0.0004798271111212671\n",
      "Iteration 642/2000, Loss: 0.000841394707094878\n",
      "Iteration 643/2000, Loss: 0.00028397212736308575\n",
      "Iteration 644/2000, Loss: 0.0003008278727065772\n",
      "Iteration 645/2000, Loss: 0.0001794329145923257\n",
      "Iteration 646/2000, Loss: 0.0005767519469372928\n",
      "Iteration 647/2000, Loss: 0.00023586091992910951\n",
      "Iteration 648/2000, Loss: 0.00027210245025344193\n",
      "Iteration 649/2000, Loss: 0.0002917235251516104\n",
      "Iteration 650/2000, Loss: 0.00018875047680921853\n",
      "Iteration 651/2000, Loss: 0.0003349572070874274\n",
      "Iteration 652/2000, Loss: 0.00026121476548723876\n",
      "Iteration 653/2000, Loss: 0.00019436240836512297\n",
      "Iteration 654/2000, Loss: 0.00020810536807402968\n",
      "Iteration 655/2000, Loss: 0.00021250300051178783\n",
      "Iteration 656/2000, Loss: 0.0001984215050470084\n",
      "Iteration 657/2000, Loss: 0.00023617960687261075\n",
      "Iteration 658/2000, Loss: 0.00033980494481511414\n",
      "Iteration 659/2000, Loss: 0.00019499281188473105\n",
      "Iteration 660/2000, Loss: 0.0003106009098701179\n",
      "Iteration 661/2000, Loss: 0.0002485187433194369\n",
      "Iteration 662/2000, Loss: 0.00022463359346147627\n",
      "Iteration 663/2000, Loss: 0.000219431021832861\n",
      "Iteration 664/2000, Loss: 0.0004378309240564704\n",
      "Iteration 665/2000, Loss: 0.0002388692373642698\n",
      "Iteration 666/2000, Loss: 0.0006558160530403256\n",
      "Iteration 667/2000, Loss: 0.00041608657920733094\n",
      "Iteration 668/2000, Loss: 0.0001852682325989008\n",
      "Iteration 669/2000, Loss: 0.00047848920803517103\n",
      "Iteration 670/2000, Loss: 0.00037490634713321924\n",
      "Iteration 671/2000, Loss: 0.0003169028495904058\n",
      "Iteration 672/2000, Loss: 0.0004003253998234868\n",
      "Iteration 673/2000, Loss: 0.00022944874945096672\n",
      "Iteration 674/2000, Loss: 0.0003267153224442154\n",
      "Iteration 675/2000, Loss: 0.0002075566299026832\n",
      "Iteration 676/2000, Loss: 0.00033761319355107844\n",
      "Iteration 677/2000, Loss: 0.0002850809250958264\n",
      "Iteration 678/2000, Loss: 0.0003457048733253032\n",
      "Iteration 679/2000, Loss: 0.00022837691358290613\n",
      "Iteration 680/2000, Loss: 0.0005323716322891414\n",
      "Iteration 681/2000, Loss: 0.0003261409583501518\n",
      "Iteration 682/2000, Loss: 0.00013627862790599465\n",
      "Iteration 683/2000, Loss: 0.00031788315391168\n",
      "Iteration 684/2000, Loss: 0.00018079343135468662\n",
      "Iteration 685/2000, Loss: 0.00016794879047665745\n",
      "Iteration 686/2000, Loss: 0.0003150525444652885\n",
      "Iteration 687/2000, Loss: 0.00025774750974960625\n",
      "Iteration 688/2000, Loss: 0.0006388986366800964\n",
      "Iteration 689/2000, Loss: 0.0002128152409568429\n",
      "Iteration 690/2000, Loss: 0.00021134760754648596\n",
      "Iteration 691/2000, Loss: 0.00017395956092514098\n",
      "Iteration 692/2000, Loss: 0.00029338206513784826\n",
      "Iteration 693/2000, Loss: 0.0001923747331602499\n",
      "Iteration 694/2000, Loss: 0.00024118607689160854\n",
      "Iteration 695/2000, Loss: 0.00019668019376695156\n",
      "Iteration 696/2000, Loss: 0.00016255317314062268\n",
      "Iteration 697/2000, Loss: 0.00027904572198167443\n",
      "Iteration 698/2000, Loss: 0.00017592318181414157\n",
      "Iteration 699/2000, Loss: 0.0005892712506465614\n",
      "Iteration 700/2000, Loss: 0.00020292721455916762\n",
      "Iteration 701/2000, Loss: 0.0004017497121822089\n",
      "Iteration 702/2000, Loss: 0.0007022990030236542\n",
      "Iteration 703/2000, Loss: 0.00021620311599690467\n",
      "Iteration 704/2000, Loss: 0.0003211229050066322\n",
      "Iteration 705/2000, Loss: 0.00034636989585123956\n",
      "Iteration 706/2000, Loss: 0.0003752184275072068\n",
      "Iteration 707/2000, Loss: 0.00030793173937126994\n",
      "Iteration 708/2000, Loss: 0.0003400850691832602\n",
      "Iteration 709/2000, Loss: 0.00033920235000550747\n",
      "Iteration 710/2000, Loss: 0.0008617597632110119\n",
      "Iteration 711/2000, Loss: 0.00019146308477502316\n",
      "Iteration 712/2000, Loss: 0.00023049897572491318\n",
      "Iteration 713/2000, Loss: 0.00037750706542283297\n",
      "Iteration 714/2000, Loss: 0.0001946070697158575\n",
      "Iteration 715/2000, Loss: 0.00019611221796367317\n",
      "Iteration 716/2000, Loss: 0.0001515004987595603\n",
      "Iteration 717/2000, Loss: 0.00018569116946309805\n",
      "Iteration 718/2000, Loss: 0.0002756137400865555\n",
      "Iteration 719/2000, Loss: 0.00017796175961848348\n",
      "Iteration 720/2000, Loss: 0.00028119937633164227\n",
      "Iteration 721/2000, Loss: 0.00023218784190248698\n",
      "Iteration 722/2000, Loss: 0.00021505083714146167\n",
      "Iteration 723/2000, Loss: 0.0002735973976086825\n",
      "Iteration 724/2000, Loss: 0.000243210990447551\n",
      "Iteration 725/2000, Loss: 0.00021174628636799753\n",
      "Iteration 726/2000, Loss: 0.00020870170556008816\n",
      "Iteration 727/2000, Loss: 0.00034487139782868326\n",
      "Iteration 728/2000, Loss: 0.00016636881628073752\n",
      "Iteration 729/2000, Loss: 0.0002629888476803899\n",
      "Iteration 730/2000, Loss: 0.00017303104687016457\n",
      "Iteration 731/2000, Loss: 0.00013182111433707178\n",
      "Iteration 732/2000, Loss: 0.0002100902347592637\n",
      "Iteration 733/2000, Loss: 0.00015500240260735154\n",
      "Iteration 734/2000, Loss: 0.0002125034516211599\n",
      "Iteration 735/2000, Loss: 0.00014685788482893258\n",
      "Iteration 736/2000, Loss: 0.00013657627278007567\n",
      "Iteration 737/2000, Loss: 0.00021160044707357883\n",
      "Iteration 738/2000, Loss: 0.00017289792594965547\n",
      "Iteration 739/2000, Loss: 0.00023521322873421013\n",
      "Iteration 740/2000, Loss: 0.00017076412041205913\n",
      "Iteration 741/2000, Loss: 0.00015680644719395787\n",
      "Iteration 742/2000, Loss: 0.00019781496666837484\n",
      "Iteration 743/2000, Loss: 0.00023111875634640455\n",
      "Iteration 744/2000, Loss: 0.00024051890068221837\n",
      "Iteration 745/2000, Loss: 0.00014405525871552527\n",
      "Iteration 746/2000, Loss: 0.00019289825286250561\n",
      "Iteration 747/2000, Loss: 0.00023047826834954321\n",
      "Iteration 748/2000, Loss: 0.00014289331738837063\n",
      "Iteration 749/2000, Loss: 0.00022019812604412436\n",
      "Iteration 750/2000, Loss: 0.0001966365525731817\n",
      "Iteration 751/2000, Loss: 0.00015511932724621147\n",
      "Iteration 752/2000, Loss: 0.0002013974153669551\n",
      "Iteration 753/2000, Loss: 0.00015496127889491618\n",
      "Iteration 754/2000, Loss: 0.00032471370650455356\n",
      "Iteration 755/2000, Loss: 0.00014103972353041172\n",
      "Iteration 756/2000, Loss: 0.00034009854425676167\n",
      "Iteration 757/2000, Loss: 0.00030792044708505273\n",
      "Iteration 758/2000, Loss: 0.00021670127171091735\n",
      "Iteration 759/2000, Loss: 0.00022209224698599428\n",
      "Iteration 760/2000, Loss: 0.00017843703972175717\n",
      "Iteration 761/2000, Loss: 0.0003132450510747731\n",
      "Iteration 762/2000, Loss: 0.00039754423778504133\n",
      "Iteration 763/2000, Loss: 0.0002760972420219332\n",
      "Iteration 764/2000, Loss: 0.000237792162806727\n",
      "Iteration 765/2000, Loss: 0.0002661465550772846\n",
      "Iteration 766/2000, Loss: 0.00028842297615483403\n",
      "Iteration 767/2000, Loss: 0.00015526640345342457\n",
      "Iteration 768/2000, Loss: 0.00024076395493466407\n",
      "Iteration 769/2000, Loss: 0.00028029849636368454\n",
      "Iteration 770/2000, Loss: 0.000231214813538827\n",
      "Iteration 771/2000, Loss: 0.0003189661365468055\n",
      "Iteration 772/2000, Loss: 0.00011252292460994795\n",
      "Iteration 773/2000, Loss: 0.0001496064942330122\n",
      "Iteration 774/2000, Loss: 0.0002556068357080221\n",
      "Iteration 775/2000, Loss: 0.00028280430706217885\n",
      "Iteration 776/2000, Loss: 0.00017937549273483455\n",
      "Iteration 777/2000, Loss: 0.0002382379025220871\n",
      "Iteration 778/2000, Loss: 0.0006345666479319334\n",
      "Iteration 779/2000, Loss: 0.0003246163541916758\n",
      "Iteration 780/2000, Loss: 0.00029726672801189125\n",
      "Iteration 781/2000, Loss: 0.00020225715707056224\n",
      "Iteration 782/2000, Loss: 0.00026044133119285107\n",
      "Iteration 783/2000, Loss: 0.0004691375361289829\n",
      "Iteration 784/2000, Loss: 0.0001313545653829351\n",
      "Iteration 785/2000, Loss: 0.00019608737784437835\n",
      "Iteration 786/2000, Loss: 0.00013428613601718098\n",
      "Iteration 787/2000, Loss: 0.00012797846284229308\n",
      "Iteration 788/2000, Loss: 0.0001258188276551664\n",
      "Iteration 789/2000, Loss: 0.00022117991466075182\n",
      "Iteration 790/2000, Loss: 0.0004072493175044656\n",
      "Iteration 791/2000, Loss: 0.00022883601195644587\n",
      "Iteration 792/2000, Loss: 0.0002661719045136124\n",
      "Iteration 793/2000, Loss: 0.00023543910356238484\n",
      "Iteration 794/2000, Loss: 0.00026229876675643027\n",
      "Iteration 795/2000, Loss: 0.0003027220082003623\n",
      "Iteration 796/2000, Loss: 0.0002977042749989778\n",
      "Iteration 797/2000, Loss: 0.0001745628542266786\n",
      "Iteration 798/2000, Loss: 0.00019202617113478482\n",
      "Iteration 799/2000, Loss: 0.00013900978956371546\n",
      "Iteration 800/2000, Loss: 0.000748432707041502\n",
      "Iteration 801/2000, Loss: 0.0001605847355676815\n",
      "Iteration 802/2000, Loss: 0.0009400463895872235\n",
      "Iteration 803/2000, Loss: 0.0001345568016404286\n",
      "Iteration 804/2000, Loss: 0.00014489825116470456\n",
      "Iteration 805/2000, Loss: 0.00020246767962817103\n",
      "Iteration 806/2000, Loss: 0.0002580242871772498\n",
      "Iteration 807/2000, Loss: 0.00018763185653369874\n",
      "Iteration 808/2000, Loss: 0.00019517065084073693\n",
      "Iteration 809/2000, Loss: 0.0001515093317721039\n",
      "Iteration 810/2000, Loss: 0.00019833119586110115\n",
      "Iteration 811/2000, Loss: 0.00029366902890615165\n",
      "Iteration 812/2000, Loss: 0.000135901864268817\n",
      "Iteration 813/2000, Loss: 0.0001812666596379131\n",
      "Iteration 814/2000, Loss: 0.00023511216568294913\n",
      "Iteration 815/2000, Loss: 0.0001645297306822613\n",
      "Iteration 816/2000, Loss: 0.00026620435528457165\n",
      "Iteration 817/2000, Loss: 0.00020045314158778638\n",
      "Iteration 818/2000, Loss: 0.00016649323515594006\n",
      "Iteration 819/2000, Loss: 0.0005117267719469965\n",
      "Iteration 820/2000, Loss: 0.000939013552851975\n",
      "Iteration 821/2000, Loss: 0.00024824481806717813\n",
      "Iteration 822/2000, Loss: 0.00012686869013123214\n",
      "Iteration 823/2000, Loss: 0.0004172793996986002\n",
      "Iteration 824/2000, Loss: 0.0002836629864759743\n",
      "Iteration 825/2000, Loss: 0.00021617035963572562\n",
      "Iteration 826/2000, Loss: 0.00015555003483314067\n",
      "Iteration 827/2000, Loss: 0.0001969535369426012\n",
      "Iteration 828/2000, Loss: 0.00017344921070616692\n",
      "Iteration 829/2000, Loss: 0.000327592744724825\n",
      "Iteration 830/2000, Loss: 0.00015173449355643243\n",
      "Iteration 831/2000, Loss: 0.0001752111129462719\n",
      "Iteration 832/2000, Loss: 0.0001735570840537548\n",
      "Iteration 833/2000, Loss: 0.00020256986317690462\n",
      "Iteration 834/2000, Loss: 0.0001714199606794864\n",
      "Iteration 835/2000, Loss: 0.00031900970498099923\n",
      "Iteration 836/2000, Loss: 0.00022556664771400392\n",
      "Iteration 837/2000, Loss: 0.0001786147040547803\n",
      "Iteration 838/2000, Loss: 0.0005646092467941344\n",
      "Iteration 839/2000, Loss: 0.0002460516116116196\n",
      "Iteration 840/2000, Loss: 0.00021523218310903758\n",
      "Iteration 841/2000, Loss: 0.00014690274838358164\n",
      "Iteration 842/2000, Loss: 0.00034152716398239136\n",
      "Iteration 843/2000, Loss: 0.00032078500953502953\n",
      "Iteration 844/2000, Loss: 0.00015285940025933087\n",
      "Iteration 845/2000, Loss: 0.00028331991052255034\n",
      "Iteration 846/2000, Loss: 0.00017865588597487658\n",
      "Iteration 847/2000, Loss: 0.0002109805354848504\n",
      "Iteration 848/2000, Loss: 0.00023202411830425262\n",
      "Iteration 849/2000, Loss: 0.00017866496636997908\n",
      "Iteration 850/2000, Loss: 0.00017857000057119876\n",
      "Iteration 851/2000, Loss: 0.0003005733888130635\n",
      "Iteration 852/2000, Loss: 0.0002852003090083599\n",
      "Iteration 853/2000, Loss: 0.00018404457659926265\n",
      "Iteration 854/2000, Loss: 0.000159348375746049\n",
      "Iteration 855/2000, Loss: 0.00023839414643589407\n",
      "Iteration 856/2000, Loss: 0.0004722783633042127\n",
      "Iteration 857/2000, Loss: 0.0001758440921548754\n",
      "Iteration 858/2000, Loss: 0.0001536268973723054\n",
      "Iteration 859/2000, Loss: 0.0005059691029600799\n",
      "Iteration 860/2000, Loss: 0.0002034333738265559\n",
      "Iteration 861/2000, Loss: 0.00037209971924312413\n",
      "Iteration 862/2000, Loss: 0.00032944156555458903\n",
      "Iteration 863/2000, Loss: 0.00022445507056545466\n",
      "Iteration 864/2000, Loss: 0.0003291986358817667\n",
      "Iteration 865/2000, Loss: 0.00040649194852449\n",
      "Iteration 866/2000, Loss: 0.00027783523546531796\n",
      "Iteration 867/2000, Loss: 0.0002678312303032726\n",
      "Iteration 868/2000, Loss: 0.00021465013560373336\n",
      "Iteration 869/2000, Loss: 0.0002272268320666626\n",
      "Iteration 870/2000, Loss: 0.00020129852055106312\n",
      "Iteration 871/2000, Loss: 0.00018822621495928615\n",
      "Iteration 872/2000, Loss: 0.0005564854945987463\n",
      "Iteration 873/2000, Loss: 0.00022384972544386983\n",
      "Iteration 874/2000, Loss: 0.0002524009323678911\n",
      "Iteration 875/2000, Loss: 0.00027921958826482296\n",
      "Iteration 876/2000, Loss: 0.0004233097133692354\n",
      "Iteration 877/2000, Loss: 0.00024850593763403594\n",
      "Iteration 878/2000, Loss: 0.0003533072303980589\n",
      "Iteration 879/2000, Loss: 0.00020762925851158798\n",
      "Iteration 880/2000, Loss: 0.000148550549056381\n",
      "Iteration 881/2000, Loss: 0.0003527714579831809\n",
      "Iteration 882/2000, Loss: 0.0003972112317569554\n",
      "Iteration 883/2000, Loss: 0.00028213864425197244\n",
      "Iteration 884/2000, Loss: 0.00033928517950698733\n",
      "Iteration 885/2000, Loss: 0.00036070082569494843\n",
      "Iteration 886/2000, Loss: 0.00036030978662893176\n",
      "Iteration 887/2000, Loss: 0.0002780898939818144\n",
      "Iteration 888/2000, Loss: 0.0003095745050814003\n",
      "Iteration 889/2000, Loss: 0.00025943524087779224\n",
      "Iteration 890/2000, Loss: 0.00015373494534287602\n",
      "Iteration 891/2000, Loss: 0.00022676181106362492\n",
      "Iteration 892/2000, Loss: 0.00027413159841671586\n",
      "Iteration 893/2000, Loss: 0.0001852974819485098\n",
      "Iteration 894/2000, Loss: 0.00021979690063744783\n",
      "Iteration 895/2000, Loss: 0.00033516998519189656\n",
      "Iteration 896/2000, Loss: 0.00012927051284350455\n",
      "Iteration 897/2000, Loss: 0.000156711510499008\n",
      "Iteration 898/2000, Loss: 0.00038572592893615365\n",
      "Iteration 899/2000, Loss: 0.0001208069224958308\n",
      "Iteration 900/2000, Loss: 0.00032054114853963256\n",
      "Iteration 901/2000, Loss: 0.00016745011089369655\n",
      "Iteration 902/2000, Loss: 0.0001475851604482159\n",
      "Iteration 903/2000, Loss: 0.00022967175755184144\n",
      "Iteration 904/2000, Loss: 0.00028223992558196187\n",
      "Iteration 905/2000, Loss: 0.00012979161692783237\n",
      "Iteration 906/2000, Loss: 0.00018701139197219163\n",
      "Iteration 907/2000, Loss: 0.00019825453637167811\n",
      "Iteration 908/2000, Loss: 0.00012871174840256572\n",
      "Iteration 909/2000, Loss: 9.063524339580908e-05\n",
      "Iteration 910/2000, Loss: 0.00011193518730578944\n",
      "Iteration 911/2000, Loss: 0.0002144810714526102\n",
      "Iteration 912/2000, Loss: 0.00018399803957436234\n",
      "Iteration 913/2000, Loss: 0.0003982062335126102\n",
      "Iteration 914/2000, Loss: 0.0001670768397161737\n",
      "Iteration 915/2000, Loss: 0.00018208498659078032\n",
      "Iteration 916/2000, Loss: 0.0001803948835004121\n",
      "Iteration 917/2000, Loss: 0.000323132990160957\n",
      "Iteration 918/2000, Loss: 0.00026361001073382795\n",
      "Iteration 919/2000, Loss: 0.00016676266386639327\n",
      "Iteration 920/2000, Loss: 0.00018694355094339699\n",
      "Iteration 921/2000, Loss: 0.00016335972759407014\n",
      "Iteration 922/2000, Loss: 0.00017822789959609509\n",
      "Iteration 923/2000, Loss: 0.00015831945347599685\n",
      "Iteration 924/2000, Loss: 0.0002656894503161311\n",
      "Iteration 925/2000, Loss: 0.00030721453367732465\n",
      "Iteration 926/2000, Loss: 0.00028513482538983226\n",
      "Iteration 927/2000, Loss: 0.0003442823071964085\n",
      "Iteration 928/2000, Loss: 0.00018817638920154423\n",
      "Iteration 929/2000, Loss: 0.00016171508468687534\n",
      "Iteration 930/2000, Loss: 0.0003366961027495563\n",
      "Iteration 931/2000, Loss: 0.00015043721941765398\n",
      "Iteration 932/2000, Loss: 0.00018073426326736808\n",
      "Iteration 933/2000, Loss: 0.00018779616220854223\n",
      "Iteration 934/2000, Loss: 0.0001796170399757102\n",
      "Iteration 935/2000, Loss: 0.000312872783979401\n",
      "Iteration 936/2000, Loss: 0.00036058807745575905\n",
      "Iteration 937/2000, Loss: 0.000271906639682129\n",
      "Iteration 938/2000, Loss: 0.00011645720223896205\n",
      "Iteration 939/2000, Loss: 0.00027772309840656817\n",
      "Iteration 940/2000, Loss: 0.00019171784515492618\n",
      "Iteration 941/2000, Loss: 0.0001664571900619194\n",
      "Iteration 942/2000, Loss: 0.00022533058654516935\n",
      "Iteration 943/2000, Loss: 0.00015526163042522967\n",
      "Iteration 944/2000, Loss: 0.00029849406564608216\n",
      "Iteration 945/2000, Loss: 0.00031102271168492734\n",
      "Iteration 946/2000, Loss: 0.0001053353917086497\n",
      "Iteration 947/2000, Loss: 0.0002358488127356395\n",
      "Iteration 948/2000, Loss: 0.0003230369184166193\n",
      "Iteration 949/2000, Loss: 0.0002201664901804179\n",
      "Iteration 950/2000, Loss: 0.00040483244811184704\n",
      "Iteration 951/2000, Loss: 0.00012796171358786523\n",
      "Iteration 952/2000, Loss: 0.00037303552380762994\n",
      "Iteration 953/2000, Loss: 0.00018147855007555336\n",
      "Iteration 954/2000, Loss: 0.0002458880189806223\n",
      "Iteration 955/2000, Loss: 0.0003385237359907478\n",
      "Iteration 956/2000, Loss: 0.00029084846028126776\n",
      "Iteration 957/2000, Loss: 0.0003394329105503857\n",
      "Iteration 958/2000, Loss: 0.00017581082647666335\n",
      "Iteration 959/2000, Loss: 0.00026347977109253407\n",
      "Iteration 960/2000, Loss: 0.00025569848367013037\n",
      "Iteration 961/2000, Loss: 0.0002483045682311058\n",
      "Iteration 962/2000, Loss: 0.0003605514648370445\n",
      "Iteration 963/2000, Loss: 0.00014809494314249605\n",
      "Iteration 964/2000, Loss: 0.00027071445947512984\n",
      "Iteration 965/2000, Loss: 0.0002957358374260366\n",
      "Iteration 966/2000, Loss: 0.000281099317362532\n",
      "Iteration 967/2000, Loss: 0.0002506229793652892\n",
      "Iteration 968/2000, Loss: 0.0003198663762304932\n",
      "Iteration 969/2000, Loss: 0.00038881434011273086\n",
      "Iteration 970/2000, Loss: 0.00035697591374628246\n",
      "Iteration 971/2000, Loss: 0.0002654003328643739\n",
      "Iteration 972/2000, Loss: 0.00035759908496402204\n",
      "Iteration 973/2000, Loss: 0.00031408449285663664\n",
      "Iteration 974/2000, Loss: 0.00036089029163122177\n",
      "Iteration 975/2000, Loss: 0.00043213015305809677\n",
      "Iteration 976/2000, Loss: 0.00045139112626202404\n",
      "Iteration 977/2000, Loss: 0.0003196696925442666\n",
      "Iteration 978/2000, Loss: 0.00012197499017929658\n",
      "Iteration 979/2000, Loss: 0.00045213819248601794\n",
      "Iteration 980/2000, Loss: 0.0003263652906753123\n",
      "Iteration 981/2000, Loss: 0.00022709583572577685\n",
      "Iteration 982/2000, Loss: 0.000508392695337534\n",
      "Iteration 983/2000, Loss: 0.00016688126197550446\n",
      "Iteration 984/2000, Loss: 0.0003696803469210863\n",
      "Iteration 985/2000, Loss: 0.0003356230154167861\n",
      "Iteration 986/2000, Loss: 0.0006037128623574972\n",
      "Iteration 987/2000, Loss: 0.00032630490022711456\n",
      "Iteration 988/2000, Loss: 0.00029279853333719075\n",
      "Iteration 989/2000, Loss: 0.0003746205766219646\n",
      "Iteration 990/2000, Loss: 0.00018744519911706448\n",
      "Iteration 991/2000, Loss: 0.0002776357578113675\n",
      "Iteration 992/2000, Loss: 0.0002974557282868773\n",
      "Iteration 993/2000, Loss: 0.0005041917320340872\n",
      "Iteration 994/2000, Loss: 0.0002680458128452301\n",
      "Iteration 995/2000, Loss: 0.00018475543765816838\n",
      "Iteration 996/2000, Loss: 0.0002767257101368159\n",
      "Iteration 997/2000, Loss: 0.00014571566134691238\n",
      "Iteration 998/2000, Loss: 0.00023984081053640693\n",
      "Iteration 999/2000, Loss: 0.00019465110381133854\n",
      "Iteration 1000/2000, Loss: 0.00021449875202961266\n",
      "Iteration 1001/2000, Loss: 0.0005200578016228974\n",
      "Iteration 1002/2000, Loss: 0.000196759938262403\n",
      "Iteration 1003/2000, Loss: 0.000259529915638268\n",
      "Iteration 1004/2000, Loss: 0.00028956652386114\n",
      "Iteration 1005/2000, Loss: 0.0002712296845857054\n",
      "Iteration 1006/2000, Loss: 0.00027087610214948654\n",
      "Iteration 1007/2000, Loss: 0.00024087967176456004\n",
      "Iteration 1008/2000, Loss: 0.0008406573324464262\n",
      "Iteration 1009/2000, Loss: 0.00021701607329305261\n",
      "Iteration 1010/2000, Loss: 0.00021583748457487673\n",
      "Iteration 1011/2000, Loss: 0.0006846854230388999\n",
      "Iteration 1012/2000, Loss: 0.00022135768085718155\n",
      "Iteration 1013/2000, Loss: 0.00032900439691729844\n",
      "Iteration 1014/2000, Loss: 0.0005172562086954713\n",
      "Iteration 1015/2000, Loss: 0.0003902741300407797\n",
      "Iteration 1016/2000, Loss: 0.0004603833076544106\n",
      "Iteration 1017/2000, Loss: 0.0002438957744743675\n",
      "Iteration 1018/2000, Loss: 0.0003423172456678003\n",
      "Iteration 1019/2000, Loss: 0.00014133728109300137\n",
      "Iteration 1020/2000, Loss: 0.0003340044931974262\n",
      "Iteration 1021/2000, Loss: 0.0002993405214510858\n",
      "Iteration 1022/2000, Loss: 0.00039093499071896076\n",
      "Iteration 1023/2000, Loss: 0.00032478879438713193\n",
      "Iteration 1024/2000, Loss: 0.00033143311156891286\n",
      "Iteration 1025/2000, Loss: 0.00023110928304959089\n",
      "Iteration 1026/2000, Loss: 0.00021553062833845615\n",
      "Iteration 1027/2000, Loss: 0.0001691158104222268\n",
      "Iteration 1028/2000, Loss: 0.00016177675570361316\n",
      "Iteration 1029/2000, Loss: 0.00023686322674620897\n",
      "Iteration 1030/2000, Loss: 0.00019701183191500604\n",
      "Iteration 1031/2000, Loss: 0.00021062993619125336\n",
      "Iteration 1032/2000, Loss: 0.00025101524079218507\n",
      "Iteration 1033/2000, Loss: 0.00026133094797842205\n",
      "Iteration 1034/2000, Loss: 0.00025860327878035605\n",
      "Iteration 1035/2000, Loss: 0.0002117407857440412\n",
      "Iteration 1036/2000, Loss: 0.000215489199035801\n",
      "Iteration 1037/2000, Loss: 0.00018768315203487873\n",
      "Iteration 1038/2000, Loss: 0.00026239868020638824\n",
      "Iteration 1039/2000, Loss: 0.00022412906400859356\n",
      "Iteration 1040/2000, Loss: 0.00023307601804845035\n",
      "Iteration 1041/2000, Loss: 0.00026902894023805857\n",
      "Iteration 1042/2000, Loss: 0.00034636384225450456\n",
      "Iteration 1043/2000, Loss: 0.00015976002032402903\n",
      "Iteration 1044/2000, Loss: 0.00021656167518813163\n",
      "Iteration 1045/2000, Loss: 0.00013456324813887477\n",
      "Iteration 1046/2000, Loss: 0.000195049011381343\n",
      "Iteration 1047/2000, Loss: 0.00023759747273288667\n",
      "Iteration 1048/2000, Loss: 0.0002779706846922636\n",
      "Iteration 1049/2000, Loss: 0.0003289507294539362\n",
      "Iteration 1050/2000, Loss: 0.0002117387775797397\n",
      "Iteration 1051/2000, Loss: 0.0002166635968023911\n",
      "Iteration 1052/2000, Loss: 0.00016872920969035476\n",
      "Iteration 1053/2000, Loss: 0.0002591306692920625\n",
      "Iteration 1054/2000, Loss: 0.0001874297158792615\n",
      "Iteration 1055/2000, Loss: 0.00022301929129753262\n",
      "Iteration 1056/2000, Loss: 0.00015681244258303195\n",
      "Iteration 1057/2000, Loss: 0.00017943790589924902\n",
      "Iteration 1058/2000, Loss: 0.000127944047562778\n",
      "Iteration 1059/2000, Loss: 0.0002176061097998172\n",
      "Iteration 1060/2000, Loss: 0.00022526252723764628\n",
      "Iteration 1061/2000, Loss: 0.00014784130326006562\n",
      "Iteration 1062/2000, Loss: 0.00022863711637910455\n",
      "Iteration 1063/2000, Loss: 0.0001463868102291599\n",
      "Iteration 1064/2000, Loss: 0.00016480291378684342\n",
      "Iteration 1065/2000, Loss: 0.00017970456974580884\n",
      "Iteration 1066/2000, Loss: 0.00012581085320562124\n",
      "Iteration 1067/2000, Loss: 0.00024508521892130375\n",
      "Iteration 1068/2000, Loss: 0.0002007609436986968\n",
      "Iteration 1069/2000, Loss: 0.00023130665067583323\n",
      "Iteration 1070/2000, Loss: 0.00017805740935727954\n",
      "Iteration 1071/2000, Loss: 0.00039342124364338815\n",
      "Iteration 1072/2000, Loss: 0.00021951721282675862\n",
      "Iteration 1073/2000, Loss: 0.0003115159925073385\n",
      "Iteration 1074/2000, Loss: 0.00033961847657337785\n",
      "Iteration 1075/2000, Loss: 0.0003021600132342428\n",
      "Iteration 1076/2000, Loss: 0.00028259807731956244\n",
      "Iteration 1077/2000, Loss: 0.00020610811770893633\n",
      "Iteration 1078/2000, Loss: 0.0002810125006362796\n",
      "Iteration 1079/2000, Loss: 0.0005052177584730089\n",
      "Iteration 1080/2000, Loss: 0.0001984781993087381\n",
      "Iteration 1081/2000, Loss: 0.00033523773890919983\n",
      "Iteration 1082/2000, Loss: 0.00021811050828546286\n",
      "Iteration 1083/2000, Loss: 0.0003633306478150189\n",
      "Iteration 1084/2000, Loss: 0.0003978866443503648\n",
      "Iteration 1085/2000, Loss: 0.0003588206018321216\n",
      "Iteration 1086/2000, Loss: 0.0002211225073551759\n",
      "Iteration 1087/2000, Loss: 0.0005202782340347767\n",
      "Iteration 1088/2000, Loss: 0.0003945058269891888\n",
      "Iteration 1089/2000, Loss: 0.0006250871228985488\n",
      "Iteration 1090/2000, Loss: 0.00042131892405450344\n",
      "Iteration 1091/2000, Loss: 0.00027057385887019336\n",
      "Iteration 1092/2000, Loss: 0.00023429692373611033\n",
      "Iteration 1093/2000, Loss: 0.0003077498113270849\n",
      "Iteration 1094/2000, Loss: 0.00027440322446636856\n",
      "Iteration 1095/2000, Loss: 0.00025464114150963724\n",
      "Iteration 1096/2000, Loss: 0.00030435799271799624\n",
      "Iteration 1097/2000, Loss: 0.0004011050332337618\n",
      "Iteration 1098/2000, Loss: 0.0002109923807438463\n",
      "Iteration 1099/2000, Loss: 0.00039088085759431124\n",
      "Iteration 1100/2000, Loss: 0.0002946711902040988\n",
      "Iteration 1101/2000, Loss: 0.00016930812853388488\n",
      "Iteration 1102/2000, Loss: 0.0003625784011092037\n",
      "Iteration 1103/2000, Loss: 0.00018797288066707551\n",
      "Iteration 1104/2000, Loss: 0.0002274074504384771\n",
      "Iteration 1105/2000, Loss: 0.0001904944219859317\n",
      "Iteration 1106/2000, Loss: 0.00022457865998148918\n",
      "Iteration 1107/2000, Loss: 0.00046088811359368265\n",
      "Iteration 1108/2000, Loss: 0.0002138876443495974\n",
      "Iteration 1109/2000, Loss: 0.00012842696742154658\n",
      "Iteration 1110/2000, Loss: 0.00016509488341398537\n",
      "Iteration 1111/2000, Loss: 0.00037144767702557147\n",
      "Iteration 1112/2000, Loss: 0.0002174210239900276\n",
      "Iteration 1113/2000, Loss: 0.00017502406262792647\n",
      "Iteration 1114/2000, Loss: 0.00029619623092003167\n",
      "Iteration 1115/2000, Loss: 0.00020029225561302155\n",
      "Iteration 1116/2000, Loss: 0.00023313860583584756\n",
      "Iteration 1117/2000, Loss: 0.00018926331540569663\n",
      "Iteration 1118/2000, Loss: 0.00030826791771687567\n",
      "Iteration 1119/2000, Loss: 0.00020329089602455497\n",
      "Iteration 1120/2000, Loss: 0.00013881763152312487\n",
      "Iteration 1121/2000, Loss: 0.00023522396804764867\n",
      "Iteration 1122/2000, Loss: 0.0001980099914362654\n",
      "Iteration 1123/2000, Loss: 0.00022967005497775972\n",
      "Iteration 1124/2000, Loss: 0.00017323136853519827\n",
      "Iteration 1125/2000, Loss: 0.00028779765125364065\n",
      "Iteration 1126/2000, Loss: 0.000388230022508651\n",
      "Iteration 1127/2000, Loss: 0.00041849917033687234\n",
      "Iteration 1128/2000, Loss: 0.0001637502427911386\n",
      "Iteration 1129/2000, Loss: 0.0002933751093223691\n",
      "Iteration 1130/2000, Loss: 0.00032924386323429644\n",
      "Iteration 1131/2000, Loss: 0.00011458538938313723\n",
      "Iteration 1132/2000, Loss: 0.00020542625861708075\n",
      "Iteration 1133/2000, Loss: 0.00027438491815701127\n",
      "Iteration 1134/2000, Loss: 0.00027906204923056066\n",
      "Iteration 1135/2000, Loss: 0.00041462897206656635\n",
      "Iteration 1136/2000, Loss: 0.00017174553067889065\n",
      "Iteration 1137/2000, Loss: 0.00034018117003142834\n",
      "Iteration 1138/2000, Loss: 0.00019047243404202163\n",
      "Iteration 1139/2000, Loss: 0.00027489123749546707\n",
      "Iteration 1140/2000, Loss: 0.00038732108077965677\n",
      "Iteration 1141/2000, Loss: 0.00018916117551270872\n",
      "Iteration 1142/2000, Loss: 0.0002978639386128634\n",
      "Iteration 1143/2000, Loss: 0.0005457571824081242\n",
      "Iteration 1144/2000, Loss: 0.00033524641185067594\n",
      "Iteration 1145/2000, Loss: 0.0001307258353335783\n",
      "Iteration 1146/2000, Loss: 0.00017302825290244073\n",
      "Iteration 1147/2000, Loss: 0.0001736569101922214\n",
      "Iteration 1148/2000, Loss: 0.00013702129945158958\n",
      "Iteration 1149/2000, Loss: 0.00020345582743175328\n",
      "Iteration 1150/2000, Loss: 0.0001857404422480613\n",
      "Iteration 1151/2000, Loss: 0.00016573487664572895\n",
      "Iteration 1152/2000, Loss: 0.00026419811183586717\n",
      "Iteration 1153/2000, Loss: 0.0001773805561242625\n",
      "Iteration 1154/2000, Loss: 0.00017842004308477044\n",
      "Iteration 1155/2000, Loss: 0.00022385992633644491\n",
      "Iteration 1156/2000, Loss: 0.0001797308650566265\n",
      "Iteration 1157/2000, Loss: 0.00012600593618117273\n",
      "Iteration 1158/2000, Loss: 0.0001455540332244709\n",
      "Iteration 1159/2000, Loss: 0.00014787704276386648\n",
      "Iteration 1160/2000, Loss: 0.00016080022032838315\n",
      "Iteration 1161/2000, Loss: 0.00020821904763579369\n",
      "Iteration 1162/2000, Loss: 0.00016436268924735487\n",
      "Iteration 1163/2000, Loss: 0.00010366452625021338\n",
      "Iteration 1164/2000, Loss: 0.00020890214364044368\n",
      "Iteration 1165/2000, Loss: 0.0002097034448524937\n",
      "Iteration 1166/2000, Loss: 0.00011290198744973168\n",
      "Iteration 1167/2000, Loss: 0.0002416299539618194\n",
      "Iteration 1168/2000, Loss: 0.00011813394667115062\n",
      "Iteration 1169/2000, Loss: 0.00013267142639961094\n",
      "Iteration 1170/2000, Loss: 0.0001336755813099444\n",
      "Iteration 1171/2000, Loss: 0.00015481228183489293\n",
      "Iteration 1172/2000, Loss: 0.00010414494317956269\n",
      "Iteration 1173/2000, Loss: 0.00012747992877848446\n",
      "Iteration 1174/2000, Loss: 0.00012901259469799697\n",
      "Iteration 1175/2000, Loss: 0.00020037885406054556\n",
      "Iteration 1176/2000, Loss: 0.00046393307275138795\n",
      "Iteration 1177/2000, Loss: 0.00015653976879548281\n",
      "Iteration 1178/2000, Loss: 0.00013305064931046218\n",
      "Iteration 1179/2000, Loss: 9.052451787283644e-05\n",
      "Iteration 1180/2000, Loss: 0.000133331268443726\n",
      "Iteration 1181/2000, Loss: 0.00017063731502275914\n",
      "Iteration 1182/2000, Loss: 0.00011365480168024078\n",
      "Iteration 1183/2000, Loss: 0.0002332095173187554\n",
      "Iteration 1184/2000, Loss: 0.00018965530034620315\n",
      "Iteration 1185/2000, Loss: 0.00018167660164181143\n",
      "Iteration 1186/2000, Loss: 0.0002502174465917051\n",
      "Iteration 1187/2000, Loss: 0.0001852360728662461\n",
      "Iteration 1188/2000, Loss: 0.00017540786939207464\n",
      "Iteration 1189/2000, Loss: 0.00022439112944994122\n",
      "Iteration 1190/2000, Loss: 0.00014708261005580425\n",
      "Iteration 1191/2000, Loss: 0.00022738623374607414\n",
      "Iteration 1192/2000, Loss: 0.0002287046954734251\n",
      "Iteration 1193/2000, Loss: 0.00015054138202685863\n",
      "Iteration 1194/2000, Loss: 0.00024069318897090852\n",
      "Iteration 1195/2000, Loss: 0.00013035025040153414\n",
      "Iteration 1196/2000, Loss: 0.00019384731422178447\n",
      "Iteration 1197/2000, Loss: 0.00016068413970060647\n",
      "Iteration 1198/2000, Loss: 0.00017675010894890875\n",
      "Iteration 1199/2000, Loss: 0.00015677357441745698\n",
      "Iteration 1200/2000, Loss: 0.00015624481602571905\n",
      "Iteration 1201/2000, Loss: 0.0002027201117016375\n",
      "Iteration 1202/2000, Loss: 0.00043144417577423155\n",
      "Iteration 1203/2000, Loss: 0.00011649453517748043\n",
      "Iteration 1204/2000, Loss: 0.00014844107499811798\n",
      "Iteration 1205/2000, Loss: 0.0001866717211669311\n",
      "Iteration 1206/2000, Loss: 0.00021512444072868675\n",
      "Iteration 1207/2000, Loss: 0.00013584679982159287\n",
      "Iteration 1208/2000, Loss: 0.0002642592880874872\n",
      "Iteration 1209/2000, Loss: 0.0005743792280554771\n",
      "Iteration 1210/2000, Loss: 0.0003510302340146154\n",
      "Iteration 1211/2000, Loss: 0.0002564493624959141\n",
      "Iteration 1212/2000, Loss: 0.00015678534691687673\n",
      "Iteration 1213/2000, Loss: 0.0003356129745952785\n",
      "Iteration 1214/2000, Loss: 0.00018285008263774216\n",
      "Iteration 1215/2000, Loss: 0.00016755887190811336\n",
      "Iteration 1216/2000, Loss: 0.00016640641842968762\n",
      "Iteration 1217/2000, Loss: 0.00014755416486877948\n",
      "Iteration 1218/2000, Loss: 0.0002668546512722969\n",
      "Iteration 1219/2000, Loss: 0.00030322879320010543\n",
      "Iteration 1220/2000, Loss: 0.00023411112488247454\n",
      "Iteration 1221/2000, Loss: 0.00020123524882365018\n",
      "Iteration 1222/2000, Loss: 0.0003774754877667874\n",
      "Iteration 1223/2000, Loss: 0.00028098735492676497\n",
      "Iteration 1224/2000, Loss: 0.00025164836551994085\n",
      "Iteration 1225/2000, Loss: 0.0003349148028064519\n",
      "Iteration 1226/2000, Loss: 0.00018691111472435296\n",
      "Iteration 1227/2000, Loss: 0.00019099075871054083\n",
      "Iteration 1228/2000, Loss: 0.00024110478989314288\n",
      "Iteration 1229/2000, Loss: 0.00021703691163565964\n",
      "Iteration 1230/2000, Loss: 0.000320902734529227\n",
      "Iteration 1231/2000, Loss: 0.00032084769918583333\n",
      "Iteration 1232/2000, Loss: 0.00031366609618999064\n",
      "Iteration 1233/2000, Loss: 0.0003309704188723117\n",
      "Iteration 1234/2000, Loss: 0.00016967773262877017\n",
      "Iteration 1235/2000, Loss: 0.00017851439770311117\n",
      "Iteration 1236/2000, Loss: 0.00015491069643758237\n",
      "Iteration 1237/2000, Loss: 0.000217311957385391\n",
      "Iteration 1238/2000, Loss: 0.0001702059671515599\n",
      "Iteration 1239/2000, Loss: 0.0002463841810822487\n",
      "Iteration 1240/2000, Loss: 0.0001345990167465061\n",
      "Iteration 1241/2000, Loss: 0.00022031052503734827\n",
      "Iteration 1242/2000, Loss: 0.00018277644994668663\n",
      "Iteration 1243/2000, Loss: 0.00019901547057088464\n",
      "Iteration 1244/2000, Loss: 0.00014090104377828538\n",
      "Iteration 1245/2000, Loss: 0.0001317966525675729\n",
      "Iteration 1246/2000, Loss: 0.00017814549210015684\n",
      "Iteration 1247/2000, Loss: 0.00021559357992373407\n",
      "Iteration 1248/2000, Loss: 0.00021328389993868768\n",
      "Iteration 1249/2000, Loss: 0.0002085036103380844\n",
      "Iteration 1250/2000, Loss: 0.00015876399993430823\n",
      "Iteration 1251/2000, Loss: 0.0001657945103943348\n",
      "Iteration 1252/2000, Loss: 0.00020425896218512207\n",
      "Iteration 1253/2000, Loss: 0.000472657207865268\n",
      "Iteration 1254/2000, Loss: 0.0001626082230359316\n",
      "Iteration 1255/2000, Loss: 0.00016098172636702657\n",
      "Iteration 1256/2000, Loss: 0.00017637356359045953\n",
      "Iteration 1257/2000, Loss: 0.00016605433484073728\n",
      "Iteration 1258/2000, Loss: 0.00035945430863648653\n",
      "Iteration 1259/2000, Loss: 0.00018125379574485123\n",
      "Iteration 1260/2000, Loss: 0.00022427875956054777\n",
      "Iteration 1261/2000, Loss: 0.0002692521666176617\n",
      "Iteration 1262/2000, Loss: 0.00027926138136535883\n",
      "Iteration 1263/2000, Loss: 0.00012976974539924413\n",
      "Iteration 1264/2000, Loss: 0.00012418994447216392\n",
      "Iteration 1265/2000, Loss: 0.00021338734950404614\n",
      "Iteration 1266/2000, Loss: 0.00039517617551609874\n",
      "Iteration 1267/2000, Loss: 0.00017978210235014558\n",
      "Iteration 1268/2000, Loss: 0.00019344333850312978\n",
      "Iteration 1269/2000, Loss: 0.00016480140038765967\n",
      "Iteration 1270/2000, Loss: 0.00015676980547141284\n",
      "Iteration 1271/2000, Loss: 0.00023312187113333493\n",
      "Iteration 1272/2000, Loss: 0.00022501034254673868\n",
      "Iteration 1273/2000, Loss: 0.00018236534378957003\n",
      "Iteration 1274/2000, Loss: 0.0004177095543127507\n",
      "Iteration 1275/2000, Loss: 0.00021933784591965377\n",
      "Iteration 1276/2000, Loss: 0.00014807761181145906\n",
      "Iteration 1277/2000, Loss: 0.00020862440578639507\n",
      "Iteration 1278/2000, Loss: 0.00021170408581383526\n",
      "Iteration 1279/2000, Loss: 0.00014471809845417738\n",
      "Iteration 1280/2000, Loss: 0.00021740190277341753\n",
      "Iteration 1281/2000, Loss: 0.00033731854637153447\n",
      "Iteration 1282/2000, Loss: 0.0002595323894638568\n",
      "Iteration 1283/2000, Loss: 0.00016223273996729404\n",
      "Iteration 1284/2000, Loss: 0.0001896815956570208\n",
      "Iteration 1285/2000, Loss: 0.00012964663619641215\n",
      "Iteration 1286/2000, Loss: 0.00020240750745870173\n",
      "Iteration 1287/2000, Loss: 0.0001486220135120675\n",
      "Iteration 1288/2000, Loss: 0.00020799141202587634\n",
      "Iteration 1289/2000, Loss: 0.0001619948452571407\n",
      "Iteration 1290/2000, Loss: 0.00012948605581186712\n",
      "Iteration 1291/2000, Loss: 0.0002104623563354835\n",
      "Iteration 1292/2000, Loss: 0.0002165662735933438\n",
      "Iteration 1293/2000, Loss: 0.00016627517470624298\n",
      "Iteration 1294/2000, Loss: 0.00014487604494206607\n",
      "Iteration 1295/2000, Loss: 0.00014409817231353372\n",
      "Iteration 1296/2000, Loss: 0.00015673540474381298\n",
      "Iteration 1297/2000, Loss: 0.00020177889382466674\n",
      "Iteration 1298/2000, Loss: 0.00020962920098099858\n",
      "Iteration 1299/2000, Loss: 0.00019962001533713192\n",
      "Iteration 1300/2000, Loss: 0.00032427298719994724\n",
      "Iteration 1301/2000, Loss: 0.00029406900284811854\n",
      "Iteration 1302/2000, Loss: 0.00021688867127522826\n",
      "Iteration 1303/2000, Loss: 0.000241312402067706\n",
      "Iteration 1304/2000, Loss: 0.00015663669910281897\n",
      "Iteration 1305/2000, Loss: 0.0001351631508441642\n",
      "Iteration 1306/2000, Loss: 0.00019567053823266178\n",
      "Iteration 1307/2000, Loss: 0.00024034027592279017\n",
      "Iteration 1308/2000, Loss: 0.00017643699538893998\n",
      "Iteration 1309/2000, Loss: 0.00027223996585235\n",
      "Iteration 1310/2000, Loss: 0.000355121010215953\n",
      "Iteration 1311/2000, Loss: 0.00019514383166097105\n",
      "Iteration 1312/2000, Loss: 0.0002781864895951003\n",
      "Iteration 1313/2000, Loss: 0.0002285715308971703\n",
      "Iteration 1314/2000, Loss: 0.00028607729473151267\n",
      "Iteration 1315/2000, Loss: 0.0003379030094947666\n",
      "Iteration 1316/2000, Loss: 0.00026447573327459395\n",
      "Iteration 1317/2000, Loss: 0.000365971471183002\n",
      "Iteration 1318/2000, Loss: 0.00020518084056675434\n",
      "Iteration 1319/2000, Loss: 0.00026996954693458974\n",
      "Iteration 1320/2000, Loss: 0.00014108102186582983\n",
      "Iteration 1321/2000, Loss: 0.00028896023286506534\n",
      "Iteration 1322/2000, Loss: 0.0002423645491944626\n",
      "Iteration 1323/2000, Loss: 0.00039293235749937594\n",
      "Iteration 1324/2000, Loss: 0.00016548945859540254\n",
      "Iteration 1325/2000, Loss: 0.00026078452356159687\n",
      "Iteration 1326/2000, Loss: 0.0004693558730650693\n",
      "Iteration 1327/2000, Loss: 0.0002493590291123837\n",
      "Iteration 1328/2000, Loss: 0.0002165139012504369\n",
      "Iteration 1329/2000, Loss: 0.00021254096645861864\n",
      "Iteration 1330/2000, Loss: 0.0003810579946730286\n",
      "Iteration 1331/2000, Loss: 0.0003771406481973827\n",
      "Iteration 1332/2000, Loss: 0.00039938907139003277\n",
      "Iteration 1333/2000, Loss: 0.0004754169494844973\n",
      "Iteration 1334/2000, Loss: 0.00018080251174978912\n",
      "Iteration 1335/2000, Loss: 0.0003022316377609968\n",
      "Iteration 1336/2000, Loss: 0.00019496172899380326\n",
      "Iteration 1337/2000, Loss: 0.00033329977304674685\n",
      "Iteration 1338/2000, Loss: 0.00028069299878552556\n",
      "Iteration 1339/2000, Loss: 0.00016975846665445715\n",
      "Iteration 1340/2000, Loss: 0.00016072814469225705\n",
      "Iteration 1341/2000, Loss: 0.00020494262571446598\n",
      "Iteration 1342/2000, Loss: 0.0003172239812556654\n",
      "Iteration 1343/2000, Loss: 0.0002175591653212905\n",
      "Iteration 1344/2000, Loss: 0.0001694809616310522\n",
      "Iteration 1345/2000, Loss: 0.0002105277089867741\n",
      "Iteration 1346/2000, Loss: 0.00023620622232556343\n",
      "Iteration 1347/2000, Loss: 0.0002218163717770949\n",
      "Iteration 1348/2000, Loss: 0.00032455759355798364\n",
      "Iteration 1349/2000, Loss: 0.00014989756164141\n",
      "Iteration 1350/2000, Loss: 0.00020318804308772087\n",
      "Iteration 1351/2000, Loss: 0.00028184751863591373\n",
      "Iteration 1352/2000, Loss: 0.0001089240686269477\n",
      "Iteration 1353/2000, Loss: 0.0001890497369458899\n",
      "Iteration 1354/2000, Loss: 0.00017522981215734035\n",
      "Iteration 1355/2000, Loss: 0.0001588046579854563\n",
      "Iteration 1356/2000, Loss: 0.00018393434584140778\n",
      "Iteration 1357/2000, Loss: 0.00018740579253062606\n",
      "Iteration 1358/2000, Loss: 0.00042657184530980885\n",
      "Iteration 1359/2000, Loss: 0.00022420301684178412\n",
      "Iteration 1360/2000, Loss: 0.0002614222757983953\n",
      "Iteration 1361/2000, Loss: 0.00011565857857931405\n",
      "Iteration 1362/2000, Loss: 0.00012315691856201738\n",
      "Iteration 1363/2000, Loss: 0.00017993133224081248\n",
      "Iteration 1364/2000, Loss: 0.00016255752416327596\n",
      "Iteration 1365/2000, Loss: 0.00026689996593631804\n",
      "Iteration 1366/2000, Loss: 0.00012632180005311966\n",
      "Iteration 1367/2000, Loss: 9.909382788464427e-05\n",
      "Iteration 1368/2000, Loss: 0.00018852032371796668\n",
      "Iteration 1369/2000, Loss: 0.00018914483371190727\n",
      "Iteration 1370/2000, Loss: 9.939341543940827e-05\n",
      "Iteration 1371/2000, Loss: 0.00015710679872427136\n",
      "Iteration 1372/2000, Loss: 0.0001523636601632461\n",
      "Iteration 1373/2000, Loss: 0.000515529653057456\n",
      "Iteration 1374/2000, Loss: 0.00022790560615248978\n",
      "Iteration 1375/2000, Loss: 0.0002121798024745658\n",
      "Iteration 1376/2000, Loss: 0.00013513844169210643\n",
      "Iteration 1377/2000, Loss: 0.00015177347813732922\n",
      "Iteration 1378/2000, Loss: 0.000309725699480623\n",
      "Iteration 1379/2000, Loss: 0.00013753754319623113\n",
      "Iteration 1380/2000, Loss: 0.00013860764738637954\n",
      "Iteration 1381/2000, Loss: 0.00020514536299742758\n",
      "Iteration 1382/2000, Loss: 0.0001527237327536568\n",
      "Iteration 1383/2000, Loss: 0.00016467165551148355\n",
      "Iteration 1384/2000, Loss: 0.00014969846233725548\n",
      "Iteration 1385/2000, Loss: 0.00015696045011281967\n",
      "Iteration 1386/2000, Loss: 0.00017660866433288902\n",
      "Iteration 1387/2000, Loss: 0.00011089055624324828\n",
      "Iteration 1388/2000, Loss: 0.0001401530607836321\n",
      "Iteration 1389/2000, Loss: 0.0001576195063535124\n",
      "Iteration 1390/2000, Loss: 0.00016885946388356388\n",
      "Iteration 1391/2000, Loss: 0.0007978781941346824\n",
      "Iteration 1392/2000, Loss: 0.0001303571043536067\n",
      "Iteration 1393/2000, Loss: 0.00022982282098382711\n",
      "Iteration 1394/2000, Loss: 0.0002550955396145582\n",
      "Iteration 1395/2000, Loss: 0.0001308925566263497\n",
      "Iteration 1396/2000, Loss: 0.00024816280347295105\n",
      "Iteration 1397/2000, Loss: 0.0002531589416321367\n",
      "Iteration 1398/2000, Loss: 0.00019408912339713424\n",
      "Iteration 1399/2000, Loss: 0.00019138166680932045\n",
      "Iteration 1400/2000, Loss: 0.00011329285916872323\n",
      "Iteration 1401/2000, Loss: 0.00017080490943044424\n",
      "Iteration 1402/2000, Loss: 0.00018486259796191007\n",
      "Iteration 1403/2000, Loss: 0.00014821837248746306\n",
      "Iteration 1404/2000, Loss: 0.00015245842223521322\n",
      "Iteration 1405/2000, Loss: 0.00017850313452072442\n",
      "Iteration 1406/2000, Loss: 0.0001452100113965571\n",
      "Iteration 1407/2000, Loss: 0.000357096956577152\n",
      "Iteration 1408/2000, Loss: 0.00014144858869258314\n",
      "Iteration 1409/2000, Loss: 0.00016272415814455599\n",
      "Iteration 1410/2000, Loss: 0.00025855156127363443\n",
      "Iteration 1411/2000, Loss: 0.0001756354613462463\n",
      "Iteration 1412/2000, Loss: 0.00016698800027370453\n",
      "Iteration 1413/2000, Loss: 0.0001603838463779539\n",
      "Iteration 1414/2000, Loss: 0.0001864613004727289\n",
      "Iteration 1415/2000, Loss: 0.00017477647634223104\n",
      "Iteration 1416/2000, Loss: 0.00010307817137800157\n",
      "Iteration 1417/2000, Loss: 0.000545226619578898\n",
      "Iteration 1418/2000, Loss: 0.0002478444075677544\n",
      "Iteration 1419/2000, Loss: 0.00013214665523264557\n",
      "Iteration 1420/2000, Loss: 0.00012749766756314784\n",
      "Iteration 1421/2000, Loss: 0.0002205410710303113\n",
      "Iteration 1422/2000, Loss: 0.0002320722269359976\n",
      "Iteration 1423/2000, Loss: 0.0001279416319448501\n",
      "Iteration 1424/2000, Loss: 0.00023215101100504398\n",
      "Iteration 1425/2000, Loss: 0.00022686345619149506\n",
      "Iteration 1426/2000, Loss: 0.0004043733642902225\n",
      "Iteration 1427/2000, Loss: 0.0002679606550373137\n",
      "Iteration 1428/2000, Loss: 0.000519577763043344\n",
      "Iteration 1429/2000, Loss: 0.0003272366593591869\n",
      "Iteration 1430/2000, Loss: 0.0003120046458207071\n",
      "Iteration 1431/2000, Loss: 0.00014129135524854064\n",
      "Iteration 1432/2000, Loss: 0.00036628032103180885\n",
      "Iteration 1433/2000, Loss: 0.00022000847093295306\n",
      "Iteration 1434/2000, Loss: 0.00022594290203414857\n",
      "Iteration 1435/2000, Loss: 0.00015781520050950348\n",
      "Iteration 1436/2000, Loss: 0.0002133239759132266\n",
      "Iteration 1437/2000, Loss: 0.00018918765999842435\n",
      "Iteration 1438/2000, Loss: 0.00015159824397414923\n",
      "Iteration 1439/2000, Loss: 0.00018596499285195023\n",
      "Iteration 1440/2000, Loss: 0.0001781239698175341\n",
      "Iteration 1441/2000, Loss: 0.00016383589536417276\n",
      "Iteration 1442/2000, Loss: 0.0001762672036420554\n",
      "Iteration 1443/2000, Loss: 0.0001504966348875314\n",
      "Iteration 1444/2000, Loss: 0.00013201481488067657\n",
      "Iteration 1445/2000, Loss: 0.00012603089271578938\n",
      "Iteration 1446/2000, Loss: 0.00025742073194123805\n",
      "Iteration 1447/2000, Loss: 0.0001864621153799817\n",
      "Iteration 1448/2000, Loss: 0.0001826481893658638\n",
      "Iteration 1449/2000, Loss: 0.00015393503417726606\n",
      "Iteration 1450/2000, Loss: 0.00018814532086253166\n",
      "Iteration 1451/2000, Loss: 0.00010641143308021128\n",
      "Iteration 1452/2000, Loss: 0.00016797958232928067\n",
      "Iteration 1453/2000, Loss: 0.00023915863130241632\n",
      "Iteration 1454/2000, Loss: 0.00020910169405397028\n",
      "Iteration 1455/2000, Loss: 0.0002600979059934616\n",
      "Iteration 1456/2000, Loss: 0.0002460113901179284\n",
      "Iteration 1457/2000, Loss: 0.00043995867599733174\n",
      "Iteration 1458/2000, Loss: 0.00014876120258122683\n",
      "Iteration 1459/2000, Loss: 0.00032230940996669233\n",
      "Iteration 1460/2000, Loss: 0.0003496370918583125\n",
      "Iteration 1461/2000, Loss: 0.00016081843932624906\n",
      "Iteration 1462/2000, Loss: 0.0002650340902619064\n",
      "Iteration 1463/2000, Loss: 0.0002292675490025431\n",
      "Iteration 1464/2000, Loss: 0.000370348512660712\n",
      "Iteration 1465/2000, Loss: 0.00030151792452670634\n",
      "Iteration 1466/2000, Loss: 0.0001814600545912981\n",
      "Iteration 1467/2000, Loss: 0.00023634827812202275\n",
      "Iteration 1468/2000, Loss: 0.00017263746121898293\n",
      "Iteration 1469/2000, Loss: 0.00040819746209308505\n",
      "Iteration 1470/2000, Loss: 0.0002043708082055673\n",
      "Iteration 1471/2000, Loss: 0.00014803306839894503\n",
      "Iteration 1472/2000, Loss: 0.00043446998461149633\n",
      "Iteration 1473/2000, Loss: 0.0003645905526354909\n",
      "Iteration 1474/2000, Loss: 0.0003067671204917133\n",
      "Iteration 1475/2000, Loss: 0.00020925125863868743\n",
      "Iteration 1476/2000, Loss: 0.0002885080175474286\n",
      "Iteration 1477/2000, Loss: 0.00037713945494033396\n",
      "Iteration 1478/2000, Loss: 0.0002881237887777388\n",
      "Iteration 1479/2000, Loss: 0.0003002458834089339\n",
      "Iteration 1480/2000, Loss: 0.00030346642597578466\n",
      "Iteration 1481/2000, Loss: 0.0002931419585365802\n",
      "Iteration 1482/2000, Loss: 0.00022015624563209713\n",
      "Iteration 1483/2000, Loss: 0.0002189644583268091\n",
      "Iteration 1484/2000, Loss: 0.000197863788343966\n",
      "Iteration 1485/2000, Loss: 0.00023559315013699234\n",
      "Iteration 1486/2000, Loss: 0.00015306903515011072\n",
      "Iteration 1487/2000, Loss: 0.0001610246836207807\n",
      "Iteration 1488/2000, Loss: 0.0001409410178894177\n",
      "Iteration 1489/2000, Loss: 0.0005668009398505092\n",
      "Iteration 1490/2000, Loss: 0.0001813232956919819\n",
      "Iteration 1491/2000, Loss: 0.00025993669987656176\n",
      "Iteration 1492/2000, Loss: 0.00015687332779634744\n",
      "Iteration 1493/2000, Loss: 0.00011864247062476352\n",
      "Iteration 1494/2000, Loss: 0.00011530029587447643\n",
      "Iteration 1495/2000, Loss: 0.00029981089755892754\n",
      "Iteration 1496/2000, Loss: 0.00010424742504255846\n",
      "Iteration 1497/2000, Loss: 0.0001846476225182414\n",
      "Iteration 1498/2000, Loss: 0.00018667163385543972\n",
      "Iteration 1499/2000, Loss: 0.0002056814410025254\n",
      "Iteration 1500/2000, Loss: 0.000199354937649332\n",
      "Iteration 1501/2000, Loss: 0.00017563901201356202\n",
      "Iteration 1502/2000, Loss: 0.00016078155022114515\n",
      "Iteration 1503/2000, Loss: 0.00025994464522227645\n",
      "Iteration 1504/2000, Loss: 0.0002752740401774645\n",
      "Iteration 1505/2000, Loss: 0.0002261805348098278\n",
      "Iteration 1506/2000, Loss: 0.00015271478332579136\n",
      "Iteration 1507/2000, Loss: 0.00019046613306272775\n",
      "Iteration 1508/2000, Loss: 0.00011329608969390392\n",
      "Iteration 1509/2000, Loss: 0.00026122533017769456\n",
      "Iteration 1510/2000, Loss: 0.00022715610975865275\n",
      "Iteration 1511/2000, Loss: 0.0004911186988465488\n",
      "Iteration 1512/2000, Loss: 0.00018920612637884915\n",
      "Iteration 1513/2000, Loss: 0.00026621497818268836\n",
      "Iteration 1514/2000, Loss: 0.00016006617806851864\n",
      "Iteration 1515/2000, Loss: 0.00018658999761100858\n",
      "Iteration 1516/2000, Loss: 0.00022368492500390857\n",
      "Iteration 1517/2000, Loss: 0.00021105365885887295\n",
      "Iteration 1518/2000, Loss: 0.000299385777907446\n",
      "Iteration 1519/2000, Loss: 0.00018245528917759657\n",
      "Iteration 1520/2000, Loss: 0.0002578752755653113\n",
      "Iteration 1521/2000, Loss: 0.00045836015488021076\n",
      "Iteration 1522/2000, Loss: 0.00022808401263318956\n",
      "Iteration 1523/2000, Loss: 0.0002331026626052335\n",
      "Iteration 1524/2000, Loss: 0.00014697585720568895\n",
      "Iteration 1525/2000, Loss: 0.0002026650181505829\n",
      "Iteration 1526/2000, Loss: 0.00013297965051606297\n",
      "Iteration 1527/2000, Loss: 0.0005728029063902795\n",
      "Iteration 1528/2000, Loss: 0.0002900119579862803\n",
      "Iteration 1529/2000, Loss: 0.00023077860532794148\n",
      "Iteration 1530/2000, Loss: 0.00013276416575536132\n",
      "Iteration 1531/2000, Loss: 0.00014360166096594185\n",
      "Iteration 1532/2000, Loss: 0.00019551589502952993\n",
      "Iteration 1533/2000, Loss: 0.00018885225290432572\n",
      "Iteration 1534/2000, Loss: 0.0002393706381553784\n",
      "Iteration 1535/2000, Loss: 0.00042892617057077587\n",
      "Iteration 1536/2000, Loss: 0.0001793797709979117\n",
      "Iteration 1537/2000, Loss: 0.0001704603200778365\n",
      "Iteration 1538/2000, Loss: 0.00017250669770874083\n",
      "Iteration 1539/2000, Loss: 0.0002654889540281147\n",
      "Iteration 1540/2000, Loss: 0.0004498564812820405\n",
      "Iteration 1541/2000, Loss: 0.0003714366175699979\n",
      "Iteration 1542/2000, Loss: 0.00010445342923048884\n",
      "Iteration 1543/2000, Loss: 0.00025335224927403033\n",
      "Iteration 1544/2000, Loss: 0.00025428508524782956\n",
      "Iteration 1545/2000, Loss: 0.00017446570564061403\n",
      "Iteration 1546/2000, Loss: 0.0003184099623467773\n",
      "Iteration 1547/2000, Loss: 0.0002078482211800292\n",
      "Iteration 1548/2000, Loss: 0.0003536498988978565\n",
      "Iteration 1549/2000, Loss: 0.00014175180695019662\n",
      "Iteration 1550/2000, Loss: 0.00024223663785960525\n",
      "Iteration 1551/2000, Loss: 0.00021269641001708806\n",
      "Iteration 1552/2000, Loss: 0.00016304485325235873\n",
      "Iteration 1553/2000, Loss: 0.00010981235391227528\n",
      "Iteration 1554/2000, Loss: 0.00020847900304943323\n",
      "Iteration 1555/2000, Loss: 0.00035692332312464714\n",
      "Iteration 1556/2000, Loss: 0.0002394231123616919\n",
      "Iteration 1557/2000, Loss: 0.0001506959815742448\n",
      "Iteration 1558/2000, Loss: 0.00016033111023716629\n",
      "Iteration 1559/2000, Loss: 0.00018212056602351367\n",
      "Iteration 1560/2000, Loss: 0.0001838562311604619\n",
      "Iteration 1561/2000, Loss: 0.00015413995424751192\n",
      "Iteration 1562/2000, Loss: 0.0003476781421341002\n",
      "Iteration 1563/2000, Loss: 0.00033454920048825443\n",
      "Iteration 1564/2000, Loss: 0.00034183694515377283\n",
      "Iteration 1565/2000, Loss: 0.00022949195408727974\n",
      "Iteration 1566/2000, Loss: 0.00023636680271010846\n",
      "Iteration 1567/2000, Loss: 0.000665529165416956\n",
      "Iteration 1568/2000, Loss: 0.00030693362350575626\n",
      "Iteration 1569/2000, Loss: 0.00020066146680619568\n",
      "Iteration 1570/2000, Loss: 0.0003072864783462137\n",
      "Iteration 1571/2000, Loss: 0.0002139159187208861\n",
      "Iteration 1572/2000, Loss: 0.00035894193570129573\n",
      "Iteration 1573/2000, Loss: 0.00014506085426546633\n",
      "Iteration 1574/2000, Loss: 0.00017998984549194574\n",
      "Iteration 1575/2000, Loss: 0.00022626457212027162\n",
      "Iteration 1576/2000, Loss: 0.00024118337023537606\n",
      "Iteration 1577/2000, Loss: 0.0002497116511221975\n",
      "Iteration 1578/2000, Loss: 0.00021900577121414244\n",
      "Iteration 1579/2000, Loss: 0.00024409494653809816\n",
      "Iteration 1580/2000, Loss: 0.00018301181262359023\n",
      "Iteration 1581/2000, Loss: 0.00020757291349582374\n",
      "Iteration 1582/2000, Loss: 0.00036151555832475424\n",
      "Iteration 1583/2000, Loss: 0.0001793655101209879\n",
      "Iteration 1584/2000, Loss: 0.00011698054004227743\n",
      "Iteration 1585/2000, Loss: 0.0002816538908518851\n",
      "Iteration 1586/2000, Loss: 0.000977970426902175\n",
      "Iteration 1587/2000, Loss: 0.00014613394159823656\n",
      "Iteration 1588/2000, Loss: 0.00014900715905241668\n",
      "Iteration 1589/2000, Loss: 0.0002077704411931336\n",
      "Iteration 1590/2000, Loss: 0.0001795211574062705\n",
      "Iteration 1591/2000, Loss: 0.0002959948033094406\n",
      "Iteration 1592/2000, Loss: 0.00019428570521995425\n",
      "Iteration 1593/2000, Loss: 0.00022939624614082277\n",
      "Iteration 1594/2000, Loss: 0.0003770959156099707\n",
      "Iteration 1595/2000, Loss: 0.00032532011391595006\n",
      "Iteration 1596/2000, Loss: 0.00015350792091339827\n",
      "Iteration 1597/2000, Loss: 0.00027899834094569087\n",
      "Iteration 1598/2000, Loss: 0.0002834756742231548\n",
      "Iteration 1599/2000, Loss: 0.0003281004319433123\n",
      "Iteration 1600/2000, Loss: 0.00024760255473665893\n",
      "Iteration 1601/2000, Loss: 0.00016831676475703716\n",
      "Iteration 1602/2000, Loss: 0.00021713080059271306\n",
      "Iteration 1603/2000, Loss: 0.00022882618941366673\n",
      "Iteration 1604/2000, Loss: 0.00014781678328290582\n",
      "Iteration 1605/2000, Loss: 0.00030177991720847785\n",
      "Iteration 1606/2000, Loss: 0.00028006036882288754\n",
      "Iteration 1607/2000, Loss: 0.0002151598164346069\n",
      "Iteration 1608/2000, Loss: 0.0002982993028126657\n",
      "Iteration 1609/2000, Loss: 0.0003796543169301003\n",
      "Iteration 1610/2000, Loss: 0.0003804164589382708\n",
      "Iteration 1611/2000, Loss: 0.00024611499975435436\n",
      "Iteration 1612/2000, Loss: 0.0003322651027701795\n",
      "Iteration 1613/2000, Loss: 0.00027614846476353705\n",
      "Iteration 1614/2000, Loss: 0.0002565476461313665\n",
      "Iteration 1615/2000, Loss: 0.00014564029697794467\n",
      "Iteration 1616/2000, Loss: 0.00040415325202047825\n",
      "Iteration 1617/2000, Loss: 0.00023049578885547817\n",
      "Iteration 1618/2000, Loss: 0.00023938316735439003\n",
      "Iteration 1619/2000, Loss: 0.00019840779714286327\n",
      "Iteration 1620/2000, Loss: 0.00045897465315647423\n",
      "Iteration 1621/2000, Loss: 0.000450487423222512\n",
      "Iteration 1622/2000, Loss: 0.0002664453932084143\n",
      "Iteration 1623/2000, Loss: 0.0002138814452337101\n",
      "Iteration 1624/2000, Loss: 0.0002505433512851596\n",
      "Iteration 1625/2000, Loss: 0.00018204125808551908\n",
      "Iteration 1626/2000, Loss: 0.00029855931643396616\n",
      "Iteration 1627/2000, Loss: 0.00020450334704946727\n",
      "Iteration 1628/2000, Loss: 0.0005770352436229587\n",
      "Iteration 1629/2000, Loss: 0.0005150672513991594\n",
      "Iteration 1630/2000, Loss: 0.00014896955690346658\n",
      "Iteration 1631/2000, Loss: 0.00041199708357453346\n",
      "Iteration 1632/2000, Loss: 0.0005747454124502838\n",
      "Iteration 1633/2000, Loss: 0.00021230202401056886\n",
      "Iteration 1634/2000, Loss: 0.0002704164362512529\n",
      "Iteration 1635/2000, Loss: 0.00030605492065660655\n",
      "Iteration 1636/2000, Loss: 0.00015115812129806727\n",
      "Iteration 1637/2000, Loss: 0.00043663798714987934\n",
      "Iteration 1638/2000, Loss: 0.0001756074489094317\n",
      "Iteration 1639/2000, Loss: 0.00015378251555375755\n",
      "Iteration 1640/2000, Loss: 0.0002561860310379416\n",
      "Iteration 1641/2000, Loss: 0.00017771510465536267\n",
      "Iteration 1642/2000, Loss: 0.00025769113563001156\n",
      "Iteration 1643/2000, Loss: 0.0003520752361509949\n",
      "Iteration 1644/2000, Loss: 0.00016502397193107754\n",
      "Iteration 1645/2000, Loss: 0.00038405138184316456\n",
      "Iteration 1646/2000, Loss: 0.00019717573013622314\n",
      "Iteration 1647/2000, Loss: 0.00041506686829961836\n",
      "Iteration 1648/2000, Loss: 0.00047513042227365077\n",
      "Iteration 1649/2000, Loss: 0.00013468886027112603\n",
      "Iteration 1650/2000, Loss: 0.0006578476168215275\n",
      "Iteration 1651/2000, Loss: 0.00029704105691052973\n",
      "Iteration 1652/2000, Loss: 0.0002444164711050689\n",
      "Iteration 1653/2000, Loss: 0.00039753838791511953\n",
      "Iteration 1654/2000, Loss: 0.0003959603200200945\n",
      "Iteration 1655/2000, Loss: 0.0003445819893386215\n",
      "Iteration 1656/2000, Loss: 0.00015423870354425162\n",
      "Iteration 1657/2000, Loss: 0.00027615827275440097\n",
      "Iteration 1658/2000, Loss: 0.0002352290175622329\n",
      "Iteration 1659/2000, Loss: 0.00023621664149686694\n",
      "Iteration 1660/2000, Loss: 0.0001700322172837332\n",
      "Iteration 1661/2000, Loss: 0.00010424320498714224\n",
      "Iteration 1662/2000, Loss: 0.0002376891643507406\n",
      "Iteration 1663/2000, Loss: 0.00016685838636476547\n",
      "Iteration 1664/2000, Loss: 0.0002158723073080182\n",
      "Iteration 1665/2000, Loss: 0.0001535406190669164\n",
      "Iteration 1666/2000, Loss: 0.0002463719865772873\n",
      "Iteration 1667/2000, Loss: 0.00018015078967437148\n",
      "Iteration 1668/2000, Loss: 0.0001621480769244954\n",
      "Iteration 1669/2000, Loss: 0.0001212986244354397\n",
      "Iteration 1670/2000, Loss: 0.0002106526808347553\n",
      "Iteration 1671/2000, Loss: 0.00019974879978690296\n",
      "Iteration 1672/2000, Loss: 0.0003076776920352131\n",
      "Iteration 1673/2000, Loss: 0.00021748431026935577\n",
      "Iteration 1674/2000, Loss: 0.0002279345935676247\n",
      "Iteration 1675/2000, Loss: 0.00029366801027208567\n",
      "Iteration 1676/2000, Loss: 0.00016008902457542717\n",
      "Iteration 1677/2000, Loss: 0.00017793723964132369\n",
      "Iteration 1678/2000, Loss: 0.0002843679394572973\n",
      "Iteration 1679/2000, Loss: 0.0002567785850260407\n",
      "Iteration 1680/2000, Loss: 0.00024065151228569448\n",
      "Iteration 1681/2000, Loss: 0.00014979371917434037\n",
      "Iteration 1682/2000, Loss: 0.00020457227947190404\n",
      "Iteration 1683/2000, Loss: 0.00014950166223570704\n",
      "Iteration 1684/2000, Loss: 0.00020373745064716786\n",
      "Iteration 1685/2000, Loss: 0.00014815331087447703\n",
      "Iteration 1686/2000, Loss: 0.000135016642161645\n",
      "Iteration 1687/2000, Loss: 0.0001479687198298052\n",
      "Iteration 1688/2000, Loss: 9.776667138794437e-05\n",
      "Iteration 1689/2000, Loss: 0.00022691808408126235\n",
      "Iteration 1690/2000, Loss: 0.0001645460433792323\n",
      "Iteration 1691/2000, Loss: 0.00016266755119431764\n",
      "Iteration 1692/2000, Loss: 0.0002402800600975752\n",
      "Iteration 1693/2000, Loss: 0.00032811309210956097\n",
      "Iteration 1694/2000, Loss: 0.00020076606597285718\n",
      "Iteration 1695/2000, Loss: 0.00015688323765061796\n",
      "Iteration 1696/2000, Loss: 0.00015180444461293519\n",
      "Iteration 1697/2000, Loss: 0.00018000506679527462\n",
      "Iteration 1698/2000, Loss: 0.0002715774462558329\n",
      "Iteration 1699/2000, Loss: 0.00027867601602338254\n",
      "Iteration 1700/2000, Loss: 0.00015574818826280534\n",
      "Iteration 1701/2000, Loss: 0.00012315793719608337\n",
      "Iteration 1702/2000, Loss: 0.0003703586698975414\n",
      "Iteration 1703/2000, Loss: 0.00015034423267934471\n",
      "Iteration 1704/2000, Loss: 0.00012907633208669722\n",
      "Iteration 1705/2000, Loss: 0.00015057007840368897\n",
      "Iteration 1706/2000, Loss: 0.0003704989794641733\n",
      "Iteration 1707/2000, Loss: 0.00016778046847321093\n",
      "Iteration 1708/2000, Loss: 0.00013895977463107556\n",
      "Iteration 1709/2000, Loss: 0.0001714980899123475\n",
      "Iteration 1710/2000, Loss: 0.0003929754893761128\n",
      "Iteration 1711/2000, Loss: 0.00014632506645284593\n",
      "Iteration 1712/2000, Loss: 0.00016267217870336026\n",
      "Iteration 1713/2000, Loss: 0.00034790486097335815\n",
      "Iteration 1714/2000, Loss: 0.00021552243561018258\n",
      "Iteration 1715/2000, Loss: 0.00029910416924394667\n",
      "Iteration 1716/2000, Loss: 0.00021191149426158518\n",
      "Iteration 1717/2000, Loss: 0.0004813465930055827\n",
      "Iteration 1718/2000, Loss: 0.0002324819506611675\n",
      "Iteration 1719/2000, Loss: 0.0002213061525253579\n",
      "Iteration 1720/2000, Loss: 0.00016358392895199358\n",
      "Iteration 1721/2000, Loss: 0.00019782688468694687\n",
      "Iteration 1722/2000, Loss: 0.00020738577586598694\n",
      "Iteration 1723/2000, Loss: 0.0002369440335314721\n",
      "Iteration 1724/2000, Loss: 0.0003090263926424086\n",
      "Iteration 1725/2000, Loss: 0.0001696179824648425\n",
      "Iteration 1726/2000, Loss: 0.00022372024250216782\n",
      "Iteration 1727/2000, Loss: 0.0002740572381298989\n",
      "Iteration 1728/2000, Loss: 0.00020483427215367556\n",
      "Iteration 1729/2000, Loss: 0.0012431242503225803\n",
      "Iteration 1730/2000, Loss: 0.0001973704929696396\n",
      "Iteration 1731/2000, Loss: 0.0002206609060522169\n",
      "Iteration 1732/2000, Loss: 0.00026721839094534516\n",
      "Iteration 1733/2000, Loss: 0.00016666963347233832\n",
      "Iteration 1734/2000, Loss: 0.0003734332276508212\n",
      "Iteration 1735/2000, Loss: 0.00012260035146027803\n",
      "Iteration 1736/2000, Loss: 0.0003118887252639979\n",
      "Iteration 1737/2000, Loss: 0.00033472536597400904\n",
      "Iteration 1738/2000, Loss: 0.0003397149848751724\n",
      "Iteration 1739/2000, Loss: 0.0003077326400671154\n",
      "Iteration 1740/2000, Loss: 0.0003530091780703515\n",
      "Iteration 1741/2000, Loss: 0.0002733665460254997\n",
      "Iteration 1742/2000, Loss: 0.0003793179930653423\n",
      "Iteration 1743/2000, Loss: 0.00033786811400204897\n",
      "Iteration 1744/2000, Loss: 0.00024465189198963344\n",
      "Iteration 1745/2000, Loss: 0.0002992422669194639\n",
      "Iteration 1746/2000, Loss: 0.0001860637858044356\n",
      "Iteration 1747/2000, Loss: 0.0004300341533962637\n",
      "Iteration 1748/2000, Loss: 0.00022086438548285514\n",
      "Iteration 1749/2000, Loss: 0.0001940061483765021\n",
      "Iteration 1750/2000, Loss: 0.0003890547377523035\n",
      "Iteration 1751/2000, Loss: 0.0001257307012565434\n",
      "Iteration 1752/2000, Loss: 0.00022843980696052313\n",
      "Iteration 1753/2000, Loss: 0.00017656362615525723\n",
      "Iteration 1754/2000, Loss: 0.0002078946417896077\n",
      "Iteration 1755/2000, Loss: 0.00027737635537050664\n",
      "Iteration 1756/2000, Loss: 0.00024359760573133826\n",
      "Iteration 1757/2000, Loss: 0.0001861567870946601\n",
      "Iteration 1758/2000, Loss: 0.0002665518259163946\n",
      "Iteration 1759/2000, Loss: 0.0001413770514773205\n",
      "Iteration 1760/2000, Loss: 0.000208793455385603\n",
      "Iteration 1761/2000, Loss: 0.0001499190548202023\n",
      "Iteration 1762/2000, Loss: 0.00014115097292233258\n",
      "Iteration 1763/2000, Loss: 0.00018102061585523188\n",
      "Iteration 1764/2000, Loss: 0.00021436740644276142\n",
      "Iteration 1765/2000, Loss: 0.0002832258469425142\n",
      "Iteration 1766/2000, Loss: 0.00018121708126273006\n",
      "Iteration 1767/2000, Loss: 0.00029947434086352587\n",
      "Iteration 1768/2000, Loss: 0.00015497012645937502\n",
      "Iteration 1769/2000, Loss: 0.0005978674744255841\n",
      "Iteration 1770/2000, Loss: 0.00022615547641180456\n",
      "Iteration 1771/2000, Loss: 0.0001269350032089278\n",
      "Iteration 1772/2000, Loss: 0.0004396370495669544\n",
      "Iteration 1773/2000, Loss: 0.00018275850743521005\n",
      "Iteration 1774/2000, Loss: 0.0003903309698216617\n",
      "Iteration 1775/2000, Loss: 0.0004045752575621009\n",
      "Iteration 1776/2000, Loss: 0.00015888501366134733\n",
      "Iteration 1777/2000, Loss: 0.0002653743140399456\n",
      "Iteration 1778/2000, Loss: 0.00021402034326456487\n",
      "Iteration 1779/2000, Loss: 0.00021309191652107984\n",
      "Iteration 1780/2000, Loss: 0.00023829418933019042\n",
      "Iteration 1781/2000, Loss: 0.0001624683354748413\n",
      "Iteration 1782/2000, Loss: 0.00021773652406409383\n",
      "Iteration 1783/2000, Loss: 0.00021472420485224575\n",
      "Iteration 1784/2000, Loss: 0.00019658262317534536\n",
      "Iteration 1785/2000, Loss: 0.0002709943801164627\n",
      "Iteration 1786/2000, Loss: 0.0003130566037725657\n",
      "Iteration 1787/2000, Loss: 0.00022376143897417933\n",
      "Iteration 1788/2000, Loss: 0.00011049544991692528\n",
      "Iteration 1789/2000, Loss: 0.00020474990014918149\n",
      "Iteration 1790/2000, Loss: 0.00022034416906535625\n",
      "Iteration 1791/2000, Loss: 0.00026331166736781597\n",
      "Iteration 1792/2000, Loss: 0.0004308338975533843\n",
      "Iteration 1793/2000, Loss: 0.00018544340855441988\n",
      "Iteration 1794/2000, Loss: 0.00019015558063983917\n",
      "Iteration 1795/2000, Loss: 0.0001179121172754094\n",
      "Iteration 1796/2000, Loss: 0.00027546187629923224\n",
      "Iteration 1797/2000, Loss: 0.00022118279593996704\n",
      "Iteration 1798/2000, Loss: 0.00021899197599850595\n",
      "Iteration 1799/2000, Loss: 0.00019546563271433115\n",
      "Iteration 1800/2000, Loss: 0.0002498390094842762\n",
      "Iteration 1801/2000, Loss: 0.00012418041296768934\n",
      "Iteration 1802/2000, Loss: 0.0002052666386589408\n",
      "Iteration 1803/2000, Loss: 0.00019799704023171216\n",
      "Iteration 1804/2000, Loss: 0.00024019851116463542\n",
      "Iteration 1805/2000, Loss: 0.00015314834308810532\n",
      "Iteration 1806/2000, Loss: 0.0002590906515251845\n",
      "Iteration 1807/2000, Loss: 0.00017117132665589452\n",
      "Iteration 1808/2000, Loss: 0.00021407743042800575\n",
      "Iteration 1809/2000, Loss: 0.0002891278709284961\n",
      "Iteration 1810/2000, Loss: 0.0001378070592181757\n",
      "Iteration 1811/2000, Loss: 0.00019422758487053216\n",
      "Iteration 1812/2000, Loss: 0.00024307951389346272\n",
      "Iteration 1813/2000, Loss: 0.0002824960683938116\n",
      "Iteration 1814/2000, Loss: 0.00022048123355489224\n",
      "Iteration 1815/2000, Loss: 0.00016851932741701603\n",
      "Iteration 1816/2000, Loss: 0.00017298597958870232\n",
      "Iteration 1817/2000, Loss: 0.00025099259801208973\n",
      "Iteration 1818/2000, Loss: 0.00011275103315711021\n",
      "Iteration 1819/2000, Loss: 0.00017113069770857692\n",
      "Iteration 1820/2000, Loss: 0.00031153904274106026\n",
      "Iteration 1821/2000, Loss: 0.00020439310173969716\n",
      "Iteration 1822/2000, Loss: 0.0003031503001693636\n",
      "Iteration 1823/2000, Loss: 0.0002014545170823112\n",
      "Iteration 1824/2000, Loss: 0.00019192503532394767\n",
      "Iteration 1825/2000, Loss: 0.00024090266379062086\n",
      "Iteration 1826/2000, Loss: 0.00020464963745325804\n",
      "Iteration 1827/2000, Loss: 0.00030643236823379993\n",
      "Iteration 1828/2000, Loss: 0.000167746446095407\n",
      "Iteration 1829/2000, Loss: 0.0002825831761583686\n",
      "Iteration 1830/2000, Loss: 0.00014335477317217737\n",
      "Iteration 1831/2000, Loss: 0.0002834875776898116\n",
      "Iteration 1832/2000, Loss: 0.00018892486696131527\n",
      "Iteration 1833/2000, Loss: 0.00036758798523806036\n",
      "Iteration 1834/2000, Loss: 0.00018408792675472796\n",
      "Iteration 1835/2000, Loss: 0.00015025549510028213\n",
      "Iteration 1836/2000, Loss: 0.0001483192027080804\n",
      "Iteration 1837/2000, Loss: 0.0003451134543865919\n",
      "Iteration 1838/2000, Loss: 0.00020669453078880906\n",
      "Iteration 1839/2000, Loss: 0.00028129559359513223\n",
      "Iteration 1840/2000, Loss: 0.0001565378624945879\n",
      "Iteration 1841/2000, Loss: 0.00021027066395618021\n",
      "Iteration 1842/2000, Loss: 0.00022623791301157326\n",
      "Iteration 1843/2000, Loss: 0.00015728540893178433\n",
      "Iteration 1844/2000, Loss: 0.00015899540449026972\n",
      "Iteration 1845/2000, Loss: 0.0002192813844885677\n",
      "Iteration 1846/2000, Loss: 0.0001358940062345937\n",
      "Iteration 1847/2000, Loss: 0.00025807670317590237\n",
      "Iteration 1848/2000, Loss: 0.00014668628864455968\n",
      "Iteration 1849/2000, Loss: 0.00012487618369050324\n",
      "Iteration 1850/2000, Loss: 0.0001543601683806628\n",
      "Iteration 1851/2000, Loss: 0.00017776076856534928\n",
      "Iteration 1852/2000, Loss: 0.0005810859729535878\n",
      "Iteration 1853/2000, Loss: 0.00021097221178933978\n",
      "Iteration 1854/2000, Loss: 0.0002133802481694147\n",
      "Iteration 1855/2000, Loss: 0.0004803340998478234\n",
      "Iteration 1856/2000, Loss: 0.00014401342195924371\n",
      "Iteration 1857/2000, Loss: 0.0003417397674638778\n",
      "Iteration 1858/2000, Loss: 0.0002571937220636755\n",
      "Iteration 1859/2000, Loss: 0.00027317251078784466\n",
      "Iteration 1860/2000, Loss: 0.0004663768340833485\n",
      "Iteration 1861/2000, Loss: 0.00022480383631773293\n",
      "Iteration 1862/2000, Loss: 0.0002773767046164721\n",
      "Iteration 1863/2000, Loss: 0.00015273832832463086\n",
      "Iteration 1864/2000, Loss: 0.0003611417196225375\n",
      "Iteration 1865/2000, Loss: 0.0005781438085250556\n",
      "Iteration 1866/2000, Loss: 0.00022401763999368995\n",
      "Iteration 1867/2000, Loss: 0.0004013220313936472\n",
      "Iteration 1868/2000, Loss: 0.0003306736471131444\n",
      "Iteration 1869/2000, Loss: 0.00041914329631254077\n",
      "Iteration 1870/2000, Loss: 0.000541737477760762\n",
      "Iteration 1871/2000, Loss: 0.00013941878569312394\n",
      "Iteration 1872/2000, Loss: 0.00040303534478880465\n",
      "Iteration 1873/2000, Loss: 0.0005869904416613281\n",
      "Iteration 1874/2000, Loss: 0.00017749081598594785\n",
      "Iteration 1875/2000, Loss: 0.00026890021399594843\n",
      "Iteration 1876/2000, Loss: 0.00018912444647867233\n",
      "Iteration 1877/2000, Loss: 0.0003511916729621589\n",
      "Iteration 1878/2000, Loss: 0.00020275445422157645\n",
      "Iteration 1879/2000, Loss: 0.0002799512876663357\n",
      "Iteration 1880/2000, Loss: 0.00022631263709627092\n",
      "Iteration 1881/2000, Loss: 0.00026568645262159407\n",
      "Iteration 1882/2000, Loss: 0.0004069013230036944\n",
      "Iteration 1883/2000, Loss: 0.0005940761766396463\n",
      "Iteration 1884/2000, Loss: 0.00020358564506750554\n",
      "Iteration 1885/2000, Loss: 0.0003706414718180895\n",
      "Iteration 1886/2000, Loss: 0.00015216755855362862\n",
      "Iteration 1887/2000, Loss: 0.0002915279474109411\n",
      "Iteration 1888/2000, Loss: 0.00023248212528415024\n",
      "Iteration 1889/2000, Loss: 0.0003699063672684133\n",
      "Iteration 1890/2000, Loss: 0.0001998540828935802\n",
      "Iteration 1891/2000, Loss: 0.0001844455546233803\n",
      "Iteration 1892/2000, Loss: 0.0002896780497394502\n",
      "Iteration 1893/2000, Loss: 0.0002052710042335093\n",
      "Iteration 1894/2000, Loss: 0.0001973227335838601\n",
      "Iteration 1895/2000, Loss: 0.00032618173281662166\n",
      "Iteration 1896/2000, Loss: 0.00020370575657580048\n",
      "Iteration 1897/2000, Loss: 0.00028567088884301484\n",
      "Iteration 1898/2000, Loss: 0.00018164869106840342\n",
      "Iteration 1899/2000, Loss: 0.0001645239390200004\n",
      "Iteration 1900/2000, Loss: 0.00023989539477042854\n",
      "Iteration 1901/2000, Loss: 0.00035046268021687865\n",
      "Iteration 1902/2000, Loss: 0.00032023960375227034\n",
      "Iteration 1903/2000, Loss: 0.00020955948275513947\n",
      "Iteration 1904/2000, Loss: 0.00027998059522360563\n",
      "Iteration 1905/2000, Loss: 0.0004087144916411489\n",
      "Iteration 1906/2000, Loss: 0.00022005500795785338\n",
      "Iteration 1907/2000, Loss: 0.0002910537295974791\n",
      "Iteration 1908/2000, Loss: 0.0001345852651866153\n",
      "Iteration 1909/2000, Loss: 0.00020583294099196792\n",
      "Iteration 1910/2000, Loss: 0.000173480439116247\n",
      "Iteration 1911/2000, Loss: 0.0005665096105076373\n",
      "Iteration 1912/2000, Loss: 0.00022690680634696037\n",
      "Iteration 1913/2000, Loss: 0.0001592369080753997\n",
      "Iteration 1914/2000, Loss: 0.0002834151091519743\n",
      "Iteration 1915/2000, Loss: 0.0002529990451876074\n",
      "Iteration 1916/2000, Loss: 0.0001756086858222261\n",
      "Iteration 1917/2000, Loss: 0.00028485001530498266\n",
      "Iteration 1918/2000, Loss: 0.0002609051880426705\n",
      "Iteration 1919/2000, Loss: 0.0001560383680043742\n",
      "Iteration 1920/2000, Loss: 0.0001936290063895285\n",
      "Iteration 1921/2000, Loss: 0.0001584670098964125\n",
      "Iteration 1922/2000, Loss: 0.00021086125343572348\n",
      "Iteration 1923/2000, Loss: 0.0001370229001622647\n",
      "Iteration 1924/2000, Loss: 0.0003083838091697544\n",
      "Iteration 1925/2000, Loss: 0.00018661921785678715\n",
      "Iteration 1926/2000, Loss: 0.0003265383711550385\n",
      "Iteration 1927/2000, Loss: 0.00010982518142554909\n",
      "Iteration 1928/2000, Loss: 0.00021246919641271234\n",
      "Iteration 1929/2000, Loss: 0.0001633439533179626\n",
      "Iteration 1930/2000, Loss: 0.00014465994900092483\n",
      "Iteration 1931/2000, Loss: 0.0002206875360570848\n",
      "Iteration 1932/2000, Loss: 0.00016382045578211546\n",
      "Iteration 1933/2000, Loss: 0.00021421437850221992\n",
      "Iteration 1934/2000, Loss: 0.0001926762779476121\n",
      "Iteration 1935/2000, Loss: 0.00016830167442094535\n",
      "Iteration 1936/2000, Loss: 0.00012682197848334908\n",
      "Iteration 1937/2000, Loss: 0.0001604791614226997\n",
      "Iteration 1938/2000, Loss: 0.000207035118364729\n",
      "Iteration 1939/2000, Loss: 0.00020201873849146068\n",
      "Iteration 1940/2000, Loss: 0.00018915622786153108\n",
      "Iteration 1941/2000, Loss: 0.0001968189753824845\n",
      "Iteration 1942/2000, Loss: 0.00016700485139153898\n",
      "Iteration 1943/2000, Loss: 0.00018400826957076788\n",
      "Iteration 1944/2000, Loss: 0.0002302181237610057\n",
      "Iteration 1945/2000, Loss: 0.00017849272990133613\n",
      "Iteration 1946/2000, Loss: 0.00014576208195649087\n",
      "Iteration 1947/2000, Loss: 0.0005408732686191797\n",
      "Iteration 1948/2000, Loss: 0.00011732625716831535\n",
      "Iteration 1949/2000, Loss: 0.00015977967996150255\n",
      "Iteration 1950/2000, Loss: 0.00011672764958348125\n",
      "Iteration 1951/2000, Loss: 0.0003857087576761842\n",
      "Iteration 1952/2000, Loss: 0.0001390587567584589\n",
      "Iteration 1953/2000, Loss: 0.00013783096801489592\n",
      "Iteration 1954/2000, Loss: 0.00024069157370831817\n",
      "Iteration 1955/2000, Loss: 0.00016848831728566438\n",
      "Iteration 1956/2000, Loss: 0.00013683225552085787\n",
      "Iteration 1957/2000, Loss: 0.00017512204067315906\n",
      "Iteration 1958/2000, Loss: 0.00013166607823222876\n",
      "Iteration 1959/2000, Loss: 0.00018083659233525395\n",
      "Iteration 1960/2000, Loss: 0.00015694787725806236\n",
      "Iteration 1961/2000, Loss: 0.0001473466691095382\n",
      "Iteration 1962/2000, Loss: 0.00016862711345311254\n",
      "Iteration 1963/2000, Loss: 0.00013926018436904997\n",
      "Iteration 1964/2000, Loss: 0.00012700178194791079\n",
      "Iteration 1965/2000, Loss: 0.00012054563558194786\n",
      "Iteration 1966/2000, Loss: 0.0002156159171136096\n",
      "Iteration 1967/2000, Loss: 0.00021820316032972187\n",
      "Iteration 1968/2000, Loss: 0.00029762217309325933\n",
      "Iteration 1969/2000, Loss: 0.00016328043420799077\n",
      "Iteration 1970/2000, Loss: 0.0003226298722438514\n",
      "Iteration 1971/2000, Loss: 0.0003331423504278064\n",
      "Iteration 1972/2000, Loss: 0.0004268406773917377\n",
      "Iteration 1973/2000, Loss: 0.00020543670689221472\n",
      "Iteration 1974/2000, Loss: 0.0003387454489711672\n",
      "Iteration 1975/2000, Loss: 0.00019308690389152616\n",
      "Iteration 1976/2000, Loss: 0.00020635704277083278\n",
      "Iteration 1977/2000, Loss: 0.00047533761244267225\n",
      "Iteration 1978/2000, Loss: 0.00033350649755448103\n",
      "Iteration 1979/2000, Loss: 0.0001438111357856542\n",
      "Iteration 1980/2000, Loss: 0.00020678527653217316\n",
      "Iteration 1981/2000, Loss: 0.00015046920452732593\n",
      "Iteration 1982/2000, Loss: 0.00027852787752635777\n",
      "Iteration 1983/2000, Loss: 0.00037147727562114596\n",
      "Iteration 1984/2000, Loss: 0.0001507351698819548\n",
      "Iteration 1985/2000, Loss: 0.0002040059625869617\n",
      "Iteration 1986/2000, Loss: 0.00011143286246806383\n",
      "Iteration 1987/2000, Loss: 0.0002247234369860962\n",
      "Iteration 1988/2000, Loss: 0.00021927905618213117\n",
      "Iteration 1989/2000, Loss: 0.0001459622144466266\n",
      "Iteration 1990/2000, Loss: 0.0001979166700039059\n",
      "Iteration 1991/2000, Loss: 0.00014123394794296473\n",
      "Iteration 1992/2000, Loss: 0.0002510213234927505\n",
      "Iteration 1993/2000, Loss: 0.00012319907546043396\n",
      "Iteration 1994/2000, Loss: 0.00017725565703585744\n",
      "Iteration 1995/2000, Loss: 0.00019539156346581876\n",
      "Iteration 1996/2000, Loss: 0.0001423290668753907\n",
      "Iteration 1997/2000, Loss: 0.00020677773864008486\n",
      "Iteration 1998/2000, Loss: 0.0002018982922891155\n",
      "Iteration 1999/2000, Loss: 0.00025819140137173235\n",
      "Iteration 2000/2000, Loss: 0.00027690414572134614\n",
      "Model weights saved after training and testing with linewidth 100000.0 Hz.\n",
      "\n",
      "\n",
      "Testing with linewidth: 100000.0 Hz and Distance: 1000.0 km\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Testing MSE - Linewidth: 100000.0, Link Distance: 1000.0, Original: 0.005574880167841911, Neural Network: 0.012625995092093945\n",
      "\n",
      "Training with linewidth: 100000.0 Hz and Distance: 2000.0 km\n",
      "Iteration 1/2000, Loss: 0.00020877222414128482\n",
      "Iteration 2/2000, Loss: 0.00022887144587002695\n",
      "Iteration 3/2000, Loss: 0.00017187466437462717\n",
      "Iteration 4/2000, Loss: 0.00030751823214814067\n",
      "Iteration 5/2000, Loss: 0.00017055666830856353\n",
      "Iteration 6/2000, Loss: 0.0003499539743643254\n",
      "Iteration 7/2000, Loss: 0.00027830738690681756\n",
      "Iteration 8/2000, Loss: 0.00023313684505410492\n",
      "Iteration 9/2000, Loss: 0.0002066904999082908\n",
      "Iteration 10/2000, Loss: 0.00021784115233458579\n",
      "Iteration 11/2000, Loss: 0.00028076989110559225\n",
      "Iteration 12/2000, Loss: 0.0002675799187272787\n",
      "Iteration 13/2000, Loss: 0.000284228241071105\n",
      "Iteration 14/2000, Loss: 0.00020367687102407217\n",
      "Iteration 15/2000, Loss: 0.00017859450599644333\n",
      "Iteration 16/2000, Loss: 0.00018670926510822028\n",
      "Iteration 17/2000, Loss: 0.000256095256190747\n",
      "Iteration 18/2000, Loss: 0.0002998717245645821\n",
      "Iteration 19/2000, Loss: 0.00021161475160624832\n",
      "Iteration 20/2000, Loss: 0.00026613700902089477\n",
      "Iteration 21/2000, Loss: 0.0004091421142220497\n",
      "Iteration 22/2000, Loss: 0.00037522579077631235\n",
      "Iteration 23/2000, Loss: 0.0002487646124791354\n",
      "Iteration 24/2000, Loss: 0.00029731844551861286\n",
      "Iteration 25/2000, Loss: 0.00040037737926468253\n",
      "Iteration 26/2000, Loss: 0.0002902213600464165\n",
      "Iteration 27/2000, Loss: 0.00020608546037692577\n",
      "Iteration 28/2000, Loss: 0.0004908692208118737\n",
      "Iteration 29/2000, Loss: 0.00018197308236267418\n",
      "Iteration 30/2000, Loss: 0.0004631912161130458\n",
      "Iteration 31/2000, Loss: 0.00032433224259875715\n",
      "Iteration 32/2000, Loss: 0.00022888636158313602\n",
      "Iteration 33/2000, Loss: 0.0003438812855165452\n",
      "Iteration 34/2000, Loss: 0.0002750099520199001\n",
      "Iteration 35/2000, Loss: 0.0004734466492664069\n",
      "Iteration 36/2000, Loss: 0.00043231097515672445\n",
      "Iteration 37/2000, Loss: 0.0003601637145038694\n",
      "Iteration 38/2000, Loss: 0.0006542876362800598\n",
      "Iteration 39/2000, Loss: 0.0003191061841789633\n",
      "Iteration 40/2000, Loss: 0.0004806926299352199\n",
      "Iteration 41/2000, Loss: 0.00024792581098154187\n",
      "Iteration 42/2000, Loss: 0.000389892520615831\n",
      "Iteration 43/2000, Loss: 0.0004373765841592103\n",
      "Iteration 44/2000, Loss: 0.00026940720272250473\n",
      "Iteration 45/2000, Loss: 0.00037831731606274843\n",
      "Iteration 46/2000, Loss: 0.00034244905691593885\n",
      "Iteration 47/2000, Loss: 0.00021329589071683586\n",
      "Iteration 48/2000, Loss: 0.00020077462249901146\n",
      "Iteration 49/2000, Loss: 0.00027985271299257874\n",
      "Iteration 50/2000, Loss: 0.00035110709723085165\n",
      "Iteration 51/2000, Loss: 0.00019308437185827643\n",
      "Iteration 52/2000, Loss: 0.00019613672338891774\n",
      "Iteration 53/2000, Loss: 0.00027060648426413536\n",
      "Iteration 54/2000, Loss: 0.00042926735477522016\n",
      "Iteration 55/2000, Loss: 0.0002445775899104774\n",
      "Iteration 56/2000, Loss: 0.00043443316826596856\n",
      "Iteration 57/2000, Loss: 0.0003377361863385886\n",
      "Iteration 58/2000, Loss: 0.0003076457360293716\n",
      "Iteration 59/2000, Loss: 0.0005531578790396452\n",
      "Iteration 60/2000, Loss: 0.0003157062456011772\n",
      "Iteration 61/2000, Loss: 0.00025643393746577203\n",
      "Iteration 62/2000, Loss: 0.0003027328639291227\n",
      "Iteration 63/2000, Loss: 0.00026547457673586905\n",
      "Iteration 64/2000, Loss: 0.00021793696214444935\n",
      "Iteration 65/2000, Loss: 0.00021538385772146285\n",
      "Iteration 66/2000, Loss: 0.00025920578627847135\n",
      "Iteration 67/2000, Loss: 0.0001724307658150792\n",
      "Iteration 68/2000, Loss: 0.00023988797329366207\n",
      "Iteration 69/2000, Loss: 0.00021091375674586743\n",
      "Iteration 70/2000, Loss: 0.00044924565008841455\n",
      "Iteration 71/2000, Loss: 0.00024596479488536716\n",
      "Iteration 72/2000, Loss: 0.0002123524172930047\n",
      "Iteration 73/2000, Loss: 0.00020188040798529983\n",
      "Iteration 74/2000, Loss: 0.0002553424274083227\n",
      "Iteration 75/2000, Loss: 0.00025105528766289353\n",
      "Iteration 76/2000, Loss: 9.479968866799027e-05\n",
      "Iteration 77/2000, Loss: 0.00016098839114420116\n",
      "Iteration 78/2000, Loss: 0.0001713348610792309\n",
      "Iteration 79/2000, Loss: 0.00016239951946772635\n",
      "Iteration 80/2000, Loss: 0.00020384960225783288\n",
      "Iteration 81/2000, Loss: 0.00016722330474294722\n",
      "Iteration 82/2000, Loss: 0.00028074640431441367\n",
      "Iteration 83/2000, Loss: 0.0006781640695407987\n",
      "Iteration 84/2000, Loss: 0.00015594533761031926\n",
      "Iteration 85/2000, Loss: 0.0001377536536892876\n",
      "Iteration 86/2000, Loss: 0.00016329583013430238\n",
      "Iteration 87/2000, Loss: 0.0002247812954010442\n",
      "Iteration 88/2000, Loss: 0.00022367568453773856\n",
      "Iteration 89/2000, Loss: 0.0005820202059112489\n",
      "Iteration 90/2000, Loss: 0.0001526379055576399\n",
      "Iteration 91/2000, Loss: 0.00015033710224088281\n",
      "Iteration 92/2000, Loss: 0.0003754790814127773\n",
      "Iteration 93/2000, Loss: 0.00017900211969390512\n",
      "Iteration 94/2000, Loss: 0.00027270594728179276\n",
      "Iteration 95/2000, Loss: 0.00011910447210539132\n",
      "Iteration 96/2000, Loss: 0.0004025602829642594\n",
      "Iteration 97/2000, Loss: 0.0002718665637075901\n",
      "Iteration 98/2000, Loss: 0.00030171588878147304\n",
      "Iteration 99/2000, Loss: 0.00019209209131076932\n",
      "Iteration 100/2000, Loss: 0.000165088742505759\n",
      "Iteration 101/2000, Loss: 0.000331179762724787\n",
      "Iteration 102/2000, Loss: 0.0003195301105733961\n",
      "Iteration 103/2000, Loss: 0.000175401073647663\n",
      "Iteration 104/2000, Loss: 0.00021679315250366926\n",
      "Iteration 105/2000, Loss: 0.0002103387814713642\n",
      "Iteration 106/2000, Loss: 0.0014541716082021594\n",
      "Iteration 107/2000, Loss: 0.00023383974621538073\n",
      "Iteration 108/2000, Loss: 0.00017112033674493432\n",
      "Iteration 109/2000, Loss: 0.00026918540243059397\n",
      "Iteration 110/2000, Loss: 0.0002495295775588602\n",
      "Iteration 111/2000, Loss: 0.00026313899434171617\n",
      "Iteration 112/2000, Loss: 0.00025277433451265097\n",
      "Iteration 113/2000, Loss: 0.00033082879963330925\n",
      "Iteration 114/2000, Loss: 0.00028766048490069807\n",
      "Iteration 115/2000, Loss: 0.0002800411602947861\n",
      "Iteration 116/2000, Loss: 0.00027255481109023094\n",
      "Iteration 117/2000, Loss: 0.00032718037255108356\n",
      "Iteration 118/2000, Loss: 0.00024977343855425715\n",
      "Iteration 119/2000, Loss: 0.00038541131652891636\n",
      "Iteration 120/2000, Loss: 0.0003011080843862146\n",
      "Iteration 121/2000, Loss: 0.00028518051840364933\n",
      "Iteration 122/2000, Loss: 0.0002847074647434056\n",
      "Iteration 123/2000, Loss: 0.0002333108423044905\n",
      "Iteration 124/2000, Loss: 0.00015635069576092064\n",
      "Iteration 125/2000, Loss: 0.00019362759485375136\n",
      "Iteration 126/2000, Loss: 0.0002610208175610751\n",
      "Iteration 127/2000, Loss: 0.00016711722128093243\n",
      "Iteration 128/2000, Loss: 0.0003277465875726193\n",
      "Iteration 129/2000, Loss: 0.00015163913485594094\n",
      "Iteration 130/2000, Loss: 0.0002887322916649282\n",
      "Iteration 131/2000, Loss: 0.0002486002922523767\n",
      "Iteration 132/2000, Loss: 0.00026389193953946233\n",
      "Iteration 133/2000, Loss: 0.0002512976061552763\n",
      "Iteration 134/2000, Loss: 0.0001665312156546861\n",
      "Iteration 135/2000, Loss: 0.000252650206675753\n",
      "Iteration 136/2000, Loss: 0.00032457508496008813\n",
      "Iteration 137/2000, Loss: 0.0002623434120323509\n",
      "Iteration 138/2000, Loss: 0.0002534555096644908\n",
      "Iteration 139/2000, Loss: 0.00020586967002600431\n",
      "Iteration 140/2000, Loss: 0.00035349285462871194\n",
      "Iteration 141/2000, Loss: 0.00016505726671312004\n",
      "Iteration 142/2000, Loss: 0.00018354228814132512\n",
      "Iteration 143/2000, Loss: 0.00024119229055941105\n",
      "Iteration 144/2000, Loss: 0.00016074122686404735\n",
      "Iteration 145/2000, Loss: 0.00031101275817491114\n",
      "Iteration 146/2000, Loss: 0.00013281975407153368\n",
      "Iteration 147/2000, Loss: 0.0001844201033236459\n",
      "Iteration 148/2000, Loss: 0.00011885743151651695\n",
      "Iteration 149/2000, Loss: 0.0001549034786876291\n",
      "Iteration 150/2000, Loss: 0.00016916054300963879\n",
      "Iteration 151/2000, Loss: 9.358611714560539e-05\n",
      "Iteration 152/2000, Loss: 0.0002069475594907999\n",
      "Iteration 153/2000, Loss: 0.00021584620117209852\n",
      "Iteration 154/2000, Loss: 0.0001872711000032723\n",
      "Iteration 155/2000, Loss: 0.00016316505207214504\n",
      "Iteration 156/2000, Loss: 0.00015698783681727946\n",
      "Iteration 157/2000, Loss: 0.0002755401364993304\n",
      "Iteration 158/2000, Loss: 0.00015960147720761597\n",
      "Iteration 159/2000, Loss: 0.00024289199791382998\n",
      "Iteration 160/2000, Loss: 0.00022197597718331963\n",
      "Iteration 161/2000, Loss: 0.00040185078978538513\n",
      "Iteration 162/2000, Loss: 0.0002643733168952167\n",
      "Iteration 163/2000, Loss: 0.00016501743812114\n",
      "Iteration 164/2000, Loss: 0.0001719376159599051\n",
      "Iteration 165/2000, Loss: 0.0002572549565229565\n",
      "Iteration 166/2000, Loss: 0.00030339235672727227\n",
      "Iteration 167/2000, Loss: 0.0003909694205503911\n",
      "Iteration 168/2000, Loss: 0.0003251840244047344\n",
      "Iteration 169/2000, Loss: 0.00027887843316420913\n",
      "Iteration 170/2000, Loss: 0.00027971601230092347\n",
      "Iteration 171/2000, Loss: 0.00040638071368448436\n",
      "Iteration 172/2000, Loss: 0.0002775912289507687\n",
      "Iteration 173/2000, Loss: 0.00033019346301443875\n",
      "Iteration 174/2000, Loss: 0.00022801563318353146\n",
      "Iteration 175/2000, Loss: 0.0001989740994758904\n",
      "Iteration 176/2000, Loss: 0.0004382143961265683\n",
      "Iteration 177/2000, Loss: 0.00017744327487889677\n",
      "Iteration 178/2000, Loss: 0.00040588813135400414\n",
      "Iteration 179/2000, Loss: 0.00023579297703690827\n",
      "Iteration 180/2000, Loss: 0.00048628292279317975\n",
      "Iteration 181/2000, Loss: 0.0004015364102087915\n",
      "Iteration 182/2000, Loss: 0.000207040662644431\n",
      "Iteration 183/2000, Loss: 0.0006140550249256194\n",
      "Iteration 184/2000, Loss: 0.0005991020589135587\n",
      "Iteration 185/2000, Loss: 0.0003876319678965956\n",
      "Iteration 186/2000, Loss: 0.00031629728619009256\n",
      "Iteration 187/2000, Loss: 0.0005136798718012869\n",
      "Iteration 188/2000, Loss: 0.00042732173460535705\n",
      "Iteration 189/2000, Loss: 0.0002602098975330591\n",
      "Iteration 190/2000, Loss: 0.0004973674076609313\n",
      "Iteration 191/2000, Loss: 0.00020303459314163774\n",
      "Iteration 192/2000, Loss: 0.00021773176558781415\n",
      "Iteration 193/2000, Loss: 0.00017185992328450084\n",
      "Iteration 194/2000, Loss: 0.00019319888087920845\n",
      "Iteration 195/2000, Loss: 0.0001372677506878972\n",
      "Iteration 196/2000, Loss: 0.00027513407985679805\n",
      "Iteration 197/2000, Loss: 0.0002570384822320193\n",
      "Iteration 198/2000, Loss: 0.00015140921459533274\n",
      "Iteration 199/2000, Loss: 0.00019406819774303585\n",
      "Iteration 200/2000, Loss: 0.0002573431993369013\n",
      "Iteration 201/2000, Loss: 0.0002855205093510449\n",
      "Iteration 202/2000, Loss: 0.00037034362321719527\n",
      "Iteration 203/2000, Loss: 0.0002595487458165735\n",
      "Iteration 204/2000, Loss: 0.00010866887896554545\n",
      "Iteration 205/2000, Loss: 0.0001852517161751166\n",
      "Iteration 206/2000, Loss: 0.0002957331307698041\n",
      "Iteration 207/2000, Loss: 0.00036917778197675943\n",
      "Iteration 208/2000, Loss: 0.00018492178060114384\n",
      "Iteration 209/2000, Loss: 0.0003265310951974243\n",
      "Iteration 210/2000, Loss: 0.00028186888084746897\n",
      "Iteration 211/2000, Loss: 0.0001492288283770904\n",
      "Iteration 212/2000, Loss: 0.00021662938524968922\n",
      "Iteration 213/2000, Loss: 0.0001324351760558784\n",
      "Iteration 214/2000, Loss: 0.0001271599467145279\n",
      "Iteration 215/2000, Loss: 0.00019380109733901918\n",
      "Iteration 216/2000, Loss: 0.0001570777385495603\n",
      "Iteration 217/2000, Loss: 0.00021869802731089294\n",
      "Iteration 218/2000, Loss: 0.00016467852401547134\n",
      "Iteration 219/2000, Loss: 0.00020526029402390122\n",
      "Iteration 220/2000, Loss: 9.713361941976473e-05\n",
      "Iteration 221/2000, Loss: 0.00022017302399035543\n",
      "Iteration 222/2000, Loss: 0.0003333087370265275\n",
      "Iteration 223/2000, Loss: 0.0002545901807025075\n",
      "Iteration 224/2000, Loss: 0.00026555193471722305\n",
      "Iteration 225/2000, Loss: 0.00015394955698866397\n",
      "Iteration 226/2000, Loss: 0.00024658447364345193\n",
      "Iteration 227/2000, Loss: 0.00021261653455439955\n",
      "Iteration 228/2000, Loss: 0.00030752591555938125\n",
      "Iteration 229/2000, Loss: 0.0002292304707225412\n",
      "Iteration 230/2000, Loss: 0.00014305231161415577\n",
      "Iteration 231/2000, Loss: 0.0001435903977835551\n",
      "Iteration 232/2000, Loss: 0.00022838158474769443\n",
      "Iteration 233/2000, Loss: 0.00029127325979061425\n",
      "Iteration 234/2000, Loss: 0.00020503725681919605\n",
      "Iteration 235/2000, Loss: 0.00013409345410764217\n",
      "Iteration 236/2000, Loss: 0.00023430943838320673\n",
      "Iteration 237/2000, Loss: 0.00019126056577078998\n",
      "Iteration 238/2000, Loss: 0.00042677554301917553\n",
      "Iteration 239/2000, Loss: 0.0008229994564317167\n",
      "Iteration 240/2000, Loss: 0.0002638504665810615\n",
      "Iteration 241/2000, Loss: 0.00014456738426815718\n",
      "Iteration 242/2000, Loss: 0.00015871984942350537\n",
      "Iteration 243/2000, Loss: 0.0001576823997311294\n",
      "Iteration 244/2000, Loss: 0.00023655629775021225\n",
      "Iteration 245/2000, Loss: 0.00018635907326824963\n",
      "Iteration 246/2000, Loss: 0.00020267744548618793\n",
      "Iteration 247/2000, Loss: 0.0002597421407699585\n",
      "Iteration 248/2000, Loss: 0.0003796853416133672\n",
      "Iteration 249/2000, Loss: 0.00023000244982540607\n",
      "Iteration 250/2000, Loss: 0.0001966531272046268\n",
      "Iteration 251/2000, Loss: 0.00019187777070328593\n",
      "Iteration 252/2000, Loss: 0.00024841210688464344\n",
      "Iteration 253/2000, Loss: 0.00022657361114397645\n",
      "Iteration 254/2000, Loss: 0.00025445607025176287\n",
      "Iteration 255/2000, Loss: 0.0004467677208594978\n",
      "Iteration 256/2000, Loss: 0.00024943030439317226\n",
      "Iteration 257/2000, Loss: 0.00012574635911732912\n",
      "Iteration 258/2000, Loss: 0.00017164544260594994\n",
      "Iteration 259/2000, Loss: 0.00025797553826123476\n",
      "Iteration 260/2000, Loss: 0.00012557573791127652\n",
      "Iteration 261/2000, Loss: 0.00018761843966785818\n",
      "Iteration 262/2000, Loss: 0.00019580175285227597\n",
      "Iteration 263/2000, Loss: 0.0001652456703595817\n",
      "Iteration 264/2000, Loss: 0.0001511601876700297\n",
      "Iteration 265/2000, Loss: 0.00012663629604503512\n",
      "Iteration 266/2000, Loss: 0.00010970707080559805\n",
      "Iteration 267/2000, Loss: 0.00010839564492926002\n",
      "Iteration 268/2000, Loss: 0.00015212889411486685\n",
      "Iteration 269/2000, Loss: 0.0002755222376435995\n",
      "Iteration 270/2000, Loss: 0.0001581433753017336\n",
      "Iteration 271/2000, Loss: 0.00016512117872480303\n",
      "Iteration 272/2000, Loss: 0.0001229843619512394\n",
      "Iteration 273/2000, Loss: 0.00016487423272337765\n",
      "Iteration 274/2000, Loss: 0.00012580027396325022\n",
      "Iteration 275/2000, Loss: 0.0002289032272528857\n",
      "Iteration 276/2000, Loss: 0.0001856548769865185\n",
      "Iteration 277/2000, Loss: 0.00024254355230368674\n",
      "Iteration 278/2000, Loss: 0.00028940074844285846\n",
      "Iteration 279/2000, Loss: 0.00023514266649726778\n",
      "Iteration 280/2000, Loss: 0.00013026371016167104\n",
      "Iteration 281/2000, Loss: 0.0001769111113389954\n",
      "Iteration 282/2000, Loss: 0.00034116298775188625\n",
      "Iteration 283/2000, Loss: 0.00031435300479643047\n",
      "Iteration 284/2000, Loss: 0.0005291569977998734\n",
      "Iteration 285/2000, Loss: 0.00024955786648206413\n",
      "Iteration 286/2000, Loss: 0.00030126364436000586\n",
      "Iteration 287/2000, Loss: 0.00020155060337856412\n",
      "Iteration 288/2000, Loss: 0.00020622863667085767\n",
      "Iteration 289/2000, Loss: 0.0002008312731049955\n",
      "Iteration 290/2000, Loss: 0.00020339491311460733\n",
      "Iteration 291/2000, Loss: 0.00020283303456380963\n",
      "Iteration 292/2000, Loss: 0.00023185722238849849\n",
      "Iteration 293/2000, Loss: 0.00011102892312919721\n",
      "Iteration 294/2000, Loss: 0.0001766777568263933\n",
      "Iteration 295/2000, Loss: 0.00016144053370226175\n",
      "Iteration 296/2000, Loss: 0.00014726496010553092\n",
      "Iteration 297/2000, Loss: 0.00015191295824479312\n",
      "Iteration 298/2000, Loss: 0.0001416117447661236\n",
      "Iteration 299/2000, Loss: 0.00026067867293022573\n",
      "Iteration 300/2000, Loss: 0.00019970997527707368\n",
      "Iteration 301/2000, Loss: 0.00015004696615505964\n",
      "Iteration 302/2000, Loss: 0.00016788365610409528\n",
      "Iteration 303/2000, Loss: 0.00013867516827303916\n",
      "Iteration 304/2000, Loss: 0.00017234435654245317\n",
      "Iteration 305/2000, Loss: 0.00019192924082744867\n",
      "Iteration 306/2000, Loss: 0.00019415242422837764\n",
      "Iteration 307/2000, Loss: 0.00011987442849203944\n",
      "Iteration 308/2000, Loss: 0.0001760418963385746\n",
      "Iteration 309/2000, Loss: 0.0003068991645704955\n",
      "Iteration 310/2000, Loss: 0.0001529244182165712\n",
      "Iteration 311/2000, Loss: 0.00022248506138566881\n",
      "Iteration 312/2000, Loss: 0.00010365787602495402\n",
      "Iteration 313/2000, Loss: 0.0002668664965312928\n",
      "Iteration 314/2000, Loss: 0.00015358132077381015\n",
      "Iteration 315/2000, Loss: 0.0001228453329531476\n",
      "Iteration 316/2000, Loss: 0.00028503756038844585\n",
      "Iteration 317/2000, Loss: 0.00010930527787422761\n",
      "Iteration 318/2000, Loss: 0.00019102149235550314\n",
      "Iteration 319/2000, Loss: 0.00010280899732606485\n",
      "Iteration 320/2000, Loss: 0.00010234780347673222\n",
      "Iteration 321/2000, Loss: 0.00023772177519276738\n",
      "Iteration 322/2000, Loss: 0.00015403490397147834\n",
      "Iteration 323/2000, Loss: 0.00017159855633508414\n",
      "Iteration 324/2000, Loss: 0.00021784041018690914\n",
      "Iteration 325/2000, Loss: 0.00020464014960452914\n",
      "Iteration 326/2000, Loss: 0.00017954788927454501\n",
      "Iteration 327/2000, Loss: 0.00012160335609223694\n",
      "Iteration 328/2000, Loss: 0.00010948708222713321\n",
      "Iteration 329/2000, Loss: 0.0002717045717872679\n",
      "Iteration 330/2000, Loss: 0.00017575075617060065\n",
      "Iteration 331/2000, Loss: 0.0002561529690865427\n",
      "Iteration 332/2000, Loss: 0.00022055418230593204\n",
      "Iteration 333/2000, Loss: 0.00019280593551229686\n",
      "Iteration 334/2000, Loss: 0.0001505885156802833\n",
      "Iteration 335/2000, Loss: 0.00012135675206081942\n",
      "Iteration 336/2000, Loss: 0.00018588616512715816\n",
      "Iteration 337/2000, Loss: 0.00020820011559408158\n",
      "Iteration 338/2000, Loss: 0.00020382774528115988\n",
      "Iteration 339/2000, Loss: 0.00027651985874399543\n",
      "Iteration 340/2000, Loss: 0.00017080214456655085\n",
      "Iteration 341/2000, Loss: 9.995623986469582e-05\n",
      "Iteration 342/2000, Loss: 0.00023406500986311585\n",
      "Iteration 343/2000, Loss: 0.0001580424141138792\n",
      "Iteration 344/2000, Loss: 0.00014993498916737735\n",
      "Iteration 345/2000, Loss: 0.0003561376652214676\n",
      "Iteration 346/2000, Loss: 0.00010437768651172519\n",
      "Iteration 347/2000, Loss: 0.00010082784865517169\n",
      "Iteration 348/2000, Loss: 0.00020170604693703353\n",
      "Iteration 349/2000, Loss: 8.621974848210812e-05\n",
      "Iteration 350/2000, Loss: 0.00018787605222314596\n",
      "Iteration 351/2000, Loss: 0.00016384250193368644\n",
      "Iteration 352/2000, Loss: 0.00010415633005322888\n",
      "Iteration 353/2000, Loss: 0.0001379684399580583\n",
      "Iteration 354/2000, Loss: 0.00017252794350497425\n",
      "Iteration 355/2000, Loss: 0.00034422086901031435\n",
      "Iteration 356/2000, Loss: 0.00014657581050414592\n",
      "Iteration 357/2000, Loss: 0.00014416707563214004\n",
      "Iteration 358/2000, Loss: 0.00020716688595712185\n",
      "Iteration 359/2000, Loss: 0.00023970060283318162\n",
      "Iteration 360/2000, Loss: 0.00014497437223326415\n",
      "Iteration 361/2000, Loss: 0.00013093810412101448\n",
      "Iteration 362/2000, Loss: 0.0006572912097908556\n",
      "Iteration 363/2000, Loss: 0.0003211958974134177\n",
      "Iteration 364/2000, Loss: 0.00020128858159296215\n",
      "Iteration 365/2000, Loss: 0.00025973893934860826\n",
      "Iteration 366/2000, Loss: 0.0003891211817972362\n",
      "Iteration 367/2000, Loss: 0.00023150369815994054\n",
      "Iteration 368/2000, Loss: 0.00021375813230406493\n",
      "Iteration 369/2000, Loss: 0.00021013023797422647\n",
      "Iteration 370/2000, Loss: 0.00020893964392598718\n",
      "Iteration 371/2000, Loss: 0.0001362168841296807\n",
      "Iteration 372/2000, Loss: 0.00014258152805268764\n",
      "Iteration 373/2000, Loss: 0.0001877273607533425\n",
      "Iteration 374/2000, Loss: 0.0001418041647411883\n",
      "Iteration 375/2000, Loss: 0.00014968284813221544\n",
      "Iteration 376/2000, Loss: 0.0001391803816659376\n",
      "Iteration 377/2000, Loss: 0.00032377257593907416\n",
      "Iteration 378/2000, Loss: 0.00023567785683553666\n",
      "Iteration 379/2000, Loss: 0.0001838234456954524\n",
      "Iteration 380/2000, Loss: 0.00020289835811126977\n",
      "Iteration 381/2000, Loss: 0.00031879550078883767\n",
      "Iteration 382/2000, Loss: 0.0002320185594726354\n",
      "Iteration 383/2000, Loss: 0.00026819430058822036\n",
      "Iteration 384/2000, Loss: 0.0004255789681337774\n",
      "Iteration 385/2000, Loss: 0.00027517223497852683\n",
      "Iteration 386/2000, Loss: 0.000537226558662951\n",
      "Iteration 387/2000, Loss: 0.0002870323078241199\n",
      "Iteration 388/2000, Loss: 0.0002586542977951467\n",
      "Iteration 389/2000, Loss: 0.00024410626792814583\n",
      "Iteration 390/2000, Loss: 0.00016920878260862082\n",
      "Iteration 391/2000, Loss: 0.00022778722632210702\n",
      "Iteration 392/2000, Loss: 0.00029117465601302683\n",
      "Iteration 393/2000, Loss: 0.0005359242204576731\n",
      "Iteration 394/2000, Loss: 0.00010659599502105266\n",
      "Iteration 395/2000, Loss: 0.0001964677794603631\n",
      "Iteration 396/2000, Loss: 0.0005051567568443716\n",
      "Iteration 397/2000, Loss: 0.000275891536148265\n",
      "Iteration 398/2000, Loss: 0.00020126355229876935\n",
      "Iteration 399/2000, Loss: 0.00027286767726764083\n",
      "Iteration 400/2000, Loss: 0.0001919145870488137\n",
      "Iteration 401/2000, Loss: 0.0003574614238459617\n",
      "Iteration 402/2000, Loss: 0.0002296596358064562\n",
      "Iteration 403/2000, Loss: 0.0004081530205439776\n",
      "Iteration 404/2000, Loss: 0.0001960288209374994\n",
      "Iteration 405/2000, Loss: 0.0003433393139857799\n",
      "Iteration 406/2000, Loss: 0.0002843729453161359\n",
      "Iteration 407/2000, Loss: 0.0003417813859414309\n",
      "Iteration 408/2000, Loss: 0.000274999241810292\n",
      "Iteration 409/2000, Loss: 0.0003670891164802015\n",
      "Iteration 410/2000, Loss: 0.0005055150832049549\n",
      "Iteration 411/2000, Loss: 0.0002721318742260337\n",
      "Iteration 412/2000, Loss: 0.0004707105690613389\n",
      "Iteration 413/2000, Loss: 0.0002305171510670334\n",
      "Iteration 414/2000, Loss: 0.00062230956973508\n",
      "Iteration 415/2000, Loss: 0.0003757122321985662\n",
      "Iteration 416/2000, Loss: 0.0003347003075759858\n",
      "Iteration 417/2000, Loss: 0.0005270792171359062\n",
      "Iteration 418/2000, Loss: 0.00025797204580157995\n",
      "Iteration 419/2000, Loss: 0.0005232827388681471\n",
      "Iteration 420/2000, Loss: 0.0002976549731101841\n",
      "Iteration 421/2000, Loss: 0.0006275797495618463\n",
      "Iteration 422/2000, Loss: 0.00028616480994969606\n",
      "Iteration 423/2000, Loss: 0.00046064844354987144\n",
      "Iteration 424/2000, Loss: 0.0003870959335472435\n",
      "Iteration 425/2000, Loss: 0.00020770239643752575\n",
      "Iteration 426/2000, Loss: 0.0004164386773481965\n",
      "Iteration 427/2000, Loss: 0.00022080237977206707\n",
      "Iteration 428/2000, Loss: 0.0002346416440559551\n",
      "Iteration 429/2000, Loss: 0.00015198766777757555\n",
      "Iteration 430/2000, Loss: 0.0001586342550581321\n",
      "Iteration 431/2000, Loss: 0.0001685958995949477\n",
      "Iteration 432/2000, Loss: 0.00017345832020509988\n",
      "Iteration 433/2000, Loss: 0.00011269251990597695\n",
      "Iteration 434/2000, Loss: 0.00013588261208496988\n",
      "Iteration 435/2000, Loss: 0.00017102636047638953\n",
      "Iteration 436/2000, Loss: 0.00030502476147376\n",
      "Iteration 437/2000, Loss: 0.0004950694274157286\n",
      "Iteration 438/2000, Loss: 0.00021806111908517778\n",
      "Iteration 439/2000, Loss: 0.00016884892829693854\n",
      "Iteration 440/2000, Loss: 0.00018724436813499779\n",
      "Iteration 441/2000, Loss: 0.0001802765909815207\n",
      "Iteration 442/2000, Loss: 0.0002508086326997727\n",
      "Iteration 443/2000, Loss: 0.0001756897399900481\n",
      "Iteration 444/2000, Loss: 0.00027392629999667406\n",
      "Iteration 445/2000, Loss: 0.00013478705659508705\n",
      "Iteration 446/2000, Loss: 0.00017229373042937368\n",
      "Iteration 447/2000, Loss: 0.0002917649399023503\n",
      "Iteration 448/2000, Loss: 0.0001081733571481891\n",
      "Iteration 449/2000, Loss: 0.00017646551714278758\n",
      "Iteration 450/2000, Loss: 0.00011928952881135046\n",
      "Iteration 451/2000, Loss: 0.0002508738834876567\n",
      "Iteration 452/2000, Loss: 0.00022241813712753356\n",
      "Iteration 453/2000, Loss: 0.0001462680520489812\n",
      "Iteration 454/2000, Loss: 0.0002832055906765163\n",
      "Iteration 455/2000, Loss: 0.00019082282960880548\n",
      "Iteration 456/2000, Loss: 0.0002763767843134701\n",
      "Iteration 457/2000, Loss: 0.0002872339973691851\n",
      "Iteration 458/2000, Loss: 0.00010813695553224534\n",
      "Iteration 459/2000, Loss: 0.00020256609423086047\n",
      "Iteration 460/2000, Loss: 0.00022021174663677812\n",
      "Iteration 461/2000, Loss: 0.00024222853244282305\n",
      "Iteration 462/2000, Loss: 0.00020822248188778758\n",
      "Iteration 463/2000, Loss: 0.0004473473527468741\n",
      "Iteration 464/2000, Loss: 0.00021319404186215252\n",
      "Iteration 465/2000, Loss: 0.00047270822688005865\n",
      "Iteration 466/2000, Loss: 0.0002426601859042421\n",
      "Iteration 467/2000, Loss: 0.00027874225634150207\n",
      "Iteration 468/2000, Loss: 0.0002738645125646144\n",
      "Iteration 469/2000, Loss: 0.00026843190426006913\n",
      "Iteration 470/2000, Loss: 0.00018465060566086322\n",
      "Iteration 471/2000, Loss: 0.0003371581551618874\n",
      "Iteration 472/2000, Loss: 0.00029566942248493433\n",
      "Iteration 473/2000, Loss: 0.00028469489188864827\n",
      "Iteration 474/2000, Loss: 0.0002980329445563257\n",
      "Iteration 475/2000, Loss: 0.00020724505884572864\n",
      "Iteration 476/2000, Loss: 0.0002812774619087577\n",
      "Iteration 477/2000, Loss: 0.0002429204760119319\n",
      "Iteration 478/2000, Loss: 0.0002400431112619117\n",
      "Iteration 479/2000, Loss: 0.0003473056713119149\n",
      "Iteration 480/2000, Loss: 0.0003132360870949924\n",
      "Iteration 481/2000, Loss: 0.0004491692525334656\n",
      "Iteration 482/2000, Loss: 0.0006268639699555933\n",
      "Iteration 483/2000, Loss: 0.00019841003813780844\n",
      "Iteration 484/2000, Loss: 0.001036743400618434\n",
      "Iteration 485/2000, Loss: 0.00021357192599680275\n",
      "Iteration 486/2000, Loss: 0.0005869767046533525\n",
      "Iteration 487/2000, Loss: 0.0004906437825411558\n",
      "Iteration 488/2000, Loss: 0.00037103460635989904\n",
      "Iteration 489/2000, Loss: 0.0008627482457086444\n",
      "Iteration 490/2000, Loss: 0.0003341608971823007\n",
      "Iteration 491/2000, Loss: 0.000945432111620903\n",
      "Iteration 492/2000, Loss: 0.00046255398774519563\n",
      "Iteration 493/2000, Loss: 0.0006612291908822954\n",
      "Iteration 494/2000, Loss: 0.000595164718106389\n",
      "Iteration 495/2000, Loss: 0.0007043052464723587\n",
      "Iteration 496/2000, Loss: 0.0012306617572903633\n",
      "Iteration 497/2000, Loss: 0.0003405452298466116\n",
      "Iteration 498/2000, Loss: 0.0007070381543599069\n",
      "Iteration 499/2000, Loss: 0.0003688093856908381\n",
      "Iteration 500/2000, Loss: 0.0007558586075901985\n",
      "Iteration 501/2000, Loss: 0.00048445205902680755\n",
      "Iteration 502/2000, Loss: 0.00046245340490713716\n",
      "Iteration 503/2000, Loss: 0.0005780942738056183\n",
      "Iteration 504/2000, Loss: 0.0005598050192929804\n",
      "Iteration 505/2000, Loss: 0.0005810180446133018\n",
      "Iteration 506/2000, Loss: 0.0005018386873416603\n",
      "Iteration 507/2000, Loss: 0.00046262098476290703\n",
      "Iteration 508/2000, Loss: 0.00033025359152816236\n",
      "Iteration 509/2000, Loss: 0.0005025404971092939\n",
      "Iteration 510/2000, Loss: 0.00033578771399334073\n",
      "Iteration 511/2000, Loss: 0.0004690547939389944\n",
      "Iteration 512/2000, Loss: 0.00018256240582559258\n",
      "Iteration 513/2000, Loss: 0.0005501903942786157\n",
      "Iteration 514/2000, Loss: 0.00015703102690167725\n",
      "Iteration 515/2000, Loss: 0.00043047923827543855\n",
      "Iteration 516/2000, Loss: 0.00013668739120475948\n",
      "Iteration 517/2000, Loss: 0.0005530256312340498\n",
      "Iteration 518/2000, Loss: 0.00030028060427866876\n",
      "Iteration 519/2000, Loss: 0.0005612103268504143\n",
      "Iteration 520/2000, Loss: 0.0002418026706436649\n",
      "Iteration 521/2000, Loss: 0.0003814623923972249\n",
      "Iteration 522/2000, Loss: 0.0003010733053088188\n",
      "Iteration 523/2000, Loss: 0.00024425884475931525\n",
      "Iteration 524/2000, Loss: 0.000225007432163693\n",
      "Iteration 525/2000, Loss: 0.0003063569311052561\n",
      "Iteration 526/2000, Loss: 0.00020922489056829363\n",
      "Iteration 527/2000, Loss: 0.00026385969249531627\n",
      "Iteration 528/2000, Loss: 0.00026574046933092177\n",
      "Iteration 529/2000, Loss: 0.0004323499742895365\n",
      "Iteration 530/2000, Loss: 0.00032102203113026917\n",
      "Iteration 531/2000, Loss: 0.00025947761605493724\n",
      "Iteration 532/2000, Loss: 0.0005739204352721572\n",
      "Iteration 533/2000, Loss: 0.0002788602723740041\n",
      "Iteration 534/2000, Loss: 0.0002490275364834815\n",
      "Iteration 535/2000, Loss: 0.0001977743668248877\n",
      "Iteration 536/2000, Loss: 0.0006650672876276076\n",
      "Iteration 537/2000, Loss: 0.00020413010497577488\n",
      "Iteration 538/2000, Loss: 0.0002688755630515516\n",
      "Iteration 539/2000, Loss: 0.00012299770605750382\n",
      "Iteration 540/2000, Loss: 0.00023686961503699422\n",
      "Iteration 541/2000, Loss: 0.00013465172378346324\n",
      "Iteration 542/2000, Loss: 0.00029794819420203567\n",
      "Iteration 543/2000, Loss: 9.738095104694366e-05\n",
      "Iteration 544/2000, Loss: 0.00033930622157640755\n",
      "Iteration 545/2000, Loss: 0.00026952591724693775\n",
      "Iteration 546/2000, Loss: 0.0002638284640852362\n",
      "Iteration 547/2000, Loss: 0.00023414561292156577\n",
      "Iteration 548/2000, Loss: 0.0003072998661082238\n",
      "Iteration 549/2000, Loss: 0.0007056366885080934\n",
      "Iteration 550/2000, Loss: 0.00042626456706784666\n",
      "Iteration 551/2000, Loss: 0.00023342919303104281\n",
      "Iteration 552/2000, Loss: 0.00014975557860452682\n",
      "Iteration 553/2000, Loss: 0.000256899103987962\n",
      "Iteration 554/2000, Loss: 0.00017859249783214182\n",
      "Iteration 555/2000, Loss: 0.00015243879170157015\n",
      "Iteration 556/2000, Loss: 0.00020394079911056906\n",
      "Iteration 557/2000, Loss: 0.00026303529739379883\n",
      "Iteration 558/2000, Loss: 0.00022335966059472412\n",
      "Iteration 559/2000, Loss: 0.0001910862047225237\n",
      "Iteration 560/2000, Loss: 0.0003031429660040885\n",
      "Iteration 561/2000, Loss: 0.00023745208454784006\n",
      "Iteration 562/2000, Loss: 0.00021406097221188247\n",
      "Iteration 563/2000, Loss: 0.0002280173503095284\n",
      "Iteration 564/2000, Loss: 0.0001311892701778561\n",
      "Iteration 565/2000, Loss: 0.0002820707450155169\n",
      "Iteration 566/2000, Loss: 0.00023871425946708769\n",
      "Iteration 567/2000, Loss: 0.0003978199092671275\n",
      "Iteration 568/2000, Loss: 0.00014080444816499949\n",
      "Iteration 569/2000, Loss: 0.00026071109459735453\n",
      "Iteration 570/2000, Loss: 0.00035744436900131404\n",
      "Iteration 571/2000, Loss: 0.0003655157925095409\n",
      "Iteration 572/2000, Loss: 0.00028065929654985666\n",
      "Iteration 573/2000, Loss: 0.00030872804927639663\n",
      "Iteration 574/2000, Loss: 0.00021485670004040003\n",
      "Iteration 575/2000, Loss: 0.0001956991764018312\n",
      "Iteration 576/2000, Loss: 0.00035542334080673754\n",
      "Iteration 577/2000, Loss: 0.00023501920804847032\n",
      "Iteration 578/2000, Loss: 0.0002157299022655934\n",
      "Iteration 579/2000, Loss: 0.00021598939201794565\n",
      "Iteration 580/2000, Loss: 0.00010477943578734994\n",
      "Iteration 581/2000, Loss: 0.00021430615743156523\n",
      "Iteration 582/2000, Loss: 0.00017856662452686578\n",
      "Iteration 583/2000, Loss: 0.0001482835941715166\n",
      "Iteration 584/2000, Loss: 0.00026776394224725664\n",
      "Iteration 585/2000, Loss: 0.00018623750656843185\n",
      "Iteration 586/2000, Loss: 0.00028631798340938985\n",
      "Iteration 587/2000, Loss: 0.00024294467584695667\n",
      "Iteration 588/2000, Loss: 0.00022254955547396094\n",
      "Iteration 589/2000, Loss: 0.0006783848511986434\n",
      "Iteration 590/2000, Loss: 0.00016149829025380313\n",
      "Iteration 591/2000, Loss: 0.000135461610625498\n",
      "Iteration 592/2000, Loss: 0.0001278667914448306\n",
      "Iteration 593/2000, Loss: 0.0002184932673117146\n",
      "Iteration 594/2000, Loss: 0.0002384218678344041\n",
      "Iteration 595/2000, Loss: 0.00016003382916096598\n",
      "Iteration 596/2000, Loss: 0.0004493624728638679\n",
      "Iteration 597/2000, Loss: 0.0001371115358779207\n",
      "Iteration 598/2000, Loss: 0.00021014279627706856\n",
      "Iteration 599/2000, Loss: 0.00015778462693560869\n",
      "Iteration 600/2000, Loss: 0.00016510763089172542\n",
      "Iteration 601/2000, Loss: 0.00013941932411398739\n",
      "Iteration 602/2000, Loss: 0.00017165274766739458\n",
      "Iteration 603/2000, Loss: 0.0002024415007326752\n",
      "Iteration 604/2000, Loss: 0.00031007122015580535\n",
      "Iteration 605/2000, Loss: 0.00015175914450082928\n",
      "Iteration 606/2000, Loss: 0.0002027763839578256\n",
      "Iteration 607/2000, Loss: 0.00015189970145002007\n",
      "Iteration 608/2000, Loss: 0.0002784350363072008\n",
      "Iteration 609/2000, Loss: 0.00019708278705365956\n",
      "Iteration 610/2000, Loss: 0.00017375046445522457\n",
      "Iteration 611/2000, Loss: 0.0001780934981070459\n",
      "Iteration 612/2000, Loss: 0.0001524865219835192\n",
      "Iteration 613/2000, Loss: 0.00020015101472381502\n",
      "Iteration 614/2000, Loss: 0.00022276066010817885\n",
      "Iteration 615/2000, Loss: 0.00021293920872267336\n",
      "Iteration 616/2000, Loss: 0.000195026645087637\n",
      "Iteration 617/2000, Loss: 0.00018922922026831657\n",
      "Iteration 618/2000, Loss: 0.0003060584422200918\n",
      "Iteration 619/2000, Loss: 0.00011856758646899834\n",
      "Iteration 620/2000, Loss: 0.00015994669229257852\n",
      "Iteration 621/2000, Loss: 0.00011577395343920216\n",
      "Iteration 622/2000, Loss: 0.00024666276294738054\n",
      "Iteration 623/2000, Loss: 0.0001702838926576078\n",
      "Iteration 624/2000, Loss: 0.00015409087063744664\n",
      "Iteration 625/2000, Loss: 0.0003490753297228366\n",
      "Iteration 626/2000, Loss: 0.00019506289390847087\n",
      "Iteration 627/2000, Loss: 0.0002827078860718757\n",
      "Iteration 628/2000, Loss: 0.00016793713439255953\n",
      "Iteration 629/2000, Loss: 0.0001368937810184434\n",
      "Iteration 630/2000, Loss: 0.00014023485709913075\n",
      "Iteration 631/2000, Loss: 0.00018283684039488435\n",
      "Iteration 632/2000, Loss: 0.0001947329583344981\n",
      "Iteration 633/2000, Loss: 0.00022372827515937388\n",
      "Iteration 634/2000, Loss: 0.00010095162724610418\n",
      "Iteration 635/2000, Loss: 0.00016176853387150913\n",
      "Iteration 636/2000, Loss: 0.0001884204539237544\n",
      "Iteration 637/2000, Loss: 0.00033958349376916885\n",
      "Iteration 638/2000, Loss: 0.0002886606380343437\n",
      "Iteration 639/2000, Loss: 0.00019682022684719414\n",
      "Iteration 640/2000, Loss: 0.00018947645730804652\n",
      "Iteration 641/2000, Loss: 0.00014895993808750063\n",
      "Iteration 642/2000, Loss: 0.00013213269994594157\n",
      "Iteration 643/2000, Loss: 0.000335138407535851\n",
      "Iteration 644/2000, Loss: 0.00027562593459151685\n",
      "Iteration 645/2000, Loss: 0.00019560809596441686\n",
      "Iteration 646/2000, Loss: 0.00034495408181101084\n",
      "Iteration 647/2000, Loss: 0.00021441318676806986\n",
      "Iteration 648/2000, Loss: 0.0003081965842284262\n",
      "Iteration 649/2000, Loss: 0.00032458119676448405\n",
      "Iteration 650/2000, Loss: 0.000347029825206846\n",
      "Iteration 651/2000, Loss: 0.0001723000459605828\n",
      "Iteration 652/2000, Loss: 0.00027085430338047445\n",
      "Iteration 653/2000, Loss: 0.00021103386825416237\n",
      "Iteration 654/2000, Loss: 0.0004828007658943534\n",
      "Iteration 655/2000, Loss: 0.00018024387827608734\n",
      "Iteration 656/2000, Loss: 0.00020080195099581033\n",
      "Iteration 657/2000, Loss: 0.00042375162593089044\n",
      "Iteration 658/2000, Loss: 0.0002927025780081749\n",
      "Iteration 659/2000, Loss: 0.0003481213061604649\n",
      "Iteration 660/2000, Loss: 0.0004628349270205945\n",
      "Iteration 661/2000, Loss: 0.0002571505610831082\n",
      "Iteration 662/2000, Loss: 0.0003076959401369095\n",
      "Iteration 663/2000, Loss: 0.00019316629914101213\n",
      "Iteration 664/2000, Loss: 0.0003323020355310291\n",
      "Iteration 665/2000, Loss: 0.00023963460989762098\n",
      "Iteration 666/2000, Loss: 0.0002860252861864865\n",
      "Iteration 667/2000, Loss: 0.00024290609871968627\n",
      "Iteration 668/2000, Loss: 0.00018739562074188143\n",
      "Iteration 669/2000, Loss: 0.00045369091094471514\n",
      "Iteration 670/2000, Loss: 0.00013364960614126176\n",
      "Iteration 671/2000, Loss: 0.0002519807603675872\n",
      "Iteration 672/2000, Loss: 0.0001631700579309836\n",
      "Iteration 673/2000, Loss: 0.0001908073463710025\n",
      "Iteration 674/2000, Loss: 0.00027206187951378524\n",
      "Iteration 675/2000, Loss: 0.0001847648381954059\n",
      "Iteration 676/2000, Loss: 0.0002091817877953872\n",
      "Iteration 677/2000, Loss: 0.0002196634595748037\n",
      "Iteration 678/2000, Loss: 0.00020487949950620532\n",
      "Iteration 679/2000, Loss: 0.00017059854872059077\n",
      "Iteration 680/2000, Loss: 0.00030237017199397087\n",
      "Iteration 681/2000, Loss: 0.000226763600949198\n",
      "Iteration 682/2000, Loss: 0.00029614055529236794\n",
      "Iteration 683/2000, Loss: 0.0002925104054156691\n",
      "Iteration 684/2000, Loss: 0.00011845515109598637\n",
      "Iteration 685/2000, Loss: 0.0003699541266541928\n",
      "Iteration 686/2000, Loss: 0.0001624327851459384\n",
      "Iteration 687/2000, Loss: 0.00019216127111576498\n",
      "Iteration 688/2000, Loss: 0.00016499169578310102\n",
      "Iteration 689/2000, Loss: 0.00014875859778840095\n",
      "Iteration 690/2000, Loss: 0.0001689607452135533\n",
      "Iteration 691/2000, Loss: 0.00020496924116741866\n",
      "Iteration 692/2000, Loss: 0.0002057027886621654\n",
      "Iteration 693/2000, Loss: 0.00024032189685385674\n",
      "Iteration 694/2000, Loss: 0.0002476204535923898\n",
      "Iteration 695/2000, Loss: 0.00015817282837815583\n",
      "Iteration 696/2000, Loss: 0.00017955174553208053\n",
      "Iteration 697/2000, Loss: 0.00020086138101760298\n",
      "Iteration 698/2000, Loss: 0.00016141108062583953\n",
      "Iteration 699/2000, Loss: 0.00023858547501731664\n",
      "Iteration 700/2000, Loss: 0.0002158674760721624\n",
      "Iteration 701/2000, Loss: 0.0002768646227195859\n",
      "Iteration 702/2000, Loss: 0.0001436932507203892\n",
      "Iteration 703/2000, Loss: 0.00023676228011026978\n",
      "Iteration 704/2000, Loss: 0.00014032736362423748\n",
      "Iteration 705/2000, Loss: 0.00019291581702418625\n",
      "Iteration 706/2000, Loss: 0.00013844754721503705\n",
      "Iteration 707/2000, Loss: 0.0001987473078770563\n",
      "Iteration 708/2000, Loss: 0.00022166092821862549\n",
      "Iteration 709/2000, Loss: 0.00019291677745059133\n",
      "Iteration 710/2000, Loss: 0.00012034449900966138\n",
      "Iteration 711/2000, Loss: 0.0002791274746414274\n",
      "Iteration 712/2000, Loss: 0.0004198738606646657\n",
      "Iteration 713/2000, Loss: 0.0003141184279229492\n",
      "Iteration 714/2000, Loss: 0.00037631511804647744\n",
      "Iteration 715/2000, Loss: 0.00024912142544053495\n",
      "Iteration 716/2000, Loss: 0.00021472184744197875\n",
      "Iteration 717/2000, Loss: 0.0001523002574685961\n",
      "Iteration 718/2000, Loss: 0.00024671319988556206\n",
      "Iteration 719/2000, Loss: 0.00019579655781853944\n",
      "Iteration 720/2000, Loss: 0.0010454971343278885\n",
      "Iteration 721/2000, Loss: 0.00021474574168678373\n",
      "Iteration 722/2000, Loss: 0.0003088792145717889\n",
      "Iteration 723/2000, Loss: 0.00017886381829157472\n",
      "Iteration 724/2000, Loss: 0.00014662487956229597\n",
      "Iteration 725/2000, Loss: 0.0002335699973627925\n",
      "Iteration 726/2000, Loss: 0.0002421750978101045\n",
      "Iteration 727/2000, Loss: 0.00016914917796384543\n",
      "Iteration 728/2000, Loss: 0.0001945366384461522\n",
      "Iteration 729/2000, Loss: 0.00012603448703885078\n",
      "Iteration 730/2000, Loss: 0.00040953236748464406\n",
      "Iteration 731/2000, Loss: 0.0001574093766976148\n",
      "Iteration 732/2000, Loss: 0.00018525533960200846\n",
      "Iteration 733/2000, Loss: 0.00022152917517814785\n",
      "Iteration 734/2000, Loss: 0.000516375177539885\n",
      "Iteration 735/2000, Loss: 0.00017149472841992974\n",
      "Iteration 736/2000, Loss: 0.00018368862220086157\n",
      "Iteration 737/2000, Loss: 0.00022062621428631246\n",
      "Iteration 738/2000, Loss: 0.0005509382463060319\n",
      "Iteration 739/2000, Loss: 0.00020702133770100772\n",
      "Iteration 740/2000, Loss: 0.00016369082732126117\n",
      "Iteration 741/2000, Loss: 0.0002887629088945687\n",
      "Iteration 742/2000, Loss: 0.00017615040997043252\n",
      "Iteration 743/2000, Loss: 0.0001707444607745856\n",
      "Iteration 744/2000, Loss: 0.00023393187439069152\n",
      "Iteration 745/2000, Loss: 0.00012816539674531668\n",
      "Iteration 746/2000, Loss: 0.00017269070667680353\n",
      "Iteration 747/2000, Loss: 0.00014943322457838804\n",
      "Iteration 748/2000, Loss: 0.00017114095680881292\n",
      "Iteration 749/2000, Loss: 0.0001727832859614864\n",
      "Iteration 750/2000, Loss: 9.885999315883964e-05\n",
      "Iteration 751/2000, Loss: 0.0001229784538736567\n",
      "Iteration 752/2000, Loss: 0.00018554888083599508\n",
      "Iteration 753/2000, Loss: 0.0001427253009751439\n",
      "Iteration 754/2000, Loss: 0.0001720013387966901\n",
      "Iteration 755/2000, Loss: 0.00021903833840042353\n",
      "Iteration 756/2000, Loss: 0.00014615865075029433\n",
      "Iteration 757/2000, Loss: 0.00018470037321094424\n",
      "Iteration 758/2000, Loss: 0.0001140521198976785\n",
      "Iteration 759/2000, Loss: 0.0003066844947170466\n",
      "Iteration 760/2000, Loss: 0.000326743844198063\n",
      "Iteration 761/2000, Loss: 0.00023184805468190461\n",
      "Iteration 762/2000, Loss: 0.00018088759679812938\n",
      "Iteration 763/2000, Loss: 0.00011257440928602591\n",
      "Iteration 764/2000, Loss: 0.00016251360648311675\n",
      "Iteration 765/2000, Loss: 0.00012047262134728953\n",
      "Iteration 766/2000, Loss: 0.00023527842131443322\n",
      "Iteration 767/2000, Loss: 0.0002562376612331718\n",
      "Iteration 768/2000, Loss: 0.0003361371054779738\n",
      "Iteration 769/2000, Loss: 0.0002912812342401594\n",
      "Iteration 770/2000, Loss: 0.0002801902301143855\n",
      "Iteration 771/2000, Loss: 0.00032282579923048615\n",
      "Iteration 772/2000, Loss: 0.0003051159728784114\n",
      "Iteration 773/2000, Loss: 0.0006758926319889724\n",
      "Iteration 774/2000, Loss: 0.0002893803466577083\n",
      "Iteration 775/2000, Loss: 0.00028308830223977566\n",
      "Iteration 776/2000, Loss: 0.00024982597096823156\n",
      "Iteration 777/2000, Loss: 0.00036885723238810897\n",
      "Iteration 778/2000, Loss: 0.0001759632577886805\n",
      "Iteration 779/2000, Loss: 0.00037425316986627877\n",
      "Iteration 780/2000, Loss: 0.00028942537028342485\n",
      "Iteration 781/2000, Loss: 0.0002450150204822421\n",
      "Iteration 782/2000, Loss: 0.0003092461556661874\n",
      "Iteration 783/2000, Loss: 0.00017611424846109003\n",
      "Iteration 784/2000, Loss: 0.00029297376750037074\n",
      "Iteration 785/2000, Loss: 0.00029880102374590933\n",
      "Iteration 786/2000, Loss: 0.00021805832511745393\n",
      "Iteration 787/2000, Loss: 0.00021054645185358822\n",
      "Iteration 788/2000, Loss: 0.000244362949160859\n",
      "Iteration 789/2000, Loss: 0.0003754493372980505\n",
      "Iteration 790/2000, Loss: 0.00013190355093684047\n",
      "Iteration 791/2000, Loss: 0.00016156522906385362\n",
      "Iteration 792/2000, Loss: 0.00023262662580236793\n",
      "Iteration 793/2000, Loss: 0.00020191678777337074\n",
      "Iteration 794/2000, Loss: 0.00021289987489581108\n",
      "Iteration 795/2000, Loss: 0.0002391626767348498\n",
      "Iteration 796/2000, Loss: 0.00026462660753168166\n",
      "Iteration 797/2000, Loss: 0.0004533653846010566\n",
      "Iteration 798/2000, Loss: 0.00014507450396195054\n",
      "Iteration 799/2000, Loss: 0.00013419156312011182\n",
      "Iteration 800/2000, Loss: 0.0001728667557472363\n",
      "Iteration 801/2000, Loss: 0.0002113449154421687\n",
      "Iteration 802/2000, Loss: 0.00021119878510944545\n",
      "Iteration 803/2000, Loss: 0.00017916178330779076\n",
      "Iteration 804/2000, Loss: 0.0001335854030912742\n",
      "Iteration 805/2000, Loss: 0.00040202942909672856\n",
      "Iteration 806/2000, Loss: 0.00031804729951545596\n",
      "Iteration 807/2000, Loss: 0.00015839222760405391\n",
      "Iteration 808/2000, Loss: 0.0005491618067026138\n",
      "Iteration 809/2000, Loss: 0.00011526043090270832\n",
      "Iteration 810/2000, Loss: 0.00021808485325891525\n",
      "Iteration 811/2000, Loss: 0.0003656358167063445\n",
      "Iteration 812/2000, Loss: 0.0003182413929607719\n",
      "Iteration 813/2000, Loss: 0.0001578724040882662\n",
      "Iteration 814/2000, Loss: 0.00021307177667040378\n",
      "Iteration 815/2000, Loss: 0.00014804481179453433\n",
      "Iteration 816/2000, Loss: 0.00011159804853377864\n",
      "Iteration 817/2000, Loss: 0.0003062832693103701\n",
      "Iteration 818/2000, Loss: 0.00018190586706623435\n",
      "Iteration 819/2000, Loss: 0.00022339697170536965\n",
      "Iteration 820/2000, Loss: 0.00018034863751381636\n",
      "Iteration 821/2000, Loss: 0.0002053591306321323\n",
      "Iteration 822/2000, Loss: 0.00018043542513623834\n",
      "Iteration 823/2000, Loss: 9.98592731775716e-05\n",
      "Iteration 824/2000, Loss: 0.0001276131661143154\n",
      "Iteration 825/2000, Loss: 0.0001800455356715247\n",
      "Iteration 826/2000, Loss: 0.0001810126268537715\n",
      "Iteration 827/2000, Loss: 0.00019965317915193737\n",
      "Iteration 828/2000, Loss: 0.0004912685253657401\n",
      "Iteration 829/2000, Loss: 9.0723333414644e-05\n",
      "Iteration 830/2000, Loss: 0.0002993321977555752\n",
      "Iteration 831/2000, Loss: 0.0002658800221979618\n",
      "Iteration 832/2000, Loss: 0.00014175295655149966\n",
      "Iteration 833/2000, Loss: 0.00016122231318149716\n",
      "Iteration 834/2000, Loss: 0.00011688393715303391\n",
      "Iteration 835/2000, Loss: 0.00022121687652543187\n",
      "Iteration 836/2000, Loss: 0.00013990710431244224\n",
      "Iteration 837/2000, Loss: 0.00013689503248315305\n",
      "Iteration 838/2000, Loss: 0.00014410402218345553\n",
      "Iteration 839/2000, Loss: 0.00011490526230772957\n",
      "Iteration 840/2000, Loss: 0.00044707339839078486\n",
      "Iteration 841/2000, Loss: 0.00013417615264188498\n",
      "Iteration 842/2000, Loss: 0.00028094081790186465\n",
      "Iteration 843/2000, Loss: 0.00015265840920619667\n",
      "Iteration 844/2000, Loss: 0.00019335250544827431\n",
      "Iteration 845/2000, Loss: 0.00019293100922368467\n",
      "Iteration 846/2000, Loss: 0.00013307886547408998\n",
      "Iteration 847/2000, Loss: 0.0002547406475059688\n",
      "Iteration 848/2000, Loss: 0.00020698919252026826\n",
      "Iteration 849/2000, Loss: 0.00024282939557451755\n",
      "Iteration 850/2000, Loss: 0.000394313974538818\n",
      "Iteration 851/2000, Loss: 0.00020780165505129844\n",
      "Iteration 852/2000, Loss: 0.00017589214257895947\n",
      "Iteration 853/2000, Loss: 0.00026477963547222316\n",
      "Iteration 854/2000, Loss: 0.0003245568659622222\n",
      "Iteration 855/2000, Loss: 0.0002778741472866386\n",
      "Iteration 856/2000, Loss: 0.00015482373419217765\n",
      "Iteration 857/2000, Loss: 0.00020978030806872994\n",
      "Iteration 858/2000, Loss: 0.000167219594004564\n",
      "Iteration 859/2000, Loss: 0.00022160916705615819\n",
      "Iteration 860/2000, Loss: 0.0005615067202597857\n",
      "Iteration 861/2000, Loss: 0.00021359973470680416\n",
      "Iteration 862/2000, Loss: 0.0003878832212649286\n",
      "Iteration 863/2000, Loss: 0.000209502293728292\n",
      "Iteration 864/2000, Loss: 0.00023172602232079953\n",
      "Iteration 865/2000, Loss: 0.0003412469814065844\n",
      "Iteration 866/2000, Loss: 0.00022226627334021032\n",
      "Iteration 867/2000, Loss: 0.00037675301427952945\n",
      "Iteration 868/2000, Loss: 0.00040156659088097513\n",
      "Iteration 869/2000, Loss: 0.00021197853493504226\n",
      "Iteration 870/2000, Loss: 0.0002781641960609704\n",
      "Iteration 871/2000, Loss: 0.00032582134008407593\n",
      "Iteration 872/2000, Loss: 0.0003135248553007841\n",
      "Iteration 873/2000, Loss: 0.00026191206416115165\n",
      "Iteration 874/2000, Loss: 0.0001510723086539656\n",
      "Iteration 875/2000, Loss: 0.0001595680514583364\n",
      "Iteration 876/2000, Loss: 0.000141887430800125\n",
      "Iteration 877/2000, Loss: 0.0003246419073548168\n",
      "Iteration 878/2000, Loss: 0.00026445346884429455\n",
      "Iteration 879/2000, Loss: 0.00019901699852198362\n",
      "Iteration 880/2000, Loss: 0.00022102388902567327\n",
      "Iteration 881/2000, Loss: 0.00013374850095715374\n",
      "Iteration 882/2000, Loss: 0.00037485602661035955\n",
      "Iteration 883/2000, Loss: 0.00015804161375854164\n",
      "Iteration 884/2000, Loss: 0.00023456497001461685\n",
      "Iteration 885/2000, Loss: 0.0002140628785127774\n",
      "Iteration 886/2000, Loss: 0.00018937130516860634\n",
      "Iteration 887/2000, Loss: 0.0003118803433608264\n",
      "Iteration 888/2000, Loss: 0.0001198393729282543\n",
      "Iteration 889/2000, Loss: 0.00039812069735489786\n",
      "Iteration 890/2000, Loss: 0.000239906192291528\n",
      "Iteration 891/2000, Loss: 0.0002149296778952703\n",
      "Iteration 892/2000, Loss: 0.0002543038863223046\n",
      "Iteration 893/2000, Loss: 0.00025587406707927585\n",
      "Iteration 894/2000, Loss: 0.000556002021767199\n",
      "Iteration 895/2000, Loss: 0.0002246452058898285\n",
      "Iteration 896/2000, Loss: 0.0002205436903750524\n",
      "Iteration 897/2000, Loss: 0.0001926509867189452\n",
      "Iteration 898/2000, Loss: 0.0001803273771656677\n",
      "Iteration 899/2000, Loss: 0.00017014886543620378\n",
      "Iteration 900/2000, Loss: 0.00024243808002211154\n",
      "Iteration 901/2000, Loss: 0.00021303589164745063\n",
      "Iteration 902/2000, Loss: 0.000206295182579197\n",
      "Iteration 903/2000, Loss: 0.00016011872503440827\n",
      "Iteration 904/2000, Loss: 0.00013237976236268878\n",
      "Iteration 905/2000, Loss: 0.00038942694664001465\n",
      "Iteration 906/2000, Loss: 0.00019675512157846242\n",
      "Iteration 907/2000, Loss: 0.00042506359750404954\n",
      "Iteration 908/2000, Loss: 0.00015122411423362792\n",
      "Iteration 909/2000, Loss: 0.00022045173682272434\n",
      "Iteration 910/2000, Loss: 0.00012125849752919748\n",
      "Iteration 911/2000, Loss: 0.0003797953249886632\n",
      "Iteration 912/2000, Loss: 0.00016696666716597974\n",
      "Iteration 913/2000, Loss: 0.00015148909005802125\n",
      "Iteration 914/2000, Loss: 0.00017022706742864102\n",
      "Iteration 915/2000, Loss: 0.0001903256488731131\n",
      "Iteration 916/2000, Loss: 0.00011492337216623127\n",
      "Iteration 917/2000, Loss: 0.00022322357108350843\n",
      "Iteration 918/2000, Loss: 0.00029930612072348595\n",
      "Iteration 919/2000, Loss: 0.00021202924835961312\n",
      "Iteration 920/2000, Loss: 0.00025929961702786386\n",
      "Iteration 921/2000, Loss: 0.00019336541299708188\n",
      "Iteration 922/2000, Loss: 0.00018327857833355665\n",
      "Iteration 923/2000, Loss: 0.0003471823292784393\n",
      "Iteration 924/2000, Loss: 0.00012134323333157226\n",
      "Iteration 925/2000, Loss: 0.00023917219368740916\n",
      "Iteration 926/2000, Loss: 0.00017600442515686154\n",
      "Iteration 927/2000, Loss: 0.00014680087042506784\n",
      "Iteration 928/2000, Loss: 0.00027180329198017716\n",
      "Iteration 929/2000, Loss: 0.0002350115100853145\n",
      "Iteration 930/2000, Loss: 0.0005045686848461628\n",
      "Iteration 931/2000, Loss: 0.0002517651009839028\n",
      "Iteration 932/2000, Loss: 0.0001954276958713308\n",
      "Iteration 933/2000, Loss: 0.0002620693703647703\n",
      "Iteration 934/2000, Loss: 0.0002799079811666161\n",
      "Iteration 935/2000, Loss: 0.00024765534908510745\n",
      "Iteration 936/2000, Loss: 0.0006598550826311111\n",
      "Iteration 937/2000, Loss: 0.00017916463548317552\n",
      "Iteration 938/2000, Loss: 0.00032619546982459724\n",
      "Iteration 939/2000, Loss: 0.0002676063450053334\n",
      "Iteration 940/2000, Loss: 0.00032599977566860616\n",
      "Iteration 941/2000, Loss: 0.0001580344105605036\n",
      "Iteration 942/2000, Loss: 0.00025093069416470826\n",
      "Iteration 943/2000, Loss: 0.00018222791550215334\n",
      "Iteration 944/2000, Loss: 0.00023743128986097872\n",
      "Iteration 945/2000, Loss: 0.00030619659810326993\n",
      "Iteration 946/2000, Loss: 0.00019581244850996882\n",
      "Iteration 947/2000, Loss: 0.0003981415356975049\n",
      "Iteration 948/2000, Loss: 0.0002264315844513476\n",
      "Iteration 949/2000, Loss: 0.0002718917094171047\n",
      "Iteration 950/2000, Loss: 0.0002398448414169252\n",
      "Iteration 951/2000, Loss: 0.0002437965595163405\n",
      "Iteration 952/2000, Loss: 0.00018836990057025105\n",
      "Iteration 953/2000, Loss: 0.00014301562623586506\n",
      "Iteration 954/2000, Loss: 0.0001894814195111394\n",
      "Iteration 955/2000, Loss: 0.00030450194026343524\n",
      "Iteration 956/2000, Loss: 0.00016455986769869924\n",
      "Iteration 957/2000, Loss: 0.00021117720461916178\n",
      "Iteration 958/2000, Loss: 0.00018442505097482353\n",
      "Iteration 959/2000, Loss: 0.0001908103295136243\n",
      "Iteration 960/2000, Loss: 0.00018175797595176846\n",
      "Iteration 961/2000, Loss: 0.00016531480650883168\n",
      "Iteration 962/2000, Loss: 0.0002908955211751163\n",
      "Iteration 963/2000, Loss: 0.0003600412455853075\n",
      "Iteration 964/2000, Loss: 0.00027668598340824246\n",
      "Iteration 965/2000, Loss: 0.00017784087685868144\n",
      "Iteration 966/2000, Loss: 0.00030872473143972456\n",
      "Iteration 967/2000, Loss: 0.00018040092254523188\n",
      "Iteration 968/2000, Loss: 0.0003636144974734634\n",
      "Iteration 969/2000, Loss: 0.0006238456116989255\n",
      "Iteration 970/2000, Loss: 0.00018936306878458709\n",
      "Iteration 971/2000, Loss: 0.0003142475616186857\n",
      "Iteration 972/2000, Loss: 0.00024215497251134366\n",
      "Iteration 973/2000, Loss: 0.00027159290038980544\n",
      "Iteration 974/2000, Loss: 0.00010542375821387395\n",
      "Iteration 975/2000, Loss: 0.0007312981761060655\n",
      "Iteration 976/2000, Loss: 0.00030837676604278386\n",
      "Iteration 977/2000, Loss: 0.00018956967687699944\n",
      "Iteration 978/2000, Loss: 0.00026379124028608203\n",
      "Iteration 979/2000, Loss: 0.00023449878790415823\n",
      "Iteration 980/2000, Loss: 0.00034435640554875135\n",
      "Iteration 981/2000, Loss: 0.00018849840853363276\n",
      "Iteration 982/2000, Loss: 0.00024652830325067043\n",
      "Iteration 983/2000, Loss: 0.00016348171629942954\n",
      "Iteration 984/2000, Loss: 0.00024167279480025172\n",
      "Iteration 985/2000, Loss: 0.00025811983505263925\n",
      "Iteration 986/2000, Loss: 0.00012912524107377976\n",
      "Iteration 987/2000, Loss: 0.00020623591262847185\n",
      "Iteration 988/2000, Loss: 0.00023908285947982222\n",
      "Iteration 989/2000, Loss: 0.0003930014499928802\n",
      "Iteration 990/2000, Loss: 0.0001947011478478089\n",
      "Iteration 991/2000, Loss: 0.0003426375042181462\n",
      "Iteration 992/2000, Loss: 0.00026159625849686563\n",
      "Iteration 993/2000, Loss: 0.0002556101535446942\n",
      "Iteration 994/2000, Loss: 0.00017753081920091063\n",
      "Iteration 995/2000, Loss: 0.0003654519096016884\n",
      "Iteration 996/2000, Loss: 0.00018857397662941366\n",
      "Iteration 997/2000, Loss: 0.0001660484995227307\n",
      "Iteration 998/2000, Loss: 0.00036657811142504215\n",
      "Iteration 999/2000, Loss: 0.0003765957080759108\n",
      "Iteration 1000/2000, Loss: 0.00017299556930083781\n",
      "Iteration 1001/2000, Loss: 0.000558462634216994\n",
      "Iteration 1002/2000, Loss: 0.0002329207636648789\n",
      "Iteration 1003/2000, Loss: 0.0006008120253682137\n",
      "Iteration 1004/2000, Loss: 0.00027801026590168476\n",
      "Iteration 1005/2000, Loss: 0.00027384638087823987\n",
      "Iteration 1006/2000, Loss: 0.0002173406828660518\n",
      "Iteration 1007/2000, Loss: 0.00017015870253089815\n",
      "Iteration 1008/2000, Loss: 0.0001885662932181731\n",
      "Iteration 1009/2000, Loss: 0.0002348298585275188\n",
      "Iteration 1010/2000, Loss: 0.00024739521904848516\n",
      "Iteration 1011/2000, Loss: 0.00012332896585576236\n",
      "Iteration 1012/2000, Loss: 0.00026568290195427835\n",
      "Iteration 1013/2000, Loss: 0.0001851203851401806\n",
      "Iteration 1014/2000, Loss: 0.0002119802520610392\n",
      "Iteration 1015/2000, Loss: 0.00018018910486716777\n",
      "Iteration 1016/2000, Loss: 0.00041695241816341877\n",
      "Iteration 1017/2000, Loss: 0.00015938100113999099\n",
      "Iteration 1018/2000, Loss: 0.0001419440086465329\n",
      "Iteration 1019/2000, Loss: 0.00014195275434758514\n",
      "Iteration 1020/2000, Loss: 0.00012321760004851967\n",
      "Iteration 1021/2000, Loss: 0.00011174198152730241\n",
      "Iteration 1022/2000, Loss: 0.00012116260768380016\n",
      "Iteration 1023/2000, Loss: 9.856537508312613e-05\n",
      "Iteration 1024/2000, Loss: 0.00019147267448715866\n",
      "Iteration 1025/2000, Loss: 0.00014395892503671348\n",
      "Iteration 1026/2000, Loss: 0.0004368517838884145\n",
      "Iteration 1027/2000, Loss: 0.00013216638762969524\n",
      "Iteration 1028/2000, Loss: 0.00015261380758602172\n",
      "Iteration 1029/2000, Loss: 0.00020029873121529818\n",
      "Iteration 1030/2000, Loss: 0.0002678109158296138\n",
      "Iteration 1031/2000, Loss: 0.00023370675626210868\n",
      "Iteration 1032/2000, Loss: 0.00021358193771447986\n",
      "Iteration 1033/2000, Loss: 0.00013307633344084024\n",
      "Iteration 1034/2000, Loss: 0.00018831429770216346\n",
      "Iteration 1035/2000, Loss: 0.00022081725182943046\n",
      "Iteration 1036/2000, Loss: 0.00018095636914949864\n",
      "Iteration 1037/2000, Loss: 0.00010863377974601462\n",
      "Iteration 1038/2000, Loss: 0.0002371399459661916\n",
      "Iteration 1039/2000, Loss: 0.0001478449848946184\n",
      "Iteration 1040/2000, Loss: 0.0002463136042933911\n",
      "Iteration 1041/2000, Loss: 0.0003559752949513495\n",
      "Iteration 1042/2000, Loss: 0.00022889453975949436\n",
      "Iteration 1043/2000, Loss: 0.0001316187554039061\n",
      "Iteration 1044/2000, Loss: 0.00028905420913361013\n",
      "Iteration 1045/2000, Loss: 0.0001794453419279307\n",
      "Iteration 1046/2000, Loss: 0.0001980322558665648\n",
      "Iteration 1047/2000, Loss: 0.00016804723418317735\n",
      "Iteration 1048/2000, Loss: 0.0002052244235528633\n",
      "Iteration 1049/2000, Loss: 0.00011838805949082598\n",
      "Iteration 1050/2000, Loss: 0.0002740660565905273\n",
      "Iteration 1051/2000, Loss: 0.00018316063506063074\n",
      "Iteration 1052/2000, Loss: 0.00021764849952887744\n",
      "Iteration 1053/2000, Loss: 0.00015543225163128227\n",
      "Iteration 1054/2000, Loss: 0.0005880696699023247\n",
      "Iteration 1055/2000, Loss: 0.00016171648167073727\n",
      "Iteration 1056/2000, Loss: 0.0002834390616044402\n",
      "Iteration 1057/2000, Loss: 0.00016658789536450058\n",
      "Iteration 1058/2000, Loss: 0.00013108394341543317\n",
      "Iteration 1059/2000, Loss: 0.000566491624340415\n",
      "Iteration 1060/2000, Loss: 0.0003145277441944927\n",
      "Iteration 1061/2000, Loss: 0.00026427058037370443\n",
      "Iteration 1062/2000, Loss: 0.00012811417400371283\n",
      "Iteration 1063/2000, Loss: 0.00026234559481963515\n",
      "Iteration 1064/2000, Loss: 0.0001888533151941374\n",
      "Iteration 1065/2000, Loss: 0.000248254305915907\n",
      "Iteration 1066/2000, Loss: 0.00021158823801670223\n",
      "Iteration 1067/2000, Loss: 0.000159737843205221\n",
      "Iteration 1068/2000, Loss: 0.0002271267876494676\n",
      "Iteration 1069/2000, Loss: 0.0003577653842512518\n",
      "Iteration 1070/2000, Loss: 0.0001584494166309014\n",
      "Iteration 1071/2000, Loss: 0.000244307128014043\n",
      "Iteration 1072/2000, Loss: 0.00020244769984856248\n",
      "Iteration 1073/2000, Loss: 0.00033142376923933625\n",
      "Iteration 1074/2000, Loss: 0.00019301229622215033\n",
      "Iteration 1075/2000, Loss: 0.00014151414507068694\n",
      "Iteration 1076/2000, Loss: 0.00021823987481184304\n",
      "Iteration 1077/2000, Loss: 0.0001472623407607898\n",
      "Iteration 1078/2000, Loss: 0.00018428445036988705\n",
      "Iteration 1079/2000, Loss: 0.00010737904085544869\n",
      "Iteration 1080/2000, Loss: 0.00019974327005911618\n",
      "Iteration 1081/2000, Loss: 0.00013107135600876063\n",
      "Iteration 1082/2000, Loss: 0.00017383196973241866\n",
      "Iteration 1083/2000, Loss: 0.00017009582370519638\n",
      "Iteration 1084/2000, Loss: 0.0002668694360181689\n",
      "Iteration 1085/2000, Loss: 0.00014327680401038378\n",
      "Iteration 1086/2000, Loss: 0.000203792005777359\n",
      "Iteration 1087/2000, Loss: 0.0001736690173856914\n",
      "Iteration 1088/2000, Loss: 0.000262156332610175\n",
      "Iteration 1089/2000, Loss: 0.0004726105835288763\n",
      "Iteration 1090/2000, Loss: 0.00022883934434503317\n",
      "Iteration 1091/2000, Loss: 0.00019378594879526645\n",
      "Iteration 1092/2000, Loss: 0.0002826885611284524\n",
      "Iteration 1093/2000, Loss: 0.00015364016871899366\n",
      "Iteration 1094/2000, Loss: 0.00026930784224532545\n",
      "Iteration 1095/2000, Loss: 0.00026385640376247466\n",
      "Iteration 1096/2000, Loss: 0.000238466338487342\n",
      "Iteration 1097/2000, Loss: 0.0004094405157957226\n",
      "Iteration 1098/2000, Loss: 0.00019857581355609\n",
      "Iteration 1099/2000, Loss: 0.00020384631352499127\n",
      "Iteration 1100/2000, Loss: 0.00027230108389630914\n",
      "Iteration 1101/2000, Loss: 0.0002954092051368207\n",
      "Iteration 1102/2000, Loss: 0.0003410283534321934\n",
      "Iteration 1103/2000, Loss: 0.00035494333133101463\n",
      "Iteration 1104/2000, Loss: 0.00029718028963543475\n",
      "Iteration 1105/2000, Loss: 0.0001543565304018557\n",
      "Iteration 1106/2000, Loss: 0.00029022825765423477\n",
      "Iteration 1107/2000, Loss: 0.00010261681018164381\n",
      "Iteration 1108/2000, Loss: 0.00024660007329657674\n",
      "Iteration 1109/2000, Loss: 0.00031991442665457726\n",
      "Iteration 1110/2000, Loss: 0.00018894713139161468\n",
      "Iteration 1111/2000, Loss: 0.00023960351245477796\n",
      "Iteration 1112/2000, Loss: 0.00029687033384107053\n",
      "Iteration 1113/2000, Loss: 0.00024840282276272774\n",
      "Iteration 1114/2000, Loss: 0.00014127121539786458\n",
      "Iteration 1115/2000, Loss: 0.00030168105149641633\n",
      "Iteration 1116/2000, Loss: 0.0002228556404588744\n",
      "Iteration 1117/2000, Loss: 0.00022184870613273233\n",
      "Iteration 1118/2000, Loss: 0.00023964786669239402\n",
      "Iteration 1119/2000, Loss: 0.00021224665397312492\n",
      "Iteration 1120/2000, Loss: 0.00041182851418852806\n",
      "Iteration 1121/2000, Loss: 0.0003101915353909135\n",
      "Iteration 1122/2000, Loss: 0.00021256324544083327\n",
      "Iteration 1123/2000, Loss: 0.0004508351266849786\n",
      "Iteration 1124/2000, Loss: 0.00018816943338606507\n",
      "Iteration 1125/2000, Loss: 0.0002544602320995182\n",
      "Iteration 1126/2000, Loss: 0.00025203099357895553\n",
      "Iteration 1127/2000, Loss: 0.0002520000853110105\n",
      "Iteration 1128/2000, Loss: 0.00017283220950048417\n",
      "Iteration 1129/2000, Loss: 0.0001231539063155651\n",
      "Iteration 1130/2000, Loss: 0.00011860969971166924\n",
      "Iteration 1131/2000, Loss: 0.000537527201231569\n",
      "Iteration 1132/2000, Loss: 0.00015719770453870296\n",
      "Iteration 1133/2000, Loss: 0.00046181335346773267\n",
      "Iteration 1134/2000, Loss: 0.00019061074999626726\n",
      "Iteration 1135/2000, Loss: 0.0001329069200437516\n",
      "Iteration 1136/2000, Loss: 0.0003087465011049062\n",
      "Iteration 1137/2000, Loss: 0.00016292226791847497\n",
      "Iteration 1138/2000, Loss: 0.00023545041040051728\n",
      "Iteration 1139/2000, Loss: 0.0001702146983006969\n",
      "Iteration 1140/2000, Loss: 0.00019980018259957433\n",
      "Iteration 1141/2000, Loss: 0.00016028773097787052\n",
      "Iteration 1142/2000, Loss: 0.00017543617286719382\n",
      "Iteration 1143/2000, Loss: 0.00025374392862431705\n",
      "Iteration 1144/2000, Loss: 0.0002710242406465113\n",
      "Iteration 1145/2000, Loss: 0.00016754605167079717\n",
      "Iteration 1146/2000, Loss: 0.00042283249786123633\n",
      "Iteration 1147/2000, Loss: 0.0003234206815250218\n",
      "Iteration 1148/2000, Loss: 0.0001851198321674019\n",
      "Iteration 1149/2000, Loss: 0.00017770218255463988\n",
      "Iteration 1150/2000, Loss: 0.0001313865213887766\n",
      "Iteration 1151/2000, Loss: 0.0003339667746331543\n",
      "Iteration 1152/2000, Loss: 0.0002367599227000028\n",
      "Iteration 1153/2000, Loss: 0.00020423905516508967\n",
      "Iteration 1154/2000, Loss: 0.0004620742693077773\n",
      "Iteration 1155/2000, Loss: 0.0005077673704363406\n",
      "Iteration 1156/2000, Loss: 0.0002540317946113646\n",
      "Iteration 1157/2000, Loss: 0.00032054647454060614\n",
      "Iteration 1158/2000, Loss: 0.00039667400415055454\n",
      "Iteration 1159/2000, Loss: 0.0002278398023918271\n",
      "Iteration 1160/2000, Loss: 0.0006535597494803369\n",
      "Iteration 1161/2000, Loss: 0.0002526885364204645\n",
      "Iteration 1162/2000, Loss: 0.0002445621066726744\n",
      "Iteration 1163/2000, Loss: 0.00041414794395677745\n",
      "Iteration 1164/2000, Loss: 0.0004423512436915189\n",
      "Iteration 1165/2000, Loss: 0.00031790236243978143\n",
      "Iteration 1166/2000, Loss: 0.0005729770637117326\n",
      "Iteration 1167/2000, Loss: 0.00018509224173612893\n",
      "Iteration 1168/2000, Loss: 0.0003581266209948808\n",
      "Iteration 1169/2000, Loss: 0.00033001884003169835\n",
      "Iteration 1170/2000, Loss: 0.0004928902490064502\n",
      "Iteration 1171/2000, Loss: 0.00025539760827086866\n",
      "Iteration 1172/2000, Loss: 0.00034855984267778695\n",
      "Iteration 1173/2000, Loss: 0.0002097332471748814\n",
      "Iteration 1174/2000, Loss: 0.00019175384659320116\n",
      "Iteration 1175/2000, Loss: 0.00022595550399273634\n",
      "Iteration 1176/2000, Loss: 0.00022043325589038432\n",
      "Iteration 1177/2000, Loss: 0.0003217001212760806\n",
      "Iteration 1178/2000, Loss: 0.00013125305122230202\n",
      "Iteration 1179/2000, Loss: 0.00016303923621308059\n",
      "Iteration 1180/2000, Loss: 0.0001373186387354508\n",
      "Iteration 1181/2000, Loss: 0.00027302070520818233\n",
      "Iteration 1182/2000, Loss: 0.0003196391917299479\n",
      "Iteration 1183/2000, Loss: 0.00016591677558608353\n",
      "Iteration 1184/2000, Loss: 0.0001599324168637395\n",
      "Iteration 1185/2000, Loss: 0.00014471466420218349\n",
      "Iteration 1186/2000, Loss: 0.0002094027731800452\n",
      "Iteration 1187/2000, Loss: 0.00016910710837692022\n",
      "Iteration 1188/2000, Loss: 0.00019104366947431117\n",
      "Iteration 1189/2000, Loss: 0.00012057596177328378\n",
      "Iteration 1190/2000, Loss: 0.00035494662006385624\n",
      "Iteration 1191/2000, Loss: 0.00016114828758873045\n",
      "Iteration 1192/2000, Loss: 0.0001553688052808866\n",
      "Iteration 1193/2000, Loss: 0.00022898736642673612\n",
      "Iteration 1194/2000, Loss: 0.0001735638070385903\n",
      "Iteration 1195/2000, Loss: 0.00030492685618810356\n",
      "Iteration 1196/2000, Loss: 0.00020461836538743228\n",
      "Iteration 1197/2000, Loss: 0.0003468080540187657\n",
      "Iteration 1198/2000, Loss: 0.00019141563097946346\n",
      "Iteration 1199/2000, Loss: 0.00024225586093962193\n",
      "Iteration 1200/2000, Loss: 0.00023321539629250765\n",
      "Iteration 1201/2000, Loss: 0.00018837075913324952\n",
      "Iteration 1202/2000, Loss: 0.00029783978243358433\n",
      "Iteration 1203/2000, Loss: 0.0003027671773452312\n",
      "Iteration 1204/2000, Loss: 0.00019250063633080572\n",
      "Iteration 1205/2000, Loss: 0.00021809003374073654\n",
      "Iteration 1206/2000, Loss: 0.0002155348047381267\n",
      "Iteration 1207/2000, Loss: 0.0001465927780373022\n",
      "Iteration 1208/2000, Loss: 0.00029521621763706207\n",
      "Iteration 1209/2000, Loss: 0.00018669872952159494\n",
      "Iteration 1210/2000, Loss: 0.00021626257512252778\n",
      "Iteration 1211/2000, Loss: 0.00028368973289616406\n",
      "Iteration 1212/2000, Loss: 0.00017295280122198164\n",
      "Iteration 1213/2000, Loss: 0.0001293654495384544\n",
      "Iteration 1214/2000, Loss: 0.0001573985646246001\n",
      "Iteration 1215/2000, Loss: 0.00011878182704094797\n",
      "Iteration 1216/2000, Loss: 0.0001778455771273002\n",
      "Iteration 1217/2000, Loss: 0.00017144154116977006\n",
      "Iteration 1218/2000, Loss: 0.0001349699159618467\n",
      "Iteration 1219/2000, Loss: 0.00019768114725593477\n",
      "Iteration 1220/2000, Loss: 0.00015234261809382588\n",
      "Iteration 1221/2000, Loss: 0.0001236091775353998\n",
      "Iteration 1222/2000, Loss: 0.0003160431224387139\n",
      "Iteration 1223/2000, Loss: 0.00017907476285472512\n",
      "Iteration 1224/2000, Loss: 0.0003832606307696551\n",
      "Iteration 1225/2000, Loss: 0.0003213938616681844\n",
      "Iteration 1226/2000, Loss: 0.0003351312770973891\n",
      "Iteration 1227/2000, Loss: 0.000131692475406453\n",
      "Iteration 1228/2000, Loss: 0.00024640263291075826\n",
      "Iteration 1229/2000, Loss: 0.0003938238660339266\n",
      "Iteration 1230/2000, Loss: 0.00025259488029405475\n",
      "Iteration 1231/2000, Loss: 0.0002462038828525692\n",
      "Iteration 1232/2000, Loss: 0.00026235266705043614\n",
      "Iteration 1233/2000, Loss: 0.0003066847857553512\n",
      "Iteration 1234/2000, Loss: 0.00022957053442951292\n",
      "Iteration 1235/2000, Loss: 0.00016056142339948565\n",
      "Iteration 1236/2000, Loss: 0.0003470768569968641\n",
      "Iteration 1237/2000, Loss: 0.00022713388898409903\n",
      "Iteration 1238/2000, Loss: 0.0003044472250621766\n",
      "Iteration 1239/2000, Loss: 0.00012876253458671272\n",
      "Iteration 1240/2000, Loss: 0.00020539479737635702\n",
      "Iteration 1241/2000, Loss: 0.00016926437092479318\n",
      "Iteration 1242/2000, Loss: 0.00014590256614610553\n",
      "Iteration 1243/2000, Loss: 0.00020996487000957131\n",
      "Iteration 1244/2000, Loss: 0.00012611487181857228\n",
      "Iteration 1245/2000, Loss: 0.00022858829470351338\n",
      "Iteration 1246/2000, Loss: 0.00016401722677983344\n",
      "Iteration 1247/2000, Loss: 0.0002068284957204014\n",
      "Iteration 1248/2000, Loss: 0.00023780856281518936\n",
      "Iteration 1249/2000, Loss: 0.00017583071894478053\n",
      "Iteration 1250/2000, Loss: 0.0001756909623509273\n",
      "Iteration 1251/2000, Loss: 0.00029513228219002485\n",
      "Iteration 1252/2000, Loss: 0.00017854571342468262\n",
      "Iteration 1253/2000, Loss: 0.0001235286908922717\n",
      "Iteration 1254/2000, Loss: 0.00015701640222687274\n",
      "Iteration 1255/2000, Loss: 0.0002479165268596262\n",
      "Iteration 1256/2000, Loss: 0.0005087308236397803\n",
      "Iteration 1257/2000, Loss: 0.00012945110211148858\n",
      "Iteration 1258/2000, Loss: 0.00018456450197845697\n",
      "Iteration 1259/2000, Loss: 0.00020417346968315542\n",
      "Iteration 1260/2000, Loss: 0.00022697490931022912\n",
      "Iteration 1261/2000, Loss: 0.00032166417804546654\n",
      "Iteration 1262/2000, Loss: 0.00026326990337111056\n",
      "Iteration 1263/2000, Loss: 0.00021399810793809593\n",
      "Iteration 1264/2000, Loss: 0.00017071007459890097\n",
      "Iteration 1265/2000, Loss: 0.0001676684187259525\n",
      "Iteration 1266/2000, Loss: 0.00023921734828036278\n",
      "Iteration 1267/2000, Loss: 0.00014566756726708263\n",
      "Iteration 1268/2000, Loss: 0.0003659891081042588\n",
      "Iteration 1269/2000, Loss: 0.00030930081265978515\n",
      "Iteration 1270/2000, Loss: 0.00013816186401527375\n",
      "Iteration 1271/2000, Loss: 0.00018676009494811296\n",
      "Iteration 1272/2000, Loss: 0.00016135931946337223\n",
      "Iteration 1273/2000, Loss: 0.00034496741136536\n",
      "Iteration 1274/2000, Loss: 0.0002715189475566149\n",
      "Iteration 1275/2000, Loss: 0.00045021442929282784\n",
      "Iteration 1276/2000, Loss: 0.00034070093533955514\n",
      "Iteration 1277/2000, Loss: 0.00020418291387613863\n",
      "Iteration 1278/2000, Loss: 0.00018392593483440578\n",
      "Iteration 1279/2000, Loss: 0.00018412526696920395\n",
      "Iteration 1280/2000, Loss: 0.00014645249757450074\n",
      "Iteration 1281/2000, Loss: 0.0003193666343577206\n",
      "Iteration 1282/2000, Loss: 0.00019977774354629219\n",
      "Iteration 1283/2000, Loss: 0.0005345444660633802\n",
      "Iteration 1284/2000, Loss: 0.00016323085583280772\n",
      "Iteration 1285/2000, Loss: 0.00022632189211435616\n",
      "Iteration 1286/2000, Loss: 0.00025916376034729183\n",
      "Iteration 1287/2000, Loss: 0.00022076316236052662\n",
      "Iteration 1288/2000, Loss: 0.00015912669186946005\n",
      "Iteration 1289/2000, Loss: 0.00013793011021334678\n",
      "Iteration 1290/2000, Loss: 0.0002519259287510067\n",
      "Iteration 1291/2000, Loss: 0.000152037144289352\n",
      "Iteration 1292/2000, Loss: 0.0002299211046192795\n",
      "Iteration 1293/2000, Loss: 0.0001683132431935519\n",
      "Iteration 1294/2000, Loss: 0.00015699978393968195\n",
      "Iteration 1295/2000, Loss: 0.00021604521316476166\n",
      "Iteration 1296/2000, Loss: 0.0002038163220277056\n",
      "Iteration 1297/2000, Loss: 0.00022704093134962022\n",
      "Iteration 1298/2000, Loss: 0.00016911057173274457\n",
      "Iteration 1299/2000, Loss: 0.00022935433662496507\n",
      "Iteration 1300/2000, Loss: 0.000263340916717425\n",
      "Iteration 1301/2000, Loss: 0.0003116556035820395\n",
      "Iteration 1302/2000, Loss: 0.0003867186023853719\n",
      "Iteration 1303/2000, Loss: 0.00012741138925775886\n",
      "Iteration 1304/2000, Loss: 0.000192367922863923\n",
      "Iteration 1305/2000, Loss: 0.00014955922961235046\n",
      "Iteration 1306/2000, Loss: 0.0001155718055088073\n",
      "Iteration 1307/2000, Loss: 0.0002990140928886831\n",
      "Iteration 1308/2000, Loss: 0.00025181914679706097\n",
      "Iteration 1309/2000, Loss: 0.00028421409660950303\n",
      "Iteration 1310/2000, Loss: 0.0002636211283970624\n",
      "Iteration 1311/2000, Loss: 0.00030095281545072794\n",
      "Iteration 1312/2000, Loss: 0.000235003448324278\n",
      "Iteration 1313/2000, Loss: 0.0003766570589505136\n",
      "Iteration 1314/2000, Loss: 0.000193215993931517\n",
      "Iteration 1315/2000, Loss: 0.00022822761093266308\n",
      "Iteration 1316/2000, Loss: 0.00015835223894100636\n",
      "Iteration 1317/2000, Loss: 0.0002228005905635655\n",
      "Iteration 1318/2000, Loss: 0.00019088643603026867\n",
      "Iteration 1319/2000, Loss: 0.00014437473146244884\n",
      "Iteration 1320/2000, Loss: 0.0003431170480325818\n",
      "Iteration 1321/2000, Loss: 0.0002199657028540969\n",
      "Iteration 1322/2000, Loss: 0.0001871380809461698\n",
      "Iteration 1323/2000, Loss: 0.0002051733754342422\n",
      "Iteration 1324/2000, Loss: 0.00015278620412573218\n",
      "Iteration 1325/2000, Loss: 0.00010742498852778226\n",
      "Iteration 1326/2000, Loss: 0.00019750696083065122\n",
      "Iteration 1327/2000, Loss: 0.00012052951205987483\n",
      "Iteration 1328/2000, Loss: 0.0001832992275012657\n",
      "Iteration 1329/2000, Loss: 0.00010945289250230417\n",
      "Iteration 1330/2000, Loss: 0.00012200708442833275\n",
      "Iteration 1331/2000, Loss: 0.00017460751405451447\n",
      "Iteration 1332/2000, Loss: 0.0002781690564006567\n",
      "Iteration 1333/2000, Loss: 0.0001568230800330639\n",
      "Iteration 1334/2000, Loss: 0.00031025856151245534\n",
      "Iteration 1335/2000, Loss: 0.00015122393961064517\n",
      "Iteration 1336/2000, Loss: 0.00016075614257715642\n",
      "Iteration 1337/2000, Loss: 0.00016671113553456962\n",
      "Iteration 1338/2000, Loss: 0.00013968246639706194\n",
      "Iteration 1339/2000, Loss: 0.0002798790519591421\n",
      "Iteration 1340/2000, Loss: 0.00015999637253116816\n",
      "Iteration 1341/2000, Loss: 0.00019885117944795638\n",
      "Iteration 1342/2000, Loss: 0.00010114313045050949\n",
      "Iteration 1343/2000, Loss: 0.00021905360335949808\n",
      "Iteration 1344/2000, Loss: 0.00024107637000270188\n",
      "Iteration 1345/2000, Loss: 8.70977237354964e-05\n",
      "Iteration 1346/2000, Loss: 0.00010281083814334124\n",
      "Iteration 1347/2000, Loss: 0.00014000457304064184\n",
      "Iteration 1348/2000, Loss: 0.0002605557965580374\n",
      "Iteration 1349/2000, Loss: 0.0002668963570613414\n",
      "Iteration 1350/2000, Loss: 0.0002595695841591805\n",
      "Iteration 1351/2000, Loss: 0.00021626584930345416\n",
      "Iteration 1352/2000, Loss: 0.00014347047545015812\n",
      "Iteration 1353/2000, Loss: 0.00011404226097511128\n",
      "Iteration 1354/2000, Loss: 0.0001158912418759428\n",
      "Iteration 1355/2000, Loss: 0.00016427473747171462\n",
      "Iteration 1356/2000, Loss: 0.00014468806330114603\n",
      "Iteration 1357/2000, Loss: 0.00014473490591626614\n",
      "Iteration 1358/2000, Loss: 0.0001485731772845611\n",
      "Iteration 1359/2000, Loss: 0.00014358769112732261\n",
      "Iteration 1360/2000, Loss: 0.00014525525330100209\n",
      "Iteration 1361/2000, Loss: 0.0001993369369301945\n",
      "Iteration 1362/2000, Loss: 0.00025358962011523545\n",
      "Iteration 1363/2000, Loss: 0.00019955197058152407\n",
      "Iteration 1364/2000, Loss: 0.00021594524150714278\n",
      "Iteration 1365/2000, Loss: 0.00015867521869949996\n",
      "Iteration 1366/2000, Loss: 0.00014887515862938017\n",
      "Iteration 1367/2000, Loss: 0.00021663494408130646\n",
      "Iteration 1368/2000, Loss: 0.0005238459561951458\n",
      "Iteration 1369/2000, Loss: 0.00019663490820676088\n",
      "Iteration 1370/2000, Loss: 0.00011728530080290511\n",
      "Iteration 1371/2000, Loss: 0.0002906630397774279\n",
      "Iteration 1372/2000, Loss: 0.00011410439037717879\n",
      "Iteration 1373/2000, Loss: 0.00030254267039708793\n",
      "Iteration 1374/2000, Loss: 0.00022196907957550138\n",
      "Iteration 1375/2000, Loss: 0.0001836794544942677\n",
      "Iteration 1376/2000, Loss: 0.0003325403667986393\n",
      "Iteration 1377/2000, Loss: 0.0001823382917791605\n",
      "Iteration 1378/2000, Loss: 0.00016217930533457547\n",
      "Iteration 1379/2000, Loss: 0.00020502870029304177\n",
      "Iteration 1380/2000, Loss: 0.0004081582010257989\n",
      "Iteration 1381/2000, Loss: 0.00016160188533831388\n",
      "Iteration 1382/2000, Loss: 0.00019162951502949\n",
      "Iteration 1383/2000, Loss: 0.0003021824231836945\n",
      "Iteration 1384/2000, Loss: 0.0001239008706761524\n",
      "Iteration 1385/2000, Loss: 0.00016215119103435427\n",
      "Iteration 1386/2000, Loss: 0.00018755676865112036\n",
      "Iteration 1387/2000, Loss: 0.0004811928665731102\n",
      "Iteration 1388/2000, Loss: 0.00047772383550181985\n",
      "Iteration 1389/2000, Loss: 0.00023451261222362518\n",
      "Iteration 1390/2000, Loss: 0.00024776317877694964\n",
      "Iteration 1391/2000, Loss: 0.00029273112886585295\n",
      "Iteration 1392/2000, Loss: 0.00019003151101060212\n",
      "Iteration 1393/2000, Loss: 0.0003734310739673674\n",
      "Iteration 1394/2000, Loss: 0.0003258560609538108\n",
      "Iteration 1395/2000, Loss: 0.00018179028120357543\n",
      "Iteration 1396/2000, Loss: 0.00033450144110247493\n",
      "Iteration 1397/2000, Loss: 0.0002218377630924806\n",
      "Iteration 1398/2000, Loss: 0.00028047195519320667\n",
      "Iteration 1399/2000, Loss: 0.000270711665507406\n",
      "Iteration 1400/2000, Loss: 0.0003335221263114363\n",
      "Iteration 1401/2000, Loss: 0.00029587329481728375\n",
      "Iteration 1402/2000, Loss: 0.00014045646821614355\n",
      "Iteration 1403/2000, Loss: 0.00025308120530098677\n",
      "Iteration 1404/2000, Loss: 0.0001841540215536952\n",
      "Iteration 1405/2000, Loss: 0.00021663456573151052\n",
      "Iteration 1406/2000, Loss: 0.0001580876560183242\n",
      "Iteration 1407/2000, Loss: 0.00017900466627907008\n",
      "Iteration 1408/2000, Loss: 0.00026028024149127305\n",
      "Iteration 1409/2000, Loss: 0.00013347032654564828\n",
      "Iteration 1410/2000, Loss: 0.00017255281272809952\n",
      "Iteration 1411/2000, Loss: 0.00019916663586627692\n",
      "Iteration 1412/2000, Loss: 0.00035707480856217444\n",
      "Iteration 1413/2000, Loss: 0.00043809908675029874\n",
      "Iteration 1414/2000, Loss: 0.00035984424175694585\n",
      "Iteration 1415/2000, Loss: 0.00030235719168558717\n",
      "Iteration 1416/2000, Loss: 0.0002908928436227143\n",
      "Iteration 1417/2000, Loss: 0.00032235917751677334\n",
      "Iteration 1418/2000, Loss: 0.0003752820775844157\n",
      "Iteration 1419/2000, Loss: 0.00036018301034346223\n",
      "Iteration 1420/2000, Loss: 0.0003745469148270786\n",
      "Iteration 1421/2000, Loss: 0.0005162434536032379\n",
      "Iteration 1422/2000, Loss: 0.00024497645790688694\n",
      "Iteration 1423/2000, Loss: 0.0004322577442508191\n",
      "Iteration 1424/2000, Loss: 0.00023349988623522222\n",
      "Iteration 1425/2000, Loss: 0.0005001261015422642\n",
      "Iteration 1426/2000, Loss: 0.00026267694192938507\n",
      "Iteration 1427/2000, Loss: 0.0003672800667118281\n",
      "Iteration 1428/2000, Loss: 0.00033054137020371854\n",
      "Iteration 1429/2000, Loss: 0.0003282072430010885\n",
      "Iteration 1430/2000, Loss: 0.0003102836199104786\n",
      "Iteration 1431/2000, Loss: 0.00023545982548967004\n",
      "Iteration 1432/2000, Loss: 0.00044460725621320307\n",
      "Iteration 1433/2000, Loss: 0.00011290473776170984\n",
      "Iteration 1434/2000, Loss: 0.00035610029590316117\n",
      "Iteration 1435/2000, Loss: 0.00017388316337019205\n",
      "Iteration 1436/2000, Loss: 0.00021949874644633383\n",
      "Iteration 1437/2000, Loss: 0.00020227553613949567\n",
      "Iteration 1438/2000, Loss: 0.00037663866532966495\n",
      "Iteration 1439/2000, Loss: 0.00027909825439564884\n",
      "Iteration 1440/2000, Loss: 0.0002021319087361917\n",
      "Iteration 1441/2000, Loss: 0.00035210399073548615\n",
      "Iteration 1442/2000, Loss: 0.0003266377025283873\n",
      "Iteration 1443/2000, Loss: 0.0004794219566974789\n",
      "Iteration 1444/2000, Loss: 0.00029297376750037074\n",
      "Iteration 1445/2000, Loss: 0.00021541593014262617\n",
      "Iteration 1446/2000, Loss: 0.0001853374415077269\n",
      "Iteration 1447/2000, Loss: 0.00024142894835676998\n",
      "Iteration 1448/2000, Loss: 0.0002610097290016711\n",
      "Iteration 1449/2000, Loss: 0.0002692493435461074\n",
      "Iteration 1450/2000, Loss: 0.00023523943673353642\n",
      "Iteration 1451/2000, Loss: 0.00029196194373071194\n",
      "Iteration 1452/2000, Loss: 0.0002092151844408363\n",
      "Iteration 1453/2000, Loss: 0.00020225858315825462\n",
      "Iteration 1454/2000, Loss: 0.000183092852239497\n",
      "Iteration 1455/2000, Loss: 0.00014683220069855452\n",
      "Iteration 1456/2000, Loss: 0.00023454651818610728\n",
      "Iteration 1457/2000, Loss: 0.00017123454017564654\n",
      "Iteration 1458/2000, Loss: 0.00030027347384020686\n",
      "Iteration 1459/2000, Loss: 0.0001444632071070373\n",
      "Iteration 1460/2000, Loss: 0.00019535610044840723\n",
      "Iteration 1461/2000, Loss: 0.0003212864976376295\n",
      "Iteration 1462/2000, Loss: 0.0005204747430980206\n",
      "Iteration 1463/2000, Loss: 0.00022411129612009972\n",
      "Iteration 1464/2000, Loss: 0.00035579342511482537\n",
      "Iteration 1465/2000, Loss: 0.00017376636969856918\n",
      "Iteration 1466/2000, Loss: 0.0002439979580231011\n",
      "Iteration 1467/2000, Loss: 0.00022297282703220844\n",
      "Iteration 1468/2000, Loss: 0.00019941730715800077\n",
      "Iteration 1469/2000, Loss: 0.0003178671468049288\n",
      "Iteration 1470/2000, Loss: 0.000253969308687374\n",
      "Iteration 1471/2000, Loss: 0.00035365333314985037\n",
      "Iteration 1472/2000, Loss: 0.0002467506565153599\n",
      "Iteration 1473/2000, Loss: 0.00012814602814614773\n",
      "Iteration 1474/2000, Loss: 0.00018809418543241918\n",
      "Iteration 1475/2000, Loss: 0.00013852029223926365\n",
      "Iteration 1476/2000, Loss: 0.00029891415033489466\n",
      "Iteration 1477/2000, Loss: 0.00023445270198863\n",
      "Iteration 1478/2000, Loss: 0.00031072096317075193\n",
      "Iteration 1479/2000, Loss: 0.00012979352322872728\n",
      "Iteration 1480/2000, Loss: 0.00013970535655971617\n",
      "Iteration 1481/2000, Loss: 0.0002952575159724802\n",
      "Iteration 1482/2000, Loss: 0.00022985963732935488\n",
      "Iteration 1483/2000, Loss: 0.00011947574967052788\n",
      "Iteration 1484/2000, Loss: 0.00036285092937760055\n",
      "Iteration 1485/2000, Loss: 0.00022789993090555072\n",
      "Iteration 1486/2000, Loss: 0.0002665167849045247\n",
      "Iteration 1487/2000, Loss: 0.0001378922170260921\n",
      "Iteration 1488/2000, Loss: 0.00012461154256016016\n",
      "Iteration 1489/2000, Loss: 0.0002182306780014187\n",
      "Iteration 1490/2000, Loss: 0.0001485426619183272\n",
      "Iteration 1491/2000, Loss: 0.00014464175910688937\n",
      "Iteration 1492/2000, Loss: 0.00022605319099966437\n",
      "Iteration 1493/2000, Loss: 0.0001305824553128332\n",
      "Iteration 1494/2000, Loss: 0.000188816076843068\n",
      "Iteration 1495/2000, Loss: 0.0002411770838079974\n",
      "Iteration 1496/2000, Loss: 0.00015492239617742598\n",
      "Iteration 1497/2000, Loss: 0.00020440535445231944\n",
      "Iteration 1498/2000, Loss: 0.0002258384192828089\n",
      "Iteration 1499/2000, Loss: 0.00013725750613957644\n",
      "Iteration 1500/2000, Loss: 0.0001820036122808233\n",
      "Iteration 1501/2000, Loss: 0.0002510503982193768\n",
      "Iteration 1502/2000, Loss: 0.00020145125745330006\n",
      "Iteration 1503/2000, Loss: 0.0003148893010802567\n",
      "Iteration 1504/2000, Loss: 0.00015799170068930835\n",
      "Iteration 1505/2000, Loss: 0.0003628638223744929\n",
      "Iteration 1506/2000, Loss: 0.00033321583759970963\n",
      "Iteration 1507/2000, Loss: 0.0002323220978723839\n",
      "Iteration 1508/2000, Loss: 0.0002596908889245242\n",
      "Iteration 1509/2000, Loss: 0.00012407447502482682\n",
      "Iteration 1510/2000, Loss: 0.0002949709305539727\n",
      "Iteration 1511/2000, Loss: 0.00018043589079752564\n",
      "Iteration 1512/2000, Loss: 0.00015673616144340485\n",
      "Iteration 1513/2000, Loss: 0.00019179194350726902\n",
      "Iteration 1514/2000, Loss: 0.00012334695202298462\n",
      "Iteration 1515/2000, Loss: 0.00016476250311825424\n",
      "Iteration 1516/2000, Loss: 0.0001530904119135812\n",
      "Iteration 1517/2000, Loss: 0.00014787924010306597\n",
      "Iteration 1518/2000, Loss: 0.00022740935673937201\n",
      "Iteration 1519/2000, Loss: 0.0001791427785065025\n",
      "Iteration 1520/2000, Loss: 0.00033483447623439133\n",
      "Iteration 1521/2000, Loss: 0.00020720429893117398\n",
      "Iteration 1522/2000, Loss: 0.00044563456322066486\n",
      "Iteration 1523/2000, Loss: 0.00023143336875364184\n",
      "Iteration 1524/2000, Loss: 0.00021312151511665434\n",
      "Iteration 1525/2000, Loss: 0.00017147728067357093\n",
      "Iteration 1526/2000, Loss: 0.0001761412713676691\n",
      "Iteration 1527/2000, Loss: 0.0002952899958472699\n",
      "Iteration 1528/2000, Loss: 0.00016771182708907872\n",
      "Iteration 1529/2000, Loss: 0.0002860724925994873\n",
      "Iteration 1530/2000, Loss: 0.0002917686360888183\n",
      "Iteration 1531/2000, Loss: 0.0006230754079297185\n",
      "Iteration 1532/2000, Loss: 0.0002850888704415411\n",
      "Iteration 1533/2000, Loss: 0.00022356364934239537\n",
      "Iteration 1534/2000, Loss: 0.00036433053901419044\n",
      "Iteration 1535/2000, Loss: 0.00018563700723461807\n",
      "Iteration 1536/2000, Loss: 0.00029447273118421435\n",
      "Iteration 1537/2000, Loss: 0.00020059425150975585\n",
      "Iteration 1538/2000, Loss: 0.00021375954383984208\n",
      "Iteration 1539/2000, Loss: 0.0002599197323434055\n",
      "Iteration 1540/2000, Loss: 0.0003087772929575294\n",
      "Iteration 1541/2000, Loss: 0.000341639737598598\n",
      "Iteration 1542/2000, Loss: 0.000257563020568341\n",
      "Iteration 1543/2000, Loss: 0.00018234469462186098\n",
      "Iteration 1544/2000, Loss: 0.00026638791314326227\n",
      "Iteration 1545/2000, Loss: 0.0002248438831884414\n",
      "Iteration 1546/2000, Loss: 0.00020962586859241128\n",
      "Iteration 1547/2000, Loss: 0.0005521895946003497\n",
      "Iteration 1548/2000, Loss: 0.00034393215901218355\n",
      "Iteration 1549/2000, Loss: 0.00015770552272442728\n",
      "Iteration 1550/2000, Loss: 0.000274692167295143\n",
      "Iteration 1551/2000, Loss: 0.0003336845838930458\n",
      "Iteration 1552/2000, Loss: 0.000504541676491499\n",
      "Iteration 1553/2000, Loss: 0.00017666493658907712\n",
      "Iteration 1554/2000, Loss: 0.00044828077079728246\n",
      "Iteration 1555/2000, Loss: 0.0007465921225957572\n",
      "Iteration 1556/2000, Loss: 0.0002902893756981939\n",
      "Iteration 1557/2000, Loss: 0.0006082680774852633\n",
      "Iteration 1558/2000, Loss: 0.0004623686254490167\n",
      "Iteration 1559/2000, Loss: 0.0006798054091632366\n",
      "Iteration 1560/2000, Loss: 0.0004607137816492468\n",
      "Iteration 1561/2000, Loss: 0.0006320259417407215\n",
      "Iteration 1562/2000, Loss: 0.00036360297235660255\n",
      "Iteration 1563/2000, Loss: 0.0003708412987180054\n",
      "Iteration 1564/2000, Loss: 0.00022871930559631437\n",
      "Iteration 1565/2000, Loss: 0.0005884990678168833\n",
      "Iteration 1566/2000, Loss: 0.0002053977659670636\n",
      "Iteration 1567/2000, Loss: 0.0005259399185888469\n",
      "Iteration 1568/2000, Loss: 0.000543816655408591\n",
      "Iteration 1569/2000, Loss: 0.00038421773933805525\n",
      "Iteration 1570/2000, Loss: 0.0006516848807223141\n",
      "Iteration 1571/2000, Loss: 0.00019176999921910465\n",
      "Iteration 1572/2000, Loss: 0.0006780464318580925\n",
      "Iteration 1573/2000, Loss: 0.00018100463785231113\n",
      "Iteration 1574/2000, Loss: 0.0011482215486466885\n",
      "Iteration 1575/2000, Loss: 0.00021236768225207925\n",
      "Iteration 1576/2000, Loss: 0.0009419725392945111\n",
      "Iteration 1577/2000, Loss: 0.00033305154647678137\n",
      "Iteration 1578/2000, Loss: 0.0009430068894289434\n",
      "Iteration 1579/2000, Loss: 0.0003890249354299158\n",
      "Iteration 1580/2000, Loss: 0.0008886444848030806\n",
      "Iteration 1581/2000, Loss: 0.0011784922098740935\n",
      "Iteration 1582/2000, Loss: 0.00028464579372666776\n",
      "Iteration 1583/2000, Loss: 0.000521332782227546\n",
      "Iteration 1584/2000, Loss: 0.0004908994887955487\n",
      "Iteration 1585/2000, Loss: 0.0005534561350941658\n",
      "Iteration 1586/2000, Loss: 0.0003622942022047937\n",
      "Iteration 1587/2000, Loss: 0.0008141566067934036\n",
      "Iteration 1588/2000, Loss: 0.0005224207998253405\n",
      "Iteration 1589/2000, Loss: 0.0008227756479755044\n",
      "Iteration 1590/2000, Loss: 0.0009723260882310569\n",
      "Iteration 1591/2000, Loss: 0.0007368549122475088\n",
      "Iteration 1592/2000, Loss: 0.0012606816599145532\n",
      "Iteration 1593/2000, Loss: 0.00040522575727663934\n",
      "Iteration 1594/2000, Loss: 0.0007221389096230268\n",
      "Iteration 1595/2000, Loss: 0.00038742207107134163\n",
      "Iteration 1596/2000, Loss: 0.0005953050567768514\n",
      "Iteration 1597/2000, Loss: 0.0006079917075112462\n",
      "Iteration 1598/2000, Loss: 0.0006702252430841327\n",
      "Iteration 1599/2000, Loss: 0.0006706113927066326\n",
      "Iteration 1600/2000, Loss: 0.000588166352827102\n",
      "Iteration 1601/2000, Loss: 0.00046572828432545066\n",
      "Iteration 1602/2000, Loss: 0.00037223720573820174\n",
      "Iteration 1603/2000, Loss: 0.00043972235289402306\n",
      "Iteration 1604/2000, Loss: 0.00023814491578377783\n",
      "Iteration 1605/2000, Loss: 0.0005557695403695107\n",
      "Iteration 1606/2000, Loss: 0.0002567601331975311\n",
      "Iteration 1607/2000, Loss: 0.000353623297996819\n",
      "Iteration 1608/2000, Loss: 0.000435416994150728\n",
      "Iteration 1609/2000, Loss: 0.000293763296213001\n",
      "Iteration 1610/2000, Loss: 0.0003230719594284892\n",
      "Iteration 1611/2000, Loss: 0.000506150652654469\n",
      "Iteration 1612/2000, Loss: 0.00045297713950276375\n",
      "Iteration 1613/2000, Loss: 0.00038016177131794393\n",
      "Iteration 1614/2000, Loss: 0.00042968837078660727\n",
      "Iteration 1615/2000, Loss: 0.0007179927779361606\n",
      "Iteration 1616/2000, Loss: 0.0002977273834403604\n",
      "Iteration 1617/2000, Loss: 0.0004028130497317761\n",
      "Iteration 1618/2000, Loss: 0.0003066019562538713\n",
      "Iteration 1619/2000, Loss: 0.0003584536607377231\n",
      "Iteration 1620/2000, Loss: 0.0007436458836309612\n",
      "Iteration 1621/2000, Loss: 0.0003689310105983168\n",
      "Iteration 1622/2000, Loss: 0.0006411196663975716\n",
      "Iteration 1623/2000, Loss: 0.0007909483392722905\n",
      "Iteration 1624/2000, Loss: 0.0003180938365403563\n",
      "Iteration 1625/2000, Loss: 0.0007842183695174754\n",
      "Iteration 1626/2000, Loss: 0.0002582140441518277\n",
      "Iteration 1627/2000, Loss: 0.0010260564740747213\n",
      "Iteration 1628/2000, Loss: 0.00020077622320968658\n",
      "Iteration 1629/2000, Loss: 0.0007396966684609652\n",
      "Iteration 1630/2000, Loss: 0.0002611161908134818\n",
      "Iteration 1631/2000, Loss: 0.0009288916480727494\n",
      "Iteration 1632/2000, Loss: 0.00023322587367147207\n",
      "Iteration 1633/2000, Loss: 0.0009564373176544905\n",
      "Iteration 1634/2000, Loss: 0.0001969312143046409\n",
      "Iteration 1635/2000, Loss: 0.0006189097766764462\n",
      "Iteration 1636/2000, Loss: 0.0002894176868721843\n",
      "Iteration 1637/2000, Loss: 0.0006308783777058125\n",
      "Iteration 1638/2000, Loss: 0.00018088390061166137\n",
      "Iteration 1639/2000, Loss: 0.0006108453380875289\n",
      "Iteration 1640/2000, Loss: 0.00010732634109444916\n",
      "Iteration 1641/2000, Loss: 0.0004611156473401934\n",
      "Iteration 1642/2000, Loss: 0.00016370322555303574\n",
      "Iteration 1643/2000, Loss: 0.00040048142545856535\n",
      "Iteration 1644/2000, Loss: 0.00024151618708856404\n",
      "Iteration 1645/2000, Loss: 0.00035188146284781396\n",
      "Iteration 1646/2000, Loss: 0.00017147260950878263\n",
      "Iteration 1647/2000, Loss: 0.0002646053326316178\n",
      "Iteration 1648/2000, Loss: 0.0003537318261805922\n",
      "Iteration 1649/2000, Loss: 0.00042778559145517647\n",
      "Iteration 1650/2000, Loss: 0.00017154999659396708\n",
      "Iteration 1651/2000, Loss: 0.00032018794445320964\n",
      "Iteration 1652/2000, Loss: 0.0001531967573100701\n",
      "Iteration 1653/2000, Loss: 0.0003689208533614874\n",
      "Iteration 1654/2000, Loss: 0.00012319182860665023\n",
      "Iteration 1655/2000, Loss: 0.00047030209680087864\n",
      "Iteration 1656/2000, Loss: 0.00021825371368322521\n",
      "Iteration 1657/2000, Loss: 0.0002802179951686412\n",
      "Iteration 1658/2000, Loss: 0.0002080806443700567\n",
      "Iteration 1659/2000, Loss: 0.0001403620990458876\n",
      "Iteration 1660/2000, Loss: 0.00018934658146463335\n",
      "Iteration 1661/2000, Loss: 0.00022353978420142084\n",
      "Iteration 1662/2000, Loss: 0.00019670424808282405\n",
      "Iteration 1663/2000, Loss: 0.00020788605615962297\n",
      "Iteration 1664/2000, Loss: 0.00018600384646560997\n",
      "Iteration 1665/2000, Loss: 0.0001801913313101977\n",
      "Iteration 1666/2000, Loss: 0.0002912486088462174\n",
      "Iteration 1667/2000, Loss: 0.00022374354011844844\n",
      "Iteration 1668/2000, Loss: 0.0001399026223225519\n",
      "Iteration 1669/2000, Loss: 0.00014356645988300443\n",
      "Iteration 1670/2000, Loss: 0.00018823902064468712\n",
      "Iteration 1671/2000, Loss: 0.00012224554666318\n",
      "Iteration 1672/2000, Loss: 0.00027231688727624714\n",
      "Iteration 1673/2000, Loss: 0.00016918592154979706\n",
      "Iteration 1674/2000, Loss: 0.0001975351624423638\n",
      "Iteration 1675/2000, Loss: 0.000525216746609658\n",
      "Iteration 1676/2000, Loss: 0.00020327720267232507\n",
      "Iteration 1677/2000, Loss: 0.00014588945487048477\n",
      "Iteration 1678/2000, Loss: 0.0002595603000372648\n",
      "Iteration 1679/2000, Loss: 0.00019948203407693654\n",
      "Iteration 1680/2000, Loss: 0.00014603359159082174\n",
      "Iteration 1681/2000, Loss: 0.00014024486881680787\n",
      "Iteration 1682/2000, Loss: 0.0002065014559775591\n",
      "Iteration 1683/2000, Loss: 0.00018097889551427215\n",
      "Iteration 1684/2000, Loss: 0.0001859396870713681\n",
      "Iteration 1685/2000, Loss: 0.00022792191884946078\n",
      "Iteration 1686/2000, Loss: 0.00019010319374501705\n",
      "Iteration 1687/2000, Loss: 0.0002497470995876938\n",
      "Iteration 1688/2000, Loss: 0.00028965488309040666\n",
      "Iteration 1689/2000, Loss: 0.00025825019110925496\n",
      "Iteration 1690/2000, Loss: 0.00022037036251276731\n",
      "Iteration 1691/2000, Loss: 0.0002841346140485257\n",
      "Iteration 1692/2000, Loss: 0.00012226322724018246\n",
      "Iteration 1693/2000, Loss: 0.00017241026216652244\n",
      "Iteration 1694/2000, Loss: 0.00027433194918558\n",
      "Iteration 1695/2000, Loss: 0.0001558930380269885\n",
      "Iteration 1696/2000, Loss: 0.0001524737454019487\n",
      "Iteration 1697/2000, Loss: 0.0001590326864970848\n",
      "Iteration 1698/2000, Loss: 0.00017368202679790556\n",
      "Iteration 1699/2000, Loss: 0.0004710772482212633\n",
      "Iteration 1700/2000, Loss: 0.0001669066696194932\n",
      "Iteration 1701/2000, Loss: 0.00033760390942916274\n",
      "Iteration 1702/2000, Loss: 0.00011837403872050345\n",
      "Iteration 1703/2000, Loss: 0.0002675312862265855\n",
      "Iteration 1704/2000, Loss: 0.0003698969667311758\n",
      "Iteration 1705/2000, Loss: 0.0007187094888649881\n",
      "Iteration 1706/2000, Loss: 0.0001698692503850907\n",
      "Iteration 1707/2000, Loss: 0.0002908848400693387\n",
      "Iteration 1708/2000, Loss: 0.00028764945454895496\n",
      "Iteration 1709/2000, Loss: 0.00026373082073405385\n",
      "Iteration 1710/2000, Loss: 0.00011727247328963131\n",
      "Iteration 1711/2000, Loss: 0.00030342803802341223\n",
      "Iteration 1712/2000, Loss: 0.0004244929878041148\n",
      "Iteration 1713/2000, Loss: 0.00013650326582137495\n",
      "Iteration 1714/2000, Loss: 0.00015291366435121745\n",
      "Iteration 1715/2000, Loss: 0.00022191925381775945\n",
      "Iteration 1716/2000, Loss: 0.0002855087222997099\n",
      "Iteration 1717/2000, Loss: 0.00014933956845197827\n",
      "Iteration 1718/2000, Loss: 0.00018607995298225433\n",
      "Iteration 1719/2000, Loss: 0.00011484899732749909\n",
      "Iteration 1720/2000, Loss: 0.00014734240539837629\n",
      "Iteration 1721/2000, Loss: 0.00021357557852752507\n",
      "Iteration 1722/2000, Loss: 0.00021361050312407315\n",
      "Iteration 1723/2000, Loss: 0.00011671998800011352\n",
      "Iteration 1724/2000, Loss: 0.00026122876442968845\n",
      "Iteration 1725/2000, Loss: 0.00024078050046227872\n",
      "Iteration 1726/2000, Loss: 0.00013263423170428723\n",
      "Iteration 1727/2000, Loss: 0.00019453013374004513\n",
      "Iteration 1728/2000, Loss: 0.00019530305871739984\n",
      "Iteration 1729/2000, Loss: 0.00019443794735707343\n",
      "Iteration 1730/2000, Loss: 0.00015370977052953094\n",
      "Iteration 1731/2000, Loss: 0.0001511845039203763\n",
      "Iteration 1732/2000, Loss: 0.00033158736187033355\n",
      "Iteration 1733/2000, Loss: 0.0001501757651567459\n",
      "Iteration 1734/2000, Loss: 0.0002140900178346783\n",
      "Iteration 1735/2000, Loss: 0.00021103715698700398\n",
      "Iteration 1736/2000, Loss: 0.00013890421541873366\n",
      "Iteration 1737/2000, Loss: 0.00018673305748961866\n",
      "Iteration 1738/2000, Loss: 0.00015718203212600201\n",
      "Iteration 1739/2000, Loss: 0.00016564100224059075\n",
      "Iteration 1740/2000, Loss: 0.00022123369853943586\n",
      "Iteration 1741/2000, Loss: 0.0001307981729041785\n",
      "Iteration 1742/2000, Loss: 0.0001917156478157267\n",
      "Iteration 1743/2000, Loss: 0.000143673358252272\n",
      "Iteration 1744/2000, Loss: 0.00014425655535887927\n",
      "Iteration 1745/2000, Loss: 0.00015049325884319842\n",
      "Iteration 1746/2000, Loss: 0.00024786408175714314\n",
      "Iteration 1747/2000, Loss: 0.00014652796380687505\n",
      "Iteration 1748/2000, Loss: 0.0001574050693307072\n",
      "Iteration 1749/2000, Loss: 0.00011417870700825006\n",
      "Iteration 1750/2000, Loss: 0.00014072364137973636\n",
      "Iteration 1751/2000, Loss: 0.00011702425399562344\n",
      "Iteration 1752/2000, Loss: 0.0002098330296576023\n",
      "Iteration 1753/2000, Loss: 0.00012793853238690645\n",
      "Iteration 1754/2000, Loss: 0.00018174960860051215\n",
      "Iteration 1755/2000, Loss: 0.00016489476547576487\n",
      "Iteration 1756/2000, Loss: 0.00011650715168798342\n",
      "Iteration 1757/2000, Loss: 9.378309914609417e-05\n",
      "Iteration 1758/2000, Loss: 0.00038282672176137567\n",
      "Iteration 1759/2000, Loss: 0.000161747433594428\n",
      "Iteration 1760/2000, Loss: 0.00024748186115175486\n",
      "Iteration 1761/2000, Loss: 0.00017798543558456004\n",
      "Iteration 1762/2000, Loss: 0.00017421485972590744\n",
      "Iteration 1763/2000, Loss: 0.00015830225311219692\n",
      "Iteration 1764/2000, Loss: 0.000158169525093399\n",
      "Iteration 1765/2000, Loss: 0.00012104476627428085\n",
      "Iteration 1766/2000, Loss: 0.00033779573277570307\n",
      "Iteration 1767/2000, Loss: 0.00012093253462808207\n",
      "Iteration 1768/2000, Loss: 0.0002638187725096941\n",
      "Iteration 1769/2000, Loss: 0.00011228387302253395\n",
      "Iteration 1770/2000, Loss: 0.00017327343812212348\n",
      "Iteration 1771/2000, Loss: 0.00010078657942358404\n",
      "Iteration 1772/2000, Loss: 0.00023485985002480447\n",
      "Iteration 1773/2000, Loss: 0.0001578669180162251\n",
      "Iteration 1774/2000, Loss: 0.0006293528713285923\n",
      "Iteration 1775/2000, Loss: 0.0001944538817042485\n",
      "Iteration 1776/2000, Loss: 0.00017168982594739646\n",
      "Iteration 1777/2000, Loss: 0.00030190672259777784\n",
      "Iteration 1778/2000, Loss: 0.00021578441374003887\n",
      "Iteration 1779/2000, Loss: 0.00021104907500557601\n",
      "Iteration 1780/2000, Loss: 0.00027377885999158025\n",
      "Iteration 1781/2000, Loss: 0.00019880123727489263\n",
      "Iteration 1782/2000, Loss: 0.0001419773034285754\n",
      "Iteration 1783/2000, Loss: 0.00028952027787454426\n",
      "Iteration 1784/2000, Loss: 0.0001538152137072757\n",
      "Iteration 1785/2000, Loss: 0.00023285558563657105\n",
      "Iteration 1786/2000, Loss: 0.00019334760145284235\n",
      "Iteration 1787/2000, Loss: 0.00032494417973794043\n",
      "Iteration 1788/2000, Loss: 0.0002516551176086068\n",
      "Iteration 1789/2000, Loss: 0.0002956318494398147\n",
      "Iteration 1790/2000, Loss: 0.0003763095592148602\n",
      "Iteration 1791/2000, Loss: 0.00020632625091820955\n",
      "Iteration 1792/2000, Loss: 0.00010838017624337226\n",
      "Iteration 1793/2000, Loss: 0.00015229484415613115\n",
      "Iteration 1794/2000, Loss: 0.0002335355238756165\n",
      "Iteration 1795/2000, Loss: 0.0001971015881281346\n",
      "Iteration 1796/2000, Loss: 0.00018495583208277822\n",
      "Iteration 1797/2000, Loss: 0.00030538791907019913\n",
      "Iteration 1798/2000, Loss: 0.0001552408211864531\n",
      "Iteration 1799/2000, Loss: 0.00013962245429866016\n",
      "Iteration 1800/2000, Loss: 0.00017468199075665325\n",
      "Iteration 1801/2000, Loss: 0.00025501803611405194\n",
      "Iteration 1802/2000, Loss: 0.0002974755479954183\n",
      "Iteration 1803/2000, Loss: 0.0002934220538008958\n",
      "Iteration 1804/2000, Loss: 0.00020667123317252845\n",
      "Iteration 1805/2000, Loss: 0.0001986761053558439\n",
      "Iteration 1806/2000, Loss: 0.0001712641242193058\n",
      "Iteration 1807/2000, Loss: 0.00024937326088547707\n",
      "Iteration 1808/2000, Loss: 0.00016154847980942577\n",
      "Iteration 1809/2000, Loss: 0.0002643888001330197\n",
      "Iteration 1810/2000, Loss: 0.00013502540241461247\n",
      "Iteration 1811/2000, Loss: 0.00016690730990376323\n",
      "Iteration 1812/2000, Loss: 0.00022580812219530344\n",
      "Iteration 1813/2000, Loss: 0.00030242069624364376\n",
      "Iteration 1814/2000, Loss: 0.00017787452088668942\n",
      "Iteration 1815/2000, Loss: 0.0003092288097832352\n",
      "Iteration 1816/2000, Loss: 0.00023003778187558055\n",
      "Iteration 1817/2000, Loss: 0.00015935218834783882\n",
      "Iteration 1818/2000, Loss: 0.0001811297843232751\n",
      "Iteration 1819/2000, Loss: 0.00016849122766871005\n",
      "Iteration 1820/2000, Loss: 0.00021190701227169484\n",
      "Iteration 1821/2000, Loss: 0.0001809950772440061\n",
      "Iteration 1822/2000, Loss: 0.0003139517502859235\n",
      "Iteration 1823/2000, Loss: 0.00017806001415010542\n",
      "Iteration 1824/2000, Loss: 0.000203082017833367\n",
      "Iteration 1825/2000, Loss: 0.0002767481200862676\n",
      "Iteration 1826/2000, Loss: 0.0002492168569006026\n",
      "Iteration 1827/2000, Loss: 0.0003225280961487442\n",
      "Iteration 1828/2000, Loss: 0.00018811362679116428\n",
      "Iteration 1829/2000, Loss: 0.0002201549505116418\n",
      "Iteration 1830/2000, Loss: 0.0003946614742744714\n",
      "Iteration 1831/2000, Loss: 0.00018836872186511755\n",
      "Iteration 1832/2000, Loss: 0.00019132568559143692\n",
      "Iteration 1833/2000, Loss: 0.000311646523186937\n",
      "Iteration 1834/2000, Loss: 0.0003299449454061687\n",
      "Iteration 1835/2000, Loss: 0.00024833346833474934\n",
      "Iteration 1836/2000, Loss: 0.00018446626199875027\n",
      "Iteration 1837/2000, Loss: 0.000276306236628443\n",
      "Iteration 1838/2000, Loss: 0.000285173679003492\n",
      "Iteration 1839/2000, Loss: 0.0003144921502098441\n",
      "Iteration 1840/2000, Loss: 0.0001678817206993699\n",
      "Iteration 1841/2000, Loss: 0.0001195003860630095\n",
      "Iteration 1842/2000, Loss: 0.0005251795519143343\n",
      "Iteration 1843/2000, Loss: 0.00033243445795960724\n",
      "Iteration 1844/2000, Loss: 0.00015935026749502867\n",
      "Iteration 1845/2000, Loss: 0.0001970490557141602\n",
      "Iteration 1846/2000, Loss: 0.00040261889807879925\n",
      "Iteration 1847/2000, Loss: 0.00020745694928336889\n",
      "Iteration 1848/2000, Loss: 0.00018168948008678854\n",
      "Iteration 1849/2000, Loss: 0.0001272946537937969\n",
      "Iteration 1850/2000, Loss: 0.00036436773370951414\n",
      "Iteration 1851/2000, Loss: 0.00034358029370196164\n",
      "Iteration 1852/2000, Loss: 0.0002350185823161155\n",
      "Iteration 1853/2000, Loss: 0.0002364482352277264\n",
      "Iteration 1854/2000, Loss: 0.00018286840349901468\n",
      "Iteration 1855/2000, Loss: 0.0003803849103860557\n",
      "Iteration 1856/2000, Loss: 0.00016755402612034231\n",
      "Iteration 1857/2000, Loss: 0.00013966065307613462\n",
      "Iteration 1858/2000, Loss: 0.000184200907824561\n",
      "Iteration 1859/2000, Loss: 0.0001687938638497144\n",
      "Iteration 1860/2000, Loss: 0.0002725342055782676\n",
      "Iteration 1861/2000, Loss: 0.00017447247228119522\n",
      "Iteration 1862/2000, Loss: 0.00024917692644521594\n",
      "Iteration 1863/2000, Loss: 0.0001991001481655985\n",
      "Iteration 1864/2000, Loss: 0.00023612518270965666\n",
      "Iteration 1865/2000, Loss: 0.00021742867829743773\n",
      "Iteration 1866/2000, Loss: 0.00018685897521208972\n",
      "Iteration 1867/2000, Loss: 0.0006986538064666092\n",
      "Iteration 1868/2000, Loss: 0.00023877198691479862\n",
      "Iteration 1869/2000, Loss: 0.0003079051384702325\n",
      "Iteration 1870/2000, Loss: 0.0002844346163328737\n",
      "Iteration 1871/2000, Loss: 0.00012078108557034284\n",
      "Iteration 1872/2000, Loss: 0.0002933013311121613\n",
      "Iteration 1873/2000, Loss: 0.00014572942745871842\n",
      "Iteration 1874/2000, Loss: 0.0001981795358005911\n",
      "Iteration 1875/2000, Loss: 0.0002730937267187983\n",
      "Iteration 1876/2000, Loss: 0.000284553156234324\n",
      "Iteration 1877/2000, Loss: 0.0003003467572852969\n",
      "Iteration 1878/2000, Loss: 0.00017630300135351717\n",
      "Iteration 1879/2000, Loss: 0.00029533449560403824\n",
      "Iteration 1880/2000, Loss: 0.0001751603267621249\n",
      "Iteration 1881/2000, Loss: 0.0003774652141146362\n",
      "Iteration 1882/2000, Loss: 0.00042122488957829773\n",
      "Iteration 1883/2000, Loss: 0.0003264129045419395\n",
      "Iteration 1884/2000, Loss: 0.00037594264722429216\n",
      "Iteration 1885/2000, Loss: 0.00035215512616559863\n",
      "Iteration 1886/2000, Loss: 0.0003306520520709455\n",
      "Iteration 1887/2000, Loss: 0.0003836606629192829\n",
      "Iteration 1888/2000, Loss: 0.00024291611043736339\n",
      "Iteration 1889/2000, Loss: 0.0005111611098982394\n",
      "Iteration 1890/2000, Loss: 0.00029155725496821105\n",
      "Iteration 1891/2000, Loss: 0.0004208908067084849\n",
      "Iteration 1892/2000, Loss: 0.0002642950275912881\n",
      "Iteration 1893/2000, Loss: 0.0002487771271262318\n",
      "Iteration 1894/2000, Loss: 0.00022376667766366154\n",
      "Iteration 1895/2000, Loss: 0.00023513428459409624\n",
      "Iteration 1896/2000, Loss: 0.0002838373475242406\n",
      "Iteration 1897/2000, Loss: 0.0001699740969343111\n",
      "Iteration 1898/2000, Loss: 0.0004343792097643018\n",
      "Iteration 1899/2000, Loss: 0.0002276699960930273\n",
      "Iteration 1900/2000, Loss: 0.00015079094737302512\n",
      "Iteration 1901/2000, Loss: 0.00019855599384754896\n",
      "Iteration 1902/2000, Loss: 0.00031194579787552357\n",
      "Iteration 1903/2000, Loss: 0.00027515622787177563\n",
      "Iteration 1904/2000, Loss: 0.00018307431309949607\n",
      "Iteration 1905/2000, Loss: 0.00020339303591754287\n",
      "Iteration 1906/2000, Loss: 0.00011694020213326439\n",
      "Iteration 1907/2000, Loss: 0.00020609149942174554\n",
      "Iteration 1908/2000, Loss: 0.000122281358926557\n",
      "Iteration 1909/2000, Loss: 0.00026422596420161426\n",
      "Iteration 1910/2000, Loss: 0.00014169274072628468\n",
      "Iteration 1911/2000, Loss: 0.00021922268206253648\n",
      "Iteration 1912/2000, Loss: 0.0001059510832419619\n",
      "Iteration 1913/2000, Loss: 0.0001459288614569232\n",
      "Iteration 1914/2000, Loss: 0.00017463781114201993\n",
      "Iteration 1915/2000, Loss: 0.0002026052970904857\n",
      "Iteration 1916/2000, Loss: 0.0001130705131799914\n",
      "Iteration 1917/2000, Loss: 0.0001745751651469618\n",
      "Iteration 1918/2000, Loss: 0.00015843039727769792\n",
      "Iteration 1919/2000, Loss: 0.00017975190712604672\n",
      "Iteration 1920/2000, Loss: 0.00019808000070042908\n",
      "Iteration 1921/2000, Loss: 0.0002483282587490976\n",
      "Iteration 1922/2000, Loss: 0.00013422574556898326\n",
      "Iteration 1923/2000, Loss: 0.00017638153804000467\n",
      "Iteration 1924/2000, Loss: 0.00022415233252104372\n",
      "Iteration 1925/2000, Loss: 0.00017190852668136358\n",
      "Iteration 1926/2000, Loss: 0.0001372298429487273\n",
      "Iteration 1927/2000, Loss: 0.00034080559271387756\n",
      "Iteration 1928/2000, Loss: 0.00016887765377759933\n",
      "Iteration 1929/2000, Loss: 0.00021148400264792144\n",
      "Iteration 1930/2000, Loss: 0.00024075825058389455\n",
      "Iteration 1931/2000, Loss: 0.0002155113179469481\n",
      "Iteration 1932/2000, Loss: 0.00015665793034713715\n",
      "Iteration 1933/2000, Loss: 0.0001457534817745909\n",
      "Iteration 1934/2000, Loss: 0.0008031814359128475\n",
      "Iteration 1935/2000, Loss: 0.0001661797723500058\n",
      "Iteration 1936/2000, Loss: 0.00021492000087164342\n",
      "Iteration 1937/2000, Loss: 0.00025020597968250513\n",
      "Iteration 1938/2000, Loss: 0.0001600705145392567\n",
      "Iteration 1939/2000, Loss: 0.00017218700668308884\n",
      "Iteration 1940/2000, Loss: 0.00011724469368346035\n",
      "Iteration 1941/2000, Loss: 0.0001388364762533456\n",
      "Iteration 1942/2000, Loss: 0.00030346150742843747\n",
      "Iteration 1943/2000, Loss: 0.00019184591656085104\n",
      "Iteration 1944/2000, Loss: 0.00026164992596022785\n",
      "Iteration 1945/2000, Loss: 0.00012033421080559492\n",
      "Iteration 1946/2000, Loss: 0.0001488191628595814\n",
      "Iteration 1947/2000, Loss: 0.00019422218610998243\n",
      "Iteration 1948/2000, Loss: 0.0003844176244456321\n",
      "Iteration 1949/2000, Loss: 0.00023460082593373954\n",
      "Iteration 1950/2000, Loss: 0.00025825799093581736\n",
      "Iteration 1951/2000, Loss: 0.00017255966668017209\n",
      "Iteration 1952/2000, Loss: 0.0001316585548920557\n",
      "Iteration 1953/2000, Loss: 0.00018046508193947375\n",
      "Iteration 1954/2000, Loss: 0.00015811072080396116\n",
      "Iteration 1955/2000, Loss: 0.00014518322132062167\n",
      "Iteration 1956/2000, Loss: 0.00012358267849776894\n",
      "Iteration 1957/2000, Loss: 0.00015119249292183667\n",
      "Iteration 1958/2000, Loss: 0.00020311189291533083\n",
      "Iteration 1959/2000, Loss: 9.79066826403141e-05\n",
      "Iteration 1960/2000, Loss: 0.00022287521278485656\n",
      "Iteration 1961/2000, Loss: 0.000141947966767475\n",
      "Iteration 1962/2000, Loss: 0.00013301083527039737\n",
      "Iteration 1963/2000, Loss: 0.0003499722224660218\n",
      "Iteration 1964/2000, Loss: 9.972706902772188e-05\n",
      "Iteration 1965/2000, Loss: 0.00022585660917684436\n",
      "Iteration 1966/2000, Loss: 0.00013225797738414258\n",
      "Iteration 1967/2000, Loss: 0.0002646366192493588\n",
      "Iteration 1968/2000, Loss: 0.00018530961824581027\n",
      "Iteration 1969/2000, Loss: 0.000157806949573569\n",
      "Iteration 1970/2000, Loss: 0.00017515382205601782\n",
      "Iteration 1971/2000, Loss: 0.0001719182764645666\n",
      "Iteration 1972/2000, Loss: 0.0001872567954706028\n",
      "Iteration 1973/2000, Loss: 8.889833407010883e-05\n",
      "Iteration 1974/2000, Loss: 0.00017177924746647477\n",
      "Iteration 1975/2000, Loss: 0.00013916424359194934\n",
      "Iteration 1976/2000, Loss: 0.00016378707368858159\n",
      "Iteration 1977/2000, Loss: 0.0002694383729249239\n",
      "Iteration 1978/2000, Loss: 0.00016148266149684787\n",
      "Iteration 1979/2000, Loss: 0.00012436164251994342\n",
      "Iteration 1980/2000, Loss: 0.0007228440954349935\n",
      "Iteration 1981/2000, Loss: 0.00043590928544290364\n",
      "Iteration 1982/2000, Loss: 0.0003681972448248416\n",
      "Iteration 1983/2000, Loss: 0.00032332539558410645\n",
      "Iteration 1984/2000, Loss: 0.00024776143254712224\n",
      "Iteration 1985/2000, Loss: 0.000348800967913121\n",
      "Iteration 1986/2000, Loss: 0.00028397017740644515\n",
      "Iteration 1987/2000, Loss: 0.000208546596695669\n",
      "Iteration 1988/2000, Loss: 0.00028353295056149364\n",
      "Iteration 1989/2000, Loss: 0.00015127284859772772\n",
      "Iteration 1990/2000, Loss: 0.0002592593664303422\n",
      "Iteration 1991/2000, Loss: 0.0002359143691137433\n",
      "Iteration 1992/2000, Loss: 0.0001299535797443241\n",
      "Iteration 1993/2000, Loss: 0.00022016897855792195\n",
      "Iteration 1994/2000, Loss: 0.00024469042546115816\n",
      "Iteration 1995/2000, Loss: 0.0002531706995796412\n",
      "Iteration 1996/2000, Loss: 0.0003062339383177459\n",
      "Iteration 1997/2000, Loss: 0.0001856322051025927\n",
      "Iteration 1998/2000, Loss: 0.00021334149641916156\n",
      "Iteration 1999/2000, Loss: 0.0001915307220770046\n",
      "Iteration 2000/2000, Loss: 0.000318196282023564\n",
      "Model weights saved after training and testing with linewidth 100000.0 Hz.\n",
      "\n",
      "\n",
      "Testing with linewidth: 100000.0 Hz and Distance: 2000.0 km\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Testing MSE - Linewidth: 100000.0, Link Distance: 2000.0, Original: 0.005556570831686258, Neural Network: 0.009792916476726532\n",
      "\n",
      "Training with linewidth: 100000.0 Hz and Distance: 4000.0 km\n",
      "Iteration 1/2000, Loss: 0.00017586509056854993\n",
      "Iteration 2/2000, Loss: 0.00041270672227256\n",
      "Iteration 3/2000, Loss: 0.00013613268674816936\n",
      "Iteration 4/2000, Loss: 0.00019979487115051597\n",
      "Iteration 5/2000, Loss: 0.00036626405199058354\n",
      "Iteration 6/2000, Loss: 0.0003052088140975684\n",
      "Iteration 7/2000, Loss: 0.00017601827858015895\n",
      "Iteration 8/2000, Loss: 0.000251368444878608\n",
      "Iteration 9/2000, Loss: 0.00016883944044820964\n",
      "Iteration 10/2000, Loss: 0.00021387741435319185\n",
      "Iteration 11/2000, Loss: 0.00013960849901195616\n",
      "Iteration 12/2000, Loss: 0.00017860370280686766\n",
      "Iteration 13/2000, Loss: 0.00020959353423677385\n",
      "Iteration 14/2000, Loss: 0.00028853959520347416\n",
      "Iteration 15/2000, Loss: 0.0001401635236106813\n",
      "Iteration 16/2000, Loss: 0.0002899817191064358\n",
      "Iteration 17/2000, Loss: 0.0011228151852265\n",
      "Iteration 18/2000, Loss: 0.0001710640062810853\n",
      "Iteration 19/2000, Loss: 0.0003922061587218195\n",
      "Iteration 20/2000, Loss: 0.00017543332069180906\n",
      "Iteration 21/2000, Loss: 0.00024413237406406552\n",
      "Iteration 22/2000, Loss: 0.0002212402323493734\n",
      "Iteration 23/2000, Loss: 0.00016385817434638739\n",
      "Iteration 24/2000, Loss: 0.00021218550682533532\n",
      "Iteration 25/2000, Loss: 0.0003110810648649931\n",
      "Iteration 26/2000, Loss: 0.00021981510508339852\n",
      "Iteration 27/2000, Loss: 0.00019900371262338012\n",
      "Iteration 28/2000, Loss: 0.00020266089995857328\n",
      "Iteration 29/2000, Loss: 0.00018397676467429847\n",
      "Iteration 30/2000, Loss: 0.0001972577883861959\n",
      "Iteration 31/2000, Loss: 0.00015961074677761644\n",
      "Iteration 32/2000, Loss: 0.00018066706252284348\n",
      "Iteration 33/2000, Loss: 0.00015979605086613446\n",
      "Iteration 34/2000, Loss: 0.00013862490595784038\n",
      "Iteration 35/2000, Loss: 0.00025054780417121947\n",
      "Iteration 36/2000, Loss: 0.0002331368887098506\n",
      "Iteration 37/2000, Loss: 0.00012392039934638888\n",
      "Iteration 38/2000, Loss: 0.00030045342282392085\n",
      "Iteration 39/2000, Loss: 0.00011132431245641783\n",
      "Iteration 40/2000, Loss: 0.00031030771788209677\n",
      "Iteration 41/2000, Loss: 0.00031815737020224333\n",
      "Iteration 42/2000, Loss: 0.0003125996154267341\n",
      "Iteration 43/2000, Loss: 0.00030710597638972104\n",
      "Iteration 44/2000, Loss: 0.00019772125233430415\n",
      "Iteration 45/2000, Loss: 0.0006171645363792777\n",
      "Iteration 46/2000, Loss: 0.00016917812172323465\n",
      "Iteration 47/2000, Loss: 0.0002737560134846717\n",
      "Iteration 48/2000, Loss: 0.00018650396668817848\n",
      "Iteration 49/2000, Loss: 0.0002371540613239631\n",
      "Iteration 50/2000, Loss: 0.00013824655616190284\n",
      "Iteration 51/2000, Loss: 0.0002047225716523826\n",
      "Iteration 52/2000, Loss: 0.00013989914441481233\n",
      "Iteration 53/2000, Loss: 0.0002582778106443584\n",
      "Iteration 54/2000, Loss: 0.0002613336546346545\n",
      "Iteration 55/2000, Loss: 0.00014687125803902745\n",
      "Iteration 56/2000, Loss: 0.00020337272144388407\n",
      "Iteration 57/2000, Loss: 0.00013147163554094732\n",
      "Iteration 58/2000, Loss: 0.00022033540881238878\n",
      "Iteration 59/2000, Loss: 0.00029576453380286694\n",
      "Iteration 60/2000, Loss: 0.00015419823466800153\n",
      "Iteration 61/2000, Loss: 0.00013515526370611042\n",
      "Iteration 62/2000, Loss: 0.00013197796943131834\n",
      "Iteration 63/2000, Loss: 0.0001669695193413645\n",
      "Iteration 64/2000, Loss: 0.00015285330300685018\n",
      "Iteration 65/2000, Loss: 0.00011054418428102508\n",
      "Iteration 66/2000, Loss: 0.00011735090083675459\n",
      "Iteration 67/2000, Loss: 0.00019075690943282098\n",
      "Iteration 68/2000, Loss: 0.00022705362061969936\n",
      "Iteration 69/2000, Loss: 0.0003667628625407815\n",
      "Iteration 70/2000, Loss: 0.0001791672402760014\n",
      "Iteration 71/2000, Loss: 0.00020059729286003858\n",
      "Iteration 72/2000, Loss: 0.00013368633517529815\n",
      "Iteration 73/2000, Loss: 0.00013675492664333433\n",
      "Iteration 74/2000, Loss: 0.00012613045691978186\n",
      "Iteration 75/2000, Loss: 0.002271700883284211\n",
      "Iteration 76/2000, Loss: 0.0001621115516172722\n",
      "Iteration 77/2000, Loss: 0.0002430074237054214\n",
      "Iteration 78/2000, Loss: 0.00016552831220906228\n",
      "Iteration 79/2000, Loss: 0.0001818240707507357\n",
      "Iteration 80/2000, Loss: 0.00030651772976852953\n",
      "Iteration 81/2000, Loss: 0.00020968462922610343\n",
      "Iteration 82/2000, Loss: 0.00022770844225306064\n",
      "Iteration 83/2000, Loss: 0.00013926130486652255\n",
      "Iteration 84/2000, Loss: 0.0002566087932791561\n",
      "Iteration 85/2000, Loss: 0.00016335050167981535\n",
      "Iteration 86/2000, Loss: 0.00011174003884661943\n",
      "Iteration 87/2000, Loss: 0.00030767059070058167\n",
      "Iteration 88/2000, Loss: 0.00018051263759844005\n",
      "Iteration 89/2000, Loss: 9.210526332026348e-05\n",
      "Iteration 90/2000, Loss: 0.0001575176283949986\n",
      "Iteration 91/2000, Loss: 0.00015027783229015768\n",
      "Iteration 92/2000, Loss: 0.00014937810192350298\n",
      "Iteration 93/2000, Loss: 0.00013699526607524604\n",
      "Iteration 94/2000, Loss: 0.00014027616998646408\n",
      "Iteration 95/2000, Loss: 0.00020909903105348349\n",
      "Iteration 96/2000, Loss: 0.00015539518790319562\n",
      "Iteration 97/2000, Loss: 0.0004518398200161755\n",
      "Iteration 98/2000, Loss: 0.000409815926104784\n",
      "Iteration 99/2000, Loss: 0.00022194752818904817\n",
      "Iteration 100/2000, Loss: 0.00011623815225902945\n",
      "Iteration 101/2000, Loss: 0.00017662649042904377\n",
      "Iteration 102/2000, Loss: 0.0002304895460838452\n",
      "Iteration 103/2000, Loss: 9.128190868068486e-05\n",
      "Iteration 104/2000, Loss: 0.00010834975546458736\n",
      "Iteration 105/2000, Loss: 0.00018520494631957263\n",
      "Iteration 106/2000, Loss: 0.00020342512289062142\n",
      "Iteration 107/2000, Loss: 0.00022330365027301013\n",
      "Iteration 108/2000, Loss: 0.0001505458785686642\n",
      "Iteration 109/2000, Loss: 0.00014641648158431053\n",
      "Iteration 110/2000, Loss: 0.00026927661383524537\n",
      "Iteration 111/2000, Loss: 0.0002741088392212987\n",
      "Iteration 112/2000, Loss: 0.00038378144381567836\n",
      "Iteration 113/2000, Loss: 0.0002594917605165392\n",
      "Iteration 114/2000, Loss: 0.00031181241502054036\n",
      "Iteration 115/2000, Loss: 0.00024864327860996127\n",
      "Iteration 116/2000, Loss: 0.0002147004852304235\n",
      "Iteration 117/2000, Loss: 0.00016075480380095541\n",
      "Iteration 118/2000, Loss: 0.0003678332723211497\n",
      "Iteration 119/2000, Loss: 0.0004334550758358091\n",
      "Iteration 120/2000, Loss: 0.0001699405984254554\n",
      "Iteration 121/2000, Loss: 0.00027294873143546283\n",
      "Iteration 122/2000, Loss: 0.0003638131311163306\n",
      "Iteration 123/2000, Loss: 0.00045087357284501195\n",
      "Iteration 124/2000, Loss: 0.0003842699225060642\n",
      "Iteration 125/2000, Loss: 0.00020413660968188196\n",
      "Iteration 126/2000, Loss: 0.00022242283739615232\n",
      "Iteration 127/2000, Loss: 0.0002024001587415114\n",
      "Iteration 128/2000, Loss: 0.00024393534113187343\n",
      "Iteration 129/2000, Loss: 0.0001663709554122761\n",
      "Iteration 130/2000, Loss: 0.00017958217358682305\n",
      "Iteration 131/2000, Loss: 0.00019856267317663878\n",
      "Iteration 132/2000, Loss: 0.00018468401685822755\n",
      "Iteration 133/2000, Loss: 0.00047404805081896484\n",
      "Iteration 134/2000, Loss: 0.0002129082422470674\n",
      "Iteration 135/2000, Loss: 0.0002235115534858778\n",
      "Iteration 136/2000, Loss: 0.0003853148955386132\n",
      "Iteration 137/2000, Loss: 0.0002801435475703329\n",
      "Iteration 138/2000, Loss: 0.00013183044211473316\n",
      "Iteration 139/2000, Loss: 0.0003988304524682462\n",
      "Iteration 140/2000, Loss: 0.00027097505517303944\n",
      "Iteration 141/2000, Loss: 0.00015448873455170542\n",
      "Iteration 142/2000, Loss: 0.00021544418996199965\n",
      "Iteration 143/2000, Loss: 0.0001428481045877561\n",
      "Iteration 144/2000, Loss: 0.00020443728135433048\n",
      "Iteration 145/2000, Loss: 0.00014620189904235303\n",
      "Iteration 146/2000, Loss: 0.00023311488621402532\n",
      "Iteration 147/2000, Loss: 0.00010237559035886079\n",
      "Iteration 148/2000, Loss: 0.0002140553406206891\n",
      "Iteration 149/2000, Loss: 0.0001531795714981854\n",
      "Iteration 150/2000, Loss: 0.0005362111842259765\n",
      "Iteration 151/2000, Loss: 0.00017894789925776422\n",
      "Iteration 152/2000, Loss: 0.00015371403424069285\n",
      "Iteration 153/2000, Loss: 0.00015521921159233898\n",
      "Iteration 154/2000, Loss: 0.00015313277253881097\n",
      "Iteration 155/2000, Loss: 0.00018654903396964073\n",
      "Iteration 156/2000, Loss: 0.0005381821538321674\n",
      "Iteration 157/2000, Loss: 0.0002645233762450516\n",
      "Iteration 158/2000, Loss: 0.00013518579362425953\n",
      "Iteration 159/2000, Loss: 0.00010420148464618251\n",
      "Iteration 160/2000, Loss: 0.0001302534801652655\n",
      "Iteration 161/2000, Loss: 0.00017217847926076502\n",
      "Iteration 162/2000, Loss: 0.00016369928198400885\n",
      "Iteration 163/2000, Loss: 0.0002705180668272078\n",
      "Iteration 164/2000, Loss: 0.0002931257477030158\n",
      "Iteration 165/2000, Loss: 0.0001414503058185801\n",
      "Iteration 166/2000, Loss: 0.00031849934021010995\n",
      "Iteration 167/2000, Loss: 0.00010306182957720011\n",
      "Iteration 168/2000, Loss: 0.00022847505169920623\n",
      "Iteration 169/2000, Loss: 0.00014576279500033706\n",
      "Iteration 170/2000, Loss: 0.0002086344175040722\n",
      "Iteration 171/2000, Loss: 0.00013931328430771828\n",
      "Iteration 172/2000, Loss: 0.00023396816686727107\n",
      "Iteration 173/2000, Loss: 0.0002458752423990518\n",
      "Iteration 174/2000, Loss: 0.0001656683161854744\n",
      "Iteration 175/2000, Loss: 0.0005300546763464808\n",
      "Iteration 176/2000, Loss: 0.00019517814507707953\n",
      "Iteration 177/2000, Loss: 0.00039565862971358\n",
      "Iteration 178/2000, Loss: 0.00017307834059465677\n",
      "Iteration 179/2000, Loss: 0.0003581800847314298\n",
      "Iteration 180/2000, Loss: 0.00021416688105091453\n",
      "Iteration 181/2000, Loss: 0.00036458324757404625\n",
      "Iteration 182/2000, Loss: 0.000249724107561633\n",
      "Iteration 183/2000, Loss: 0.00022747914772480726\n",
      "Iteration 184/2000, Loss: 0.00015892030205577612\n",
      "Iteration 185/2000, Loss: 0.0003435857070144266\n",
      "Iteration 186/2000, Loss: 0.00015418273687828332\n",
      "Iteration 187/2000, Loss: 0.00015467972843907773\n",
      "Iteration 188/2000, Loss: 0.0002575057151261717\n",
      "Iteration 189/2000, Loss: 0.00022784479369875044\n",
      "Iteration 190/2000, Loss: 0.0001560775126563385\n",
      "Iteration 191/2000, Loss: 0.00026442314265295863\n",
      "Iteration 192/2000, Loss: 0.00020374503219500184\n",
      "Iteration 193/2000, Loss: 0.0003094511048402637\n",
      "Iteration 194/2000, Loss: 0.0001492412993684411\n",
      "Iteration 195/2000, Loss: 0.00017594563541933894\n",
      "Iteration 196/2000, Loss: 0.00029201823053881526\n",
      "Iteration 197/2000, Loss: 0.00016242358833551407\n",
      "Iteration 198/2000, Loss: 0.00015919040015432984\n",
      "Iteration 199/2000, Loss: 0.0001088541976059787\n",
      "Iteration 200/2000, Loss: 0.00040709233144298196\n",
      "Iteration 201/2000, Loss: 0.0002184806071454659\n",
      "Iteration 202/2000, Loss: 0.0002472644846420735\n",
      "Iteration 203/2000, Loss: 0.0002393026661593467\n",
      "Iteration 204/2000, Loss: 0.00022534924210049212\n",
      "Iteration 205/2000, Loss: 0.00022441172040998936\n",
      "Iteration 206/2000, Loss: 0.0002184445911552757\n",
      "Iteration 207/2000, Loss: 0.00032794466824270785\n",
      "Iteration 208/2000, Loss: 0.0001411124103469774\n",
      "Iteration 209/2000, Loss: 0.0003255871415603906\n",
      "Iteration 210/2000, Loss: 0.00012654362944886088\n",
      "Iteration 211/2000, Loss: 0.000281121872831136\n",
      "Iteration 212/2000, Loss: 0.0004726072365883738\n",
      "Iteration 213/2000, Loss: 0.0003813609655480832\n",
      "Iteration 214/2000, Loss: 0.00028049570391885936\n",
      "Iteration 215/2000, Loss: 0.00017437434871681035\n",
      "Iteration 216/2000, Loss: 0.00018684608221519738\n",
      "Iteration 217/2000, Loss: 0.00011556676327018067\n",
      "Iteration 218/2000, Loss: 0.00022385819465853274\n",
      "Iteration 219/2000, Loss: 0.00021921911684330553\n",
      "Iteration 220/2000, Loss: 0.00018366985023021698\n",
      "Iteration 221/2000, Loss: 0.00017208591452799737\n",
      "Iteration 222/2000, Loss: 0.00011528641334734857\n",
      "Iteration 223/2000, Loss: 0.00015948327200021595\n",
      "Iteration 224/2000, Loss: 0.00020244393090251833\n",
      "Iteration 225/2000, Loss: 0.00013325066538527608\n",
      "Iteration 226/2000, Loss: 0.00015004067972768098\n",
      "Iteration 227/2000, Loss: 0.0002297035971423611\n",
      "Iteration 228/2000, Loss: 0.0002106892061419785\n",
      "Iteration 229/2000, Loss: 0.0003940034075640142\n",
      "Iteration 230/2000, Loss: 0.0002352486626477912\n",
      "Iteration 231/2000, Loss: 0.0002975389070343226\n",
      "Iteration 232/2000, Loss: 0.00017450690211262554\n",
      "Iteration 233/2000, Loss: 0.0003201882936991751\n",
      "Iteration 234/2000, Loss: 0.0002686559164430946\n",
      "Iteration 235/2000, Loss: 0.0001880485942820087\n",
      "Iteration 236/2000, Loss: 0.00030128174694254994\n",
      "Iteration 237/2000, Loss: 0.0002537817053962499\n",
      "Iteration 238/2000, Loss: 0.00038575645885430276\n",
      "Iteration 239/2000, Loss: 0.00025687733432278037\n",
      "Iteration 240/2000, Loss: 0.00030074137612245977\n",
      "Iteration 241/2000, Loss: 0.0002729079860728234\n",
      "Iteration 242/2000, Loss: 0.00044225138844922185\n",
      "Iteration 243/2000, Loss: 0.0002914702636189759\n",
      "Iteration 244/2000, Loss: 0.00032004519016481936\n",
      "Iteration 245/2000, Loss: 0.00019596045603975654\n",
      "Iteration 246/2000, Loss: 0.00040903768967837095\n",
      "Iteration 247/2000, Loss: 0.0006306965951807797\n",
      "Iteration 248/2000, Loss: 0.00018611684208735824\n",
      "Iteration 249/2000, Loss: 0.00041560406680218875\n",
      "Iteration 250/2000, Loss: 0.0002964414597954601\n",
      "Iteration 251/2000, Loss: 0.000432313303463161\n",
      "Iteration 252/2000, Loss: 0.00017479434609413147\n",
      "Iteration 253/2000, Loss: 0.00017933445633389056\n",
      "Iteration 254/2000, Loss: 0.0003856789553537965\n",
      "Iteration 255/2000, Loss: 0.00017362358630634844\n",
      "Iteration 256/2000, Loss: 0.00028481543995440006\n",
      "Iteration 257/2000, Loss: 0.00015681059448979795\n",
      "Iteration 258/2000, Loss: 0.00020964245777577162\n",
      "Iteration 259/2000, Loss: 0.0001894463348435238\n",
      "Iteration 260/2000, Loss: 0.00026536022778600454\n",
      "Iteration 261/2000, Loss: 0.00032165198354050517\n",
      "Iteration 262/2000, Loss: 0.00019964059174526483\n",
      "Iteration 263/2000, Loss: 0.00016500949277542531\n",
      "Iteration 264/2000, Loss: 0.00023946142755448818\n",
      "Iteration 265/2000, Loss: 0.00019381508172955364\n",
      "Iteration 266/2000, Loss: 0.0002118130651069805\n",
      "Iteration 267/2000, Loss: 0.000151662970893085\n",
      "Iteration 268/2000, Loss: 0.0002048383466899395\n",
      "Iteration 269/2000, Loss: 0.0003920085437130183\n",
      "Iteration 270/2000, Loss: 0.00024199261679314077\n",
      "Iteration 271/2000, Loss: 0.0002031558979069814\n",
      "Iteration 272/2000, Loss: 0.00039536284748464823\n",
      "Iteration 273/2000, Loss: 0.00013473322906065732\n",
      "Iteration 274/2000, Loss: 0.00018123263726010919\n",
      "Iteration 275/2000, Loss: 0.00020465395937208086\n",
      "Iteration 276/2000, Loss: 0.0001505629188613966\n",
      "Iteration 277/2000, Loss: 0.000237662170547992\n",
      "Iteration 278/2000, Loss: 0.00015451188664883375\n",
      "Iteration 279/2000, Loss: 0.00031472588307224214\n",
      "Iteration 280/2000, Loss: 0.00018581862968858331\n",
      "Iteration 281/2000, Loss: 0.0002096330135827884\n",
      "Iteration 282/2000, Loss: 0.00024773692712187767\n",
      "Iteration 283/2000, Loss: 0.00036505606840364635\n",
      "Iteration 284/2000, Loss: 0.00028386825579218566\n",
      "Iteration 285/2000, Loss: 0.0004925894900225103\n",
      "Iteration 286/2000, Loss: 0.0005008694133721292\n",
      "Iteration 287/2000, Loss: 0.00024290666624438018\n",
      "Iteration 288/2000, Loss: 0.00017724838107824326\n",
      "Iteration 289/2000, Loss: 0.0004947470733895898\n",
      "Iteration 290/2000, Loss: 0.00016334743122570217\n",
      "Iteration 291/2000, Loss: 0.00021707928681280464\n",
      "Iteration 292/2000, Loss: 0.000166913348948583\n",
      "Iteration 293/2000, Loss: 0.00028605805709958076\n",
      "Iteration 294/2000, Loss: 0.0004466678074095398\n",
      "Iteration 295/2000, Loss: 0.00013365459744818509\n",
      "Iteration 296/2000, Loss: 0.00025199350784532726\n",
      "Iteration 297/2000, Loss: 0.0004140542878303677\n",
      "Iteration 298/2000, Loss: 0.00027652125572785735\n",
      "Iteration 299/2000, Loss: 0.00023072170733939856\n",
      "Iteration 300/2000, Loss: 0.0002533775696065277\n",
      "Iteration 301/2000, Loss: 0.00029788227402605116\n",
      "Iteration 302/2000, Loss: 0.00018719016225077212\n",
      "Iteration 303/2000, Loss: 0.00017677854339126498\n",
      "Iteration 304/2000, Loss: 0.00018979553715325892\n",
      "Iteration 305/2000, Loss: 0.0004081862571183592\n",
      "Iteration 306/2000, Loss: 0.00016274731024168432\n",
      "Iteration 307/2000, Loss: 0.0002604392357170582\n",
      "Iteration 308/2000, Loss: 0.00021909520728513598\n",
      "Iteration 309/2000, Loss: 0.0004423662612680346\n",
      "Iteration 310/2000, Loss: 0.00025813959655351937\n",
      "Iteration 311/2000, Loss: 0.0001858123578131199\n",
      "Iteration 312/2000, Loss: 0.0003017190610989928\n",
      "Iteration 313/2000, Loss: 0.0003279375669080764\n",
      "Iteration 314/2000, Loss: 0.00026483257533982396\n",
      "Iteration 315/2000, Loss: 0.0002088461333187297\n",
      "Iteration 316/2000, Loss: 0.00017815818137023598\n",
      "Iteration 317/2000, Loss: 0.0003524719795677811\n",
      "Iteration 318/2000, Loss: 0.0003330275940243155\n",
      "Iteration 319/2000, Loss: 0.00023559517285320908\n",
      "Iteration 320/2000, Loss: 0.0001567518775118515\n",
      "Iteration 321/2000, Loss: 0.00031768903136253357\n",
      "Iteration 322/2000, Loss: 0.00021175184519961476\n",
      "Iteration 323/2000, Loss: 0.00013506534742191434\n",
      "Iteration 324/2000, Loss: 0.0001337144203716889\n",
      "Iteration 325/2000, Loss: 0.00014512331108562648\n",
      "Iteration 326/2000, Loss: 0.00020146563474554569\n",
      "Iteration 327/2000, Loss: 0.00011469237506389618\n",
      "Iteration 328/2000, Loss: 0.00021085338084958494\n",
      "Iteration 329/2000, Loss: 0.00015259653446264565\n",
      "Iteration 330/2000, Loss: 0.00014292557898443192\n",
      "Iteration 331/2000, Loss: 0.0006882183370180428\n",
      "Iteration 332/2000, Loss: 0.0002190729574067518\n",
      "Iteration 333/2000, Loss: 0.00032349242246709764\n",
      "Iteration 334/2000, Loss: 0.00017823997768573463\n",
      "Iteration 335/2000, Loss: 0.0002405789855401963\n",
      "Iteration 336/2000, Loss: 0.0001966841664398089\n",
      "Iteration 337/2000, Loss: 0.0002160237345378846\n",
      "Iteration 338/2000, Loss: 0.00012007181794615462\n",
      "Iteration 339/2000, Loss: 0.00014191980881150812\n",
      "Iteration 340/2000, Loss: 0.00018713077588472515\n",
      "Iteration 341/2000, Loss: 0.0001243082369910553\n",
      "Iteration 342/2000, Loss: 0.00016244003199972212\n",
      "Iteration 343/2000, Loss: 0.00012350358883850276\n",
      "Iteration 344/2000, Loss: 0.0001589906169101596\n",
      "Iteration 345/2000, Loss: 0.00016386261268053204\n",
      "Iteration 346/2000, Loss: 0.00014685468340758234\n",
      "Iteration 347/2000, Loss: 0.00014652546087745577\n",
      "Iteration 348/2000, Loss: 0.00017884142289403826\n",
      "Iteration 349/2000, Loss: 0.00016921729547902942\n",
      "Iteration 350/2000, Loss: 0.0003264651168137789\n",
      "Iteration 351/2000, Loss: 0.00023315777070820332\n",
      "Iteration 352/2000, Loss: 0.00022859258751850575\n",
      "Iteration 353/2000, Loss: 0.00022945473028812557\n",
      "Iteration 354/2000, Loss: 0.00018360080139245838\n",
      "Iteration 355/2000, Loss: 0.00025254336651414633\n",
      "Iteration 356/2000, Loss: 0.00016253298963420093\n",
      "Iteration 357/2000, Loss: 0.00021136396389920264\n",
      "Iteration 358/2000, Loss: 0.00020376848988234997\n",
      "Iteration 359/2000, Loss: 0.00018971758254338056\n",
      "Iteration 360/2000, Loss: 0.00020183443848509341\n",
      "Iteration 361/2000, Loss: 0.00017788469267543405\n",
      "Iteration 362/2000, Loss: 0.0002278068132000044\n",
      "Iteration 363/2000, Loss: 0.00017724698409438133\n",
      "Iteration 364/2000, Loss: 0.00015712520689703524\n",
      "Iteration 365/2000, Loss: 0.00019704896840266883\n",
      "Iteration 366/2000, Loss: 0.00021133905102033168\n",
      "Iteration 367/2000, Loss: 0.00017495470819994807\n",
      "Iteration 368/2000, Loss: 0.00018642716167960316\n",
      "Iteration 369/2000, Loss: 0.0001234477967955172\n",
      "Iteration 370/2000, Loss: 0.0001368115481454879\n",
      "Iteration 371/2000, Loss: 0.00014486613508779556\n",
      "Iteration 372/2000, Loss: 0.00020582771685440093\n",
      "Iteration 373/2000, Loss: 0.00016236644296441227\n",
      "Iteration 374/2000, Loss: 0.00012978944869246334\n",
      "Iteration 375/2000, Loss: 0.0001760032173478976\n",
      "Iteration 376/2000, Loss: 0.00021434058726299554\n",
      "Iteration 377/2000, Loss: 0.00026736961444839835\n",
      "Iteration 378/2000, Loss: 0.00013778882566839457\n",
      "Iteration 379/2000, Loss: 0.0001531142188468948\n",
      "Iteration 380/2000, Loss: 0.00013410560495685786\n",
      "Iteration 381/2000, Loss: 0.0001588694576639682\n",
      "Iteration 382/2000, Loss: 0.0002217355213360861\n",
      "Iteration 383/2000, Loss: 0.00011716670996975154\n",
      "Iteration 384/2000, Loss: 0.00015894063108135015\n",
      "Iteration 385/2000, Loss: 0.00011792267468990758\n",
      "Iteration 386/2000, Loss: 0.00012700047227554023\n",
      "Iteration 387/2000, Loss: 0.00014655150880571455\n",
      "Iteration 388/2000, Loss: 0.0001260423450730741\n",
      "Iteration 389/2000, Loss: 0.00013136955385562032\n",
      "Iteration 390/2000, Loss: 0.00016399323067162186\n",
      "Iteration 391/2000, Loss: 0.00013607752043753862\n",
      "Iteration 392/2000, Loss: 0.00025684910360723734\n",
      "Iteration 393/2000, Loss: 0.0002701414341572672\n",
      "Iteration 394/2000, Loss: 0.0001831884728744626\n",
      "Iteration 395/2000, Loss: 0.00017693651898298413\n",
      "Iteration 396/2000, Loss: 0.00011172420636285096\n",
      "Iteration 397/2000, Loss: 0.00029249678482301533\n",
      "Iteration 398/2000, Loss: 0.00012792495544999838\n",
      "Iteration 399/2000, Loss: 0.00023436307674273849\n",
      "Iteration 400/2000, Loss: 0.00021214099251665175\n",
      "Iteration 401/2000, Loss: 0.0001831418339861557\n",
      "Iteration 402/2000, Loss: 0.0002139505377272144\n",
      "Iteration 403/2000, Loss: 0.00028248061425983906\n",
      "Iteration 404/2000, Loss: 0.000213988809264265\n",
      "Iteration 405/2000, Loss: 0.0003280133823864162\n",
      "Iteration 406/2000, Loss: 0.00013185603893361986\n",
      "Iteration 407/2000, Loss: 0.0002395139163127169\n",
      "Iteration 408/2000, Loss: 0.00015107316721696407\n",
      "Iteration 409/2000, Loss: 0.0013676543021574616\n",
      "Iteration 410/2000, Loss: 0.0002317672478966415\n",
      "Iteration 411/2000, Loss: 0.00023525791766587645\n",
      "Iteration 412/2000, Loss: 0.00018260334036312997\n",
      "Iteration 413/2000, Loss: 0.00024693712475709617\n",
      "Iteration 414/2000, Loss: 0.0003286907740402967\n",
      "Iteration 415/2000, Loss: 0.00023145446903072298\n",
      "Iteration 416/2000, Loss: 0.00016097859770525247\n",
      "Iteration 417/2000, Loss: 0.0001523577666375786\n",
      "Iteration 418/2000, Loss: 0.00021475358516909182\n",
      "Iteration 419/2000, Loss: 0.00014671342796646059\n",
      "Iteration 420/2000, Loss: 0.00016265100566670299\n",
      "Iteration 421/2000, Loss: 0.0001791834074538201\n",
      "Iteration 422/2000, Loss: 0.00014453232870437205\n",
      "Iteration 423/2000, Loss: 0.00016096910985652357\n",
      "Iteration 424/2000, Loss: 0.0001407255622325465\n",
      "Iteration 425/2000, Loss: 0.00022832956165075302\n",
      "Iteration 426/2000, Loss: 0.00012287158460821956\n",
      "Iteration 427/2000, Loss: 0.0003064998018089682\n",
      "Iteration 428/2000, Loss: 0.0003588126564864069\n",
      "Iteration 429/2000, Loss: 0.00022029902902431786\n",
      "Iteration 430/2000, Loss: 0.0003477100981399417\n",
      "Iteration 431/2000, Loss: 0.00011299947800580412\n",
      "Iteration 432/2000, Loss: 0.000273132580332458\n",
      "Iteration 433/2000, Loss: 0.0002282450150232762\n",
      "Iteration 434/2000, Loss: 0.00020082104310858995\n",
      "Iteration 435/2000, Loss: 0.0002463877317495644\n",
      "Iteration 436/2000, Loss: 0.00016631532344035804\n",
      "Iteration 437/2000, Loss: 0.00029580245609395206\n",
      "Iteration 438/2000, Loss: 0.00024453378864564\n",
      "Iteration 439/2000, Loss: 0.00022196522331796587\n",
      "Iteration 440/2000, Loss: 0.00020264447084628046\n",
      "Iteration 441/2000, Loss: 0.00023909476294647902\n",
      "Iteration 442/2000, Loss: 0.0003545971412677318\n",
      "Iteration 443/2000, Loss: 0.00021929202193859965\n",
      "Iteration 444/2000, Loss: 0.0001320651062997058\n",
      "Iteration 445/2000, Loss: 0.00038144949940033257\n",
      "Iteration 446/2000, Loss: 0.00020100941765122116\n",
      "Iteration 447/2000, Loss: 0.00018592612468637526\n",
      "Iteration 448/2000, Loss: 0.00013766334450338036\n",
      "Iteration 449/2000, Loss: 0.00023021640663500875\n",
      "Iteration 450/2000, Loss: 0.0003702815738506615\n",
      "Iteration 451/2000, Loss: 0.00023376147146336734\n",
      "Iteration 452/2000, Loss: 0.00014436518540605903\n",
      "Iteration 453/2000, Loss: 0.0002523764269426465\n",
      "Iteration 454/2000, Loss: 0.00016880426846910268\n",
      "Iteration 455/2000, Loss: 0.0003062374598812312\n",
      "Iteration 456/2000, Loss: 0.0003126685624010861\n",
      "Iteration 457/2000, Loss: 0.00016889872495085\n",
      "Iteration 458/2000, Loss: 0.0005369068239815533\n",
      "Iteration 459/2000, Loss: 0.000377212738385424\n",
      "Iteration 460/2000, Loss: 0.00022262430866248906\n",
      "Iteration 461/2000, Loss: 0.0003283482510596514\n",
      "Iteration 462/2000, Loss: 0.0001788350782589987\n",
      "Iteration 463/2000, Loss: 0.00035578684764914215\n",
      "Iteration 464/2000, Loss: 0.00019765015167649835\n",
      "Iteration 465/2000, Loss: 0.0004749994841404259\n",
      "Iteration 466/2000, Loss: 0.00032886708504520357\n",
      "Iteration 467/2000, Loss: 0.0002578389539849013\n",
      "Iteration 468/2000, Loss: 0.000421729899244383\n",
      "Iteration 469/2000, Loss: 0.0004199636459816247\n",
      "Iteration 470/2000, Loss: 0.0003549323882907629\n",
      "Iteration 471/2000, Loss: 0.00021770599414594471\n",
      "Iteration 472/2000, Loss: 0.0003980727633461356\n",
      "Iteration 473/2000, Loss: 0.00027067531482316554\n",
      "Iteration 474/2000, Loss: 0.0003030965744983405\n",
      "Iteration 475/2000, Loss: 0.00019602724933065474\n",
      "Iteration 476/2000, Loss: 0.0004501707444433123\n",
      "Iteration 477/2000, Loss: 0.00021316159109119326\n",
      "Iteration 478/2000, Loss: 0.0003828599874395877\n",
      "Iteration 479/2000, Loss: 0.0005215562996454537\n",
      "Iteration 480/2000, Loss: 0.00033681324566714466\n",
      "Iteration 481/2000, Loss: 0.000500835885759443\n",
      "Iteration 482/2000, Loss: 0.00022292339417617768\n",
      "Iteration 483/2000, Loss: 0.0004050101269967854\n",
      "Iteration 484/2000, Loss: 0.0001490423019276932\n",
      "Iteration 485/2000, Loss: 0.0004991363384760916\n",
      "Iteration 486/2000, Loss: 0.00028432675753720105\n",
      "Iteration 487/2000, Loss: 0.0005521923885680735\n",
      "Iteration 488/2000, Loss: 0.0003320863179396838\n",
      "Iteration 489/2000, Loss: 0.00026738783344626427\n",
      "Iteration 490/2000, Loss: 0.00028211751487106085\n",
      "Iteration 491/2000, Loss: 0.00022151143639348447\n",
      "Iteration 492/2000, Loss: 0.00022839405573904514\n",
      "Iteration 493/2000, Loss: 0.0002846142160706222\n",
      "Iteration 494/2000, Loss: 0.0003384326118975878\n",
      "Iteration 495/2000, Loss: 0.00022538834309671074\n",
      "Iteration 496/2000, Loss: 0.00013216095976531506\n",
      "Iteration 497/2000, Loss: 0.00023064517881721258\n",
      "Iteration 498/2000, Loss: 0.0004014335572719574\n",
      "Iteration 499/2000, Loss: 0.0004907700349576771\n",
      "Iteration 500/2000, Loss: 0.00019009625248145312\n",
      "Iteration 501/2000, Loss: 0.000287303002551198\n",
      "Iteration 502/2000, Loss: 0.00020814084564335644\n",
      "Iteration 503/2000, Loss: 0.00014704839850310236\n",
      "Iteration 504/2000, Loss: 0.0001968468859558925\n",
      "Iteration 505/2000, Loss: 0.00044260939466767013\n",
      "Iteration 506/2000, Loss: 0.0002481144038029015\n",
      "Iteration 507/2000, Loss: 0.0003054261615034193\n",
      "Iteration 508/2000, Loss: 0.00017205187759827822\n",
      "Iteration 509/2000, Loss: 0.0002295327140018344\n",
      "Iteration 510/2000, Loss: 0.00017791998106986284\n",
      "Iteration 511/2000, Loss: 0.00023106718435883522\n",
      "Iteration 512/2000, Loss: 0.00017457905050832778\n",
      "Iteration 513/2000, Loss: 0.00017933209892362356\n",
      "Iteration 514/2000, Loss: 0.0002546873001847416\n",
      "Iteration 515/2000, Loss: 0.0002244257921120152\n",
      "Iteration 516/2000, Loss: 0.0007048757979646325\n",
      "Iteration 517/2000, Loss: 0.00018070951045956463\n",
      "Iteration 518/2000, Loss: 0.0001610105246072635\n",
      "Iteration 519/2000, Loss: 0.0002903788990806788\n",
      "Iteration 520/2000, Loss: 0.0004091584123671055\n",
      "Iteration 521/2000, Loss: 0.0002764657838270068\n",
      "Iteration 522/2000, Loss: 0.00020390204736031592\n",
      "Iteration 523/2000, Loss: 0.00017928842862602323\n",
      "Iteration 524/2000, Loss: 0.00024272916198242456\n",
      "Iteration 525/2000, Loss: 0.00021639029728248715\n",
      "Iteration 526/2000, Loss: 0.00019550083379726857\n",
      "Iteration 527/2000, Loss: 0.00016294830129481852\n",
      "Iteration 528/2000, Loss: 0.00015083431208040565\n",
      "Iteration 529/2000, Loss: 0.00023696341668255627\n",
      "Iteration 530/2000, Loss: 0.00014925395953468978\n",
      "Iteration 531/2000, Loss: 0.00021974186529405415\n",
      "Iteration 532/2000, Loss: 0.00012385542504489422\n",
      "Iteration 533/2000, Loss: 0.00013467074313666672\n",
      "Iteration 534/2000, Loss: 0.00011691217514453456\n",
      "Iteration 535/2000, Loss: 0.0003517353325150907\n",
      "Iteration 536/2000, Loss: 0.00015717095811851323\n",
      "Iteration 537/2000, Loss: 0.0003178620245307684\n",
      "Iteration 538/2000, Loss: 0.0001365490024909377\n",
      "Iteration 539/2000, Loss: 0.00011399165668990463\n",
      "Iteration 540/2000, Loss: 0.00012109558883821592\n",
      "Iteration 541/2000, Loss: 0.00014469795860350132\n",
      "Iteration 542/2000, Loss: 0.00015247747069224715\n",
      "Iteration 543/2000, Loss: 0.00015688491112086922\n",
      "Iteration 544/2000, Loss: 0.0002086400199914351\n",
      "Iteration 545/2000, Loss: 0.0008158592390827835\n",
      "Iteration 546/2000, Loss: 0.00024275649047922343\n",
      "Iteration 547/2000, Loss: 0.00020398516790010035\n",
      "Iteration 548/2000, Loss: 0.000357935787178576\n",
      "Iteration 549/2000, Loss: 0.00015061642625369132\n",
      "Iteration 550/2000, Loss: 0.00025661682593636215\n",
      "Iteration 551/2000, Loss: 0.00016497094475198537\n",
      "Iteration 552/2000, Loss: 0.0002330234565306455\n",
      "Iteration 553/2000, Loss: 0.0003491131938062608\n",
      "Iteration 554/2000, Loss: 0.00011799210187746212\n",
      "Iteration 555/2000, Loss: 0.00019045421504415572\n",
      "Iteration 556/2000, Loss: 0.00012292082828935236\n",
      "Iteration 557/2000, Loss: 0.00023611071810591966\n",
      "Iteration 558/2000, Loss: 0.00042860169196501374\n",
      "Iteration 559/2000, Loss: 0.0002171085070585832\n",
      "Iteration 560/2000, Loss: 0.00031132527510635555\n",
      "Iteration 561/2000, Loss: 0.00020955967193003744\n",
      "Iteration 562/2000, Loss: 0.00017698315787129104\n",
      "Iteration 563/2000, Loss: 0.00013982805830892175\n",
      "Iteration 564/2000, Loss: 0.0001411915582139045\n",
      "Iteration 565/2000, Loss: 0.00015184161020442843\n",
      "Iteration 566/2000, Loss: 0.00017596788529772311\n",
      "Iteration 567/2000, Loss: 0.0001720365253277123\n",
      "Iteration 568/2000, Loss: 0.00021571260003838688\n",
      "Iteration 569/2000, Loss: 0.0002815886400640011\n",
      "Iteration 570/2000, Loss: 0.0001563690893817693\n",
      "Iteration 571/2000, Loss: 0.00011273643758613616\n",
      "Iteration 572/2000, Loss: 0.0003198424819856882\n",
      "Iteration 573/2000, Loss: 0.0002479683025740087\n",
      "Iteration 574/2000, Loss: 0.00015057816926855594\n",
      "Iteration 575/2000, Loss: 0.00016586708079557866\n",
      "Iteration 576/2000, Loss: 0.0001800489699235186\n",
      "Iteration 577/2000, Loss: 0.00012535687710624188\n",
      "Iteration 578/2000, Loss: 0.00012590181722771376\n",
      "Iteration 579/2000, Loss: 0.00015041534788906574\n",
      "Iteration 580/2000, Loss: 0.00016041460912674665\n",
      "Iteration 581/2000, Loss: 0.0001375098217977211\n",
      "Iteration 582/2000, Loss: 0.00014772052236367017\n",
      "Iteration 583/2000, Loss: 0.0002084077859763056\n",
      "Iteration 584/2000, Loss: 0.00015044500469230115\n",
      "Iteration 585/2000, Loss: 0.0002082362916553393\n",
      "Iteration 586/2000, Loss: 0.0002448067534714937\n",
      "Iteration 587/2000, Loss: 0.00019152273307554424\n",
      "Iteration 588/2000, Loss: 0.00041745786438696086\n",
      "Iteration 589/2000, Loss: 9.72924244706519e-05\n",
      "Iteration 590/2000, Loss: 0.00016433604469057173\n",
      "Iteration 591/2000, Loss: 0.00021614744036924094\n",
      "Iteration 592/2000, Loss: 0.00024628540268167853\n",
      "Iteration 593/2000, Loss: 0.0001328604412265122\n",
      "Iteration 594/2000, Loss: 0.00014768767869099975\n",
      "Iteration 595/2000, Loss: 0.00022362313757184893\n",
      "Iteration 596/2000, Loss: 0.00012388463073875755\n",
      "Iteration 597/2000, Loss: 0.00022162444656714797\n",
      "Iteration 598/2000, Loss: 0.00018611850100569427\n",
      "Iteration 599/2000, Loss: 0.0001735619007376954\n",
      "Iteration 600/2000, Loss: 0.00021590296819340438\n",
      "Iteration 601/2000, Loss: 0.0002100559213431552\n",
      "Iteration 602/2000, Loss: 0.00014961292617954314\n",
      "Iteration 603/2000, Loss: 0.00011539029947016388\n",
      "Iteration 604/2000, Loss: 0.00010723882587626576\n",
      "Iteration 605/2000, Loss: 0.0003881539741996676\n",
      "Iteration 606/2000, Loss: 0.00021436381211970001\n",
      "Iteration 607/2000, Loss: 0.00011495382932480425\n",
      "Iteration 608/2000, Loss: 0.0001368459197692573\n",
      "Iteration 609/2000, Loss: 0.00018546781211625785\n",
      "Iteration 610/2000, Loss: 0.0001587805600138381\n",
      "Iteration 611/2000, Loss: 0.0001674599334364757\n",
      "Iteration 612/2000, Loss: 0.00018998770974576473\n",
      "Iteration 613/2000, Loss: 0.00014913614722900093\n",
      "Iteration 614/2000, Loss: 0.00018939068831969053\n",
      "Iteration 615/2000, Loss: 0.00023240347218234092\n",
      "Iteration 616/2000, Loss: 0.00016707269242033362\n",
      "Iteration 617/2000, Loss: 0.00012340264220256358\n",
      "Iteration 618/2000, Loss: 0.00020327229867689312\n",
      "Iteration 619/2000, Loss: 0.00015736340719740838\n",
      "Iteration 620/2000, Loss: 0.00028751016361638904\n",
      "Iteration 621/2000, Loss: 0.00022397968859877437\n",
      "Iteration 622/2000, Loss: 0.00018196577730122954\n",
      "Iteration 623/2000, Loss: 0.00011662442557280883\n",
      "Iteration 624/2000, Loss: 0.0001882314682006836\n",
      "Iteration 625/2000, Loss: 8.971829083748162e-05\n",
      "Iteration 626/2000, Loss: 0.00033746447297744453\n",
      "Iteration 627/2000, Loss: 0.00019233571947552264\n",
      "Iteration 628/2000, Loss: 0.00024020095588639379\n",
      "Iteration 629/2000, Loss: 0.00010608575394144282\n",
      "Iteration 630/2000, Loss: 0.00023827685799915344\n",
      "Iteration 631/2000, Loss: 0.00023690324451308697\n",
      "Iteration 632/2000, Loss: 0.0003892772947438061\n",
      "Iteration 633/2000, Loss: 0.00024098205904010683\n",
      "Iteration 634/2000, Loss: 0.00024161746841855347\n",
      "Iteration 635/2000, Loss: 0.0002302915818290785\n",
      "Iteration 636/2000, Loss: 0.0003326816949993372\n",
      "Iteration 637/2000, Loss: 0.00022173681645654142\n",
      "Iteration 638/2000, Loss: 0.0001773130352376029\n",
      "Iteration 639/2000, Loss: 0.00016991027223411947\n",
      "Iteration 640/2000, Loss: 0.00011176037514815107\n",
      "Iteration 641/2000, Loss: 0.00018564870697446167\n",
      "Iteration 642/2000, Loss: 0.00024273771850857884\n",
      "Iteration 643/2000, Loss: 0.00019098116899840534\n",
      "Iteration 644/2000, Loss: 0.00026978415553458035\n",
      "Iteration 645/2000, Loss: 0.00019423382764216512\n",
      "Iteration 646/2000, Loss: 0.00028422923060134053\n",
      "Iteration 647/2000, Loss: 0.00020844397658947855\n",
      "Iteration 648/2000, Loss: 0.0003515480493661016\n",
      "Iteration 649/2000, Loss: 0.0003047817153856158\n",
      "Iteration 650/2000, Loss: 0.0003882316523231566\n",
      "Iteration 651/2000, Loss: 0.0002893520286306739\n",
      "Iteration 652/2000, Loss: 0.00023601904103998095\n",
      "Iteration 653/2000, Loss: 0.0005123232258483768\n",
      "Iteration 654/2000, Loss: 0.00017024898261297494\n",
      "Iteration 655/2000, Loss: 0.00017102666606660932\n",
      "Iteration 656/2000, Loss: 0.0002870741009246558\n",
      "Iteration 657/2000, Loss: 0.00023746804799884558\n",
      "Iteration 658/2000, Loss: 0.0001707141927909106\n",
      "Iteration 659/2000, Loss: 0.00018491341324988753\n",
      "Iteration 660/2000, Loss: 0.00017559320258442312\n",
      "Iteration 661/2000, Loss: 0.00021626224042847753\n",
      "Iteration 662/2000, Loss: 0.00019264013099018484\n",
      "Iteration 663/2000, Loss: 0.0003967212687712163\n",
      "Iteration 664/2000, Loss: 0.00032768797245807946\n",
      "Iteration 665/2000, Loss: 0.0002576335391495377\n",
      "Iteration 666/2000, Loss: 0.00028289001784287393\n",
      "Iteration 667/2000, Loss: 0.00025299322442151606\n",
      "Iteration 668/2000, Loss: 0.0002767637197393924\n",
      "Iteration 669/2000, Loss: 0.0004983192193321884\n",
      "Iteration 670/2000, Loss: 0.00028470627148635685\n",
      "Iteration 671/2000, Loss: 0.0007451584679074585\n",
      "Iteration 672/2000, Loss: 0.00045599660370498896\n",
      "Iteration 673/2000, Loss: 0.00025539909256622195\n",
      "Iteration 674/2000, Loss: 0.00042227134690620005\n",
      "Iteration 675/2000, Loss: 0.0009971769759431481\n",
      "Iteration 676/2000, Loss: 0.00033775638439692557\n",
      "Iteration 677/2000, Loss: 0.00028915723669342697\n",
      "Iteration 678/2000, Loss: 0.00031019854941405356\n",
      "Iteration 679/2000, Loss: 0.0004919660277664661\n",
      "Iteration 680/2000, Loss: 0.00025952441501431167\n",
      "Iteration 681/2000, Loss: 0.00028038962045684457\n",
      "Iteration 682/2000, Loss: 0.00032962049590423703\n",
      "Iteration 683/2000, Loss: 0.00030774102197028697\n",
      "Iteration 684/2000, Loss: 0.00037655996857210994\n",
      "Iteration 685/2000, Loss: 0.0002579614520072937\n",
      "Iteration 686/2000, Loss: 0.0003378485271241516\n",
      "Iteration 687/2000, Loss: 0.00042057590326294303\n",
      "Iteration 688/2000, Loss: 0.00033758956124074757\n",
      "Iteration 689/2000, Loss: 0.00032918588840402663\n",
      "Iteration 690/2000, Loss: 0.00031694164499640465\n",
      "Iteration 691/2000, Loss: 0.00023585045710206032\n",
      "Iteration 692/2000, Loss: 0.0002674060233402997\n",
      "Iteration 693/2000, Loss: 0.0003040010342374444\n",
      "Iteration 694/2000, Loss: 0.00037762054125778377\n",
      "Iteration 695/2000, Loss: 0.00032860151259228587\n",
      "Iteration 696/2000, Loss: 0.00021182422642596066\n",
      "Iteration 697/2000, Loss: 0.00034212545142509043\n",
      "Iteration 698/2000, Loss: 0.00027772277826443315\n",
      "Iteration 699/2000, Loss: 0.0002642122271936387\n",
      "Iteration 700/2000, Loss: 0.0003022630698978901\n",
      "Iteration 701/2000, Loss: 0.0001785520726116374\n",
      "Iteration 702/2000, Loss: 0.000317777186864987\n",
      "Iteration 703/2000, Loss: 0.00028537653270177543\n",
      "Iteration 704/2000, Loss: 0.00026779636391438544\n",
      "Iteration 705/2000, Loss: 0.00025554915191605687\n",
      "Iteration 706/2000, Loss: 0.00017292069969698787\n",
      "Iteration 707/2000, Loss: 0.00026256131241098046\n",
      "Iteration 708/2000, Loss: 0.00034851316013373435\n",
      "Iteration 709/2000, Loss: 0.00029269070364534855\n",
      "Iteration 710/2000, Loss: 0.00019426364451646805\n",
      "Iteration 711/2000, Loss: 0.0003942378971260041\n",
      "Iteration 712/2000, Loss: 0.00026674376567825675\n",
      "Iteration 713/2000, Loss: 0.00019614440680015832\n",
      "Iteration 714/2000, Loss: 0.00035488983849063516\n",
      "Iteration 715/2000, Loss: 0.00024106154160108417\n",
      "Iteration 716/2000, Loss: 0.0003332741616759449\n",
      "Iteration 717/2000, Loss: 0.00019986201368737966\n",
      "Iteration 718/2000, Loss: 0.00021370088506955653\n",
      "Iteration 719/2000, Loss: 0.00016669259639456868\n",
      "Iteration 720/2000, Loss: 0.00019598939979914576\n",
      "Iteration 721/2000, Loss: 0.00022493358119390905\n",
      "Iteration 722/2000, Loss: 0.00020366706303320825\n",
      "Iteration 723/2000, Loss: 0.0003179834457114339\n",
      "Iteration 724/2000, Loss: 0.00046171018038876355\n",
      "Iteration 725/2000, Loss: 0.00025568652199581265\n",
      "Iteration 726/2000, Loss: 0.0002039967803284526\n",
      "Iteration 727/2000, Loss: 0.0002460608375258744\n",
      "Iteration 728/2000, Loss: 0.0002571907243691385\n",
      "Iteration 729/2000, Loss: 0.00013728403428103775\n",
      "Iteration 730/2000, Loss: 0.00024173560086637735\n",
      "Iteration 731/2000, Loss: 0.0002554097445681691\n",
      "Iteration 732/2000, Loss: 0.0006324347923509777\n",
      "Iteration 733/2000, Loss: 0.0001925329415826127\n",
      "Iteration 734/2000, Loss: 0.0002512882638256997\n",
      "Iteration 735/2000, Loss: 0.00018652243306860328\n",
      "Iteration 736/2000, Loss: 0.00020208234491292387\n",
      "Iteration 737/2000, Loss: 0.00022739551786798984\n",
      "Iteration 738/2000, Loss: 0.000546443450730294\n",
      "Iteration 739/2000, Loss: 0.00036927193286828697\n",
      "Iteration 740/2000, Loss: 0.00015475170221179724\n",
      "Iteration 741/2000, Loss: 0.00037834938848391175\n",
      "Iteration 742/2000, Loss: 0.00029672859818674624\n",
      "Iteration 743/2000, Loss: 0.00027699314523488283\n",
      "Iteration 744/2000, Loss: 0.00017742751515470445\n",
      "Iteration 745/2000, Loss: 0.0002040864055743441\n",
      "Iteration 746/2000, Loss: 0.000245626550167799\n",
      "Iteration 747/2000, Loss: 0.00029282752075232565\n",
      "Iteration 748/2000, Loss: 0.00021935085533186793\n",
      "Iteration 749/2000, Loss: 0.0001612848718650639\n",
      "Iteration 750/2000, Loss: 0.0002408002910669893\n",
      "Iteration 751/2000, Loss: 0.0002120328863384202\n",
      "Iteration 752/2000, Loss: 0.00014402669330593199\n",
      "Iteration 753/2000, Loss: 0.0001871878484962508\n",
      "Iteration 754/2000, Loss: 0.00015658748452551663\n",
      "Iteration 755/2000, Loss: 0.0002689050743356347\n",
      "Iteration 756/2000, Loss: 0.00036912859650328755\n",
      "Iteration 757/2000, Loss: 0.0004779322480317205\n",
      "Iteration 758/2000, Loss: 0.0002671918773557991\n",
      "Iteration 759/2000, Loss: 0.00019544393580872566\n",
      "Iteration 760/2000, Loss: 0.00013084281818009913\n",
      "Iteration 761/2000, Loss: 0.0002455331850796938\n",
      "Iteration 762/2000, Loss: 0.00017005363770294935\n",
      "Iteration 763/2000, Loss: 0.0006215907051227987\n",
      "Iteration 764/2000, Loss: 0.0007946855621412396\n",
      "Iteration 765/2000, Loss: 0.00014796738105360419\n",
      "Iteration 766/2000, Loss: 0.00028472839039750397\n",
      "Iteration 767/2000, Loss: 0.00014967576134949923\n",
      "Iteration 768/2000, Loss: 0.00022347827325575054\n",
      "Iteration 769/2000, Loss: 0.00018878035189118236\n",
      "Iteration 770/2000, Loss: 0.00032355295843444765\n",
      "Iteration 771/2000, Loss: 0.00035376171581447124\n",
      "Iteration 772/2000, Loss: 0.00014135298260953277\n",
      "Iteration 773/2000, Loss: 0.00024455785751342773\n",
      "Iteration 774/2000, Loss: 0.0002827599528245628\n",
      "Iteration 775/2000, Loss: 0.00015481006994377822\n",
      "Iteration 776/2000, Loss: 0.0003433467645663768\n",
      "Iteration 777/2000, Loss: 0.00029535870999097824\n",
      "Iteration 778/2000, Loss: 0.00022513284056913108\n",
      "Iteration 779/2000, Loss: 0.0002763101947493851\n",
      "Iteration 780/2000, Loss: 0.0002742542128544301\n",
      "Iteration 781/2000, Loss: 0.00034602871164679527\n",
      "Iteration 782/2000, Loss: 0.0002150168875232339\n",
      "Iteration 783/2000, Loss: 0.00011847438872791827\n",
      "Iteration 784/2000, Loss: 0.000699845957569778\n",
      "Iteration 785/2000, Loss: 0.00013054478040430695\n",
      "Iteration 786/2000, Loss: 0.00015650001296307892\n",
      "Iteration 787/2000, Loss: 0.0002863290428649634\n",
      "Iteration 788/2000, Loss: 0.00029102107509970665\n",
      "Iteration 789/2000, Loss: 0.00027141408645547926\n",
      "Iteration 790/2000, Loss: 0.0002311954740434885\n",
      "Iteration 791/2000, Loss: 0.00020137877436354756\n",
      "Iteration 792/2000, Loss: 0.0001818435121094808\n",
      "Iteration 793/2000, Loss: 0.0004956966731697321\n",
      "Iteration 794/2000, Loss: 0.000310829869704321\n",
      "Iteration 795/2000, Loss: 0.00044553918996825814\n",
      "Iteration 796/2000, Loss: 0.00030779463122598827\n",
      "Iteration 797/2000, Loss: 0.00012901861919090152\n",
      "Iteration 798/2000, Loss: 0.00026527902809903026\n",
      "Iteration 799/2000, Loss: 0.00019867927767336369\n",
      "Iteration 800/2000, Loss: 0.00021624841610901058\n",
      "Iteration 801/2000, Loss: 0.00018483379972167313\n",
      "Iteration 802/2000, Loss: 0.0001606063306098804\n",
      "Iteration 803/2000, Loss: 0.0002459138340782374\n",
      "Iteration 804/2000, Loss: 0.00015438876289408654\n",
      "Iteration 805/2000, Loss: 0.00020936052897013724\n",
      "Iteration 806/2000, Loss: 0.00012557096488308161\n",
      "Iteration 807/2000, Loss: 0.0003070164821110666\n",
      "Iteration 808/2000, Loss: 0.0002595784608274698\n",
      "Iteration 809/2000, Loss: 0.00017668813234195113\n",
      "Iteration 810/2000, Loss: 0.0001547050487715751\n",
      "Iteration 811/2000, Loss: 0.00021985912462696433\n",
      "Iteration 812/2000, Loss: 0.00012467925262171775\n",
      "Iteration 813/2000, Loss: 0.00011438630463089794\n",
      "Iteration 814/2000, Loss: 0.00017883372493088245\n",
      "Iteration 815/2000, Loss: 0.00022903323406353593\n",
      "Iteration 816/2000, Loss: 0.00014685776841361076\n",
      "Iteration 817/2000, Loss: 0.0004963134415447712\n",
      "Iteration 818/2000, Loss: 0.00012787112791556865\n",
      "Iteration 819/2000, Loss: 0.0002171590313082561\n",
      "Iteration 820/2000, Loss: 0.00017511635087430477\n",
      "Iteration 821/2000, Loss: 0.0001221843995153904\n",
      "Iteration 822/2000, Loss: 0.00019802374299615622\n",
      "Iteration 823/2000, Loss: 0.0002107954496750608\n",
      "Iteration 824/2000, Loss: 0.00017663228209130466\n",
      "Iteration 825/2000, Loss: 0.00016132723249029368\n",
      "Iteration 826/2000, Loss: 0.00018258213822264224\n",
      "Iteration 827/2000, Loss: 0.000249409640673548\n",
      "Iteration 828/2000, Loss: 0.0002373024180997163\n",
      "Iteration 829/2000, Loss: 0.0002533042570576072\n",
      "Iteration 830/2000, Loss: 0.00010647208546288311\n",
      "Iteration 831/2000, Loss: 0.00026339010219089687\n",
      "Iteration 832/2000, Loss: 0.00014775189629290253\n",
      "Iteration 833/2000, Loss: 0.00030612031696364284\n",
      "Iteration 834/2000, Loss: 0.00014614628162235022\n",
      "Iteration 835/2000, Loss: 0.00016729530761949718\n",
      "Iteration 836/2000, Loss: 0.00015905067266430706\n",
      "Iteration 837/2000, Loss: 0.00011269044625805691\n",
      "Iteration 838/2000, Loss: 0.00013805007620248944\n",
      "Iteration 839/2000, Loss: 0.00027096885605715215\n",
      "Iteration 840/2000, Loss: 0.0002500964910723269\n",
      "Iteration 841/2000, Loss: 0.00015109003288671374\n",
      "Iteration 842/2000, Loss: 0.0002171941741835326\n",
      "Iteration 843/2000, Loss: 7.831925177015364e-05\n",
      "Iteration 844/2000, Loss: 0.00021708414715249091\n",
      "Iteration 845/2000, Loss: 0.0002083962899632752\n",
      "Iteration 846/2000, Loss: 0.00032610647031106055\n",
      "Iteration 847/2000, Loss: 0.00013750424841418862\n",
      "Iteration 848/2000, Loss: 0.0001764355693012476\n",
      "Iteration 849/2000, Loss: 0.00016996514750644565\n",
      "Iteration 850/2000, Loss: 0.0001860078191384673\n",
      "Iteration 851/2000, Loss: 0.00023802704527042806\n",
      "Iteration 852/2000, Loss: 0.00030409370083361864\n",
      "Iteration 853/2000, Loss: 0.00016332537052221596\n",
      "Iteration 854/2000, Loss: 0.00014378788182511926\n",
      "Iteration 855/2000, Loss: 0.0002404186234343797\n",
      "Iteration 856/2000, Loss: 0.00023657090787310153\n",
      "Iteration 857/2000, Loss: 0.00016718092956580222\n",
      "Iteration 858/2000, Loss: 0.00017249654047191143\n",
      "Iteration 859/2000, Loss: 0.0003151308046653867\n",
      "Iteration 860/2000, Loss: 0.00014032561739441007\n",
      "Iteration 861/2000, Loss: 0.000170151237398386\n",
      "Iteration 862/2000, Loss: 0.00035513826878741384\n",
      "Iteration 863/2000, Loss: 0.00012981047620996833\n",
      "Iteration 864/2000, Loss: 0.00020729913376271725\n",
      "Iteration 865/2000, Loss: 0.000179647613549605\n",
      "Iteration 866/2000, Loss: 0.0006443245219998062\n",
      "Iteration 867/2000, Loss: 0.00037368282210081816\n",
      "Iteration 868/2000, Loss: 0.00021558895241469145\n",
      "Iteration 869/2000, Loss: 0.00024058212875388563\n",
      "Iteration 870/2000, Loss: 0.00021952604583930224\n",
      "Iteration 871/2000, Loss: 0.0002644999185577035\n",
      "Iteration 872/2000, Loss: 0.00022171878663357347\n",
      "Iteration 873/2000, Loss: 0.00014424217806663364\n",
      "Iteration 874/2000, Loss: 0.00024401281552854925\n",
      "Iteration 875/2000, Loss: 0.00020255417621228844\n",
      "Iteration 876/2000, Loss: 0.00025924303918145597\n",
      "Iteration 877/2000, Loss: 0.00016061734640970826\n",
      "Iteration 878/2000, Loss: 0.00015020911814644933\n",
      "Iteration 879/2000, Loss: 0.0001268836931558326\n",
      "Iteration 880/2000, Loss: 0.0002120887947967276\n",
      "Iteration 881/2000, Loss: 0.00011088528844993562\n",
      "Iteration 882/2000, Loss: 0.000258028187090531\n",
      "Iteration 883/2000, Loss: 0.00015974453708622605\n",
      "Iteration 884/2000, Loss: 0.00016631989274173975\n",
      "Iteration 885/2000, Loss: 0.00034015823621302843\n",
      "Iteration 886/2000, Loss: 0.00026143176364712417\n",
      "Iteration 887/2000, Loss: 0.0004076144250575453\n",
      "Iteration 888/2000, Loss: 0.00024503079475834966\n",
      "Iteration 889/2000, Loss: 0.00015098768926691264\n",
      "Iteration 890/2000, Loss: 0.00021539078443311155\n",
      "Iteration 891/2000, Loss: 0.0005074831424281001\n",
      "Iteration 892/2000, Loss: 0.00015344985877163708\n",
      "Iteration 893/2000, Loss: 0.00034699245588853955\n",
      "Iteration 894/2000, Loss: 0.00026952975895255804\n",
      "Iteration 895/2000, Loss: 0.0001411791890859604\n",
      "Iteration 896/2000, Loss: 0.00021212057617958635\n",
      "Iteration 897/2000, Loss: 0.00040505299693904817\n",
      "Iteration 898/2000, Loss: 0.00030605250503867865\n",
      "Iteration 899/2000, Loss: 0.0003562411875464022\n",
      "Iteration 900/2000, Loss: 0.0002577549603302032\n",
      "Iteration 901/2000, Loss: 0.0005115493549965322\n",
      "Iteration 902/2000, Loss: 0.00027517290436662734\n",
      "Iteration 903/2000, Loss: 0.00022581317170988768\n",
      "Iteration 904/2000, Loss: 0.0003033260873053223\n",
      "Iteration 905/2000, Loss: 0.0003984314971603453\n",
      "Iteration 906/2000, Loss: 0.00018944032490253448\n",
      "Iteration 907/2000, Loss: 0.00033888110192492604\n",
      "Iteration 908/2000, Loss: 0.0006883386522531509\n",
      "Iteration 909/2000, Loss: 0.00042819156078621745\n",
      "Iteration 910/2000, Loss: 0.0005540457786992192\n",
      "Iteration 911/2000, Loss: 0.0009241533116437495\n",
      "Iteration 912/2000, Loss: 0.0005272981361486018\n",
      "Iteration 913/2000, Loss: 0.00023750837135594338\n",
      "Iteration 914/2000, Loss: 0.00031261853291653097\n",
      "Iteration 915/2000, Loss: 0.00029182276921346784\n",
      "Iteration 916/2000, Loss: 0.0003012397501152009\n",
      "Iteration 917/2000, Loss: 0.00045722807408310473\n",
      "Iteration 918/2000, Loss: 0.00015744967095088214\n",
      "Iteration 919/2000, Loss: 0.00037240624078549445\n",
      "Iteration 920/2000, Loss: 0.00020154152298346162\n",
      "Iteration 921/2000, Loss: 0.00029119354439899325\n",
      "Iteration 922/2000, Loss: 0.00032524947891943157\n",
      "Iteration 923/2000, Loss: 0.0003049489459954202\n",
      "Iteration 924/2000, Loss: 0.0004591262841131538\n",
      "Iteration 925/2000, Loss: 0.0002430378954159096\n",
      "Iteration 926/2000, Loss: 0.00030529184732586145\n",
      "Iteration 927/2000, Loss: 0.00018681985966395587\n",
      "Iteration 928/2000, Loss: 0.0004122011305298656\n",
      "Iteration 929/2000, Loss: 0.00022389412333723158\n",
      "Iteration 930/2000, Loss: 0.0003742267726920545\n",
      "Iteration 931/2000, Loss: 0.0003456690174061805\n",
      "Iteration 932/2000, Loss: 0.00044003830407746136\n",
      "Iteration 933/2000, Loss: 0.0005506877205334604\n",
      "Iteration 934/2000, Loss: 0.0002098699624184519\n",
      "Iteration 935/2000, Loss: 0.0005073356442153454\n",
      "Iteration 936/2000, Loss: 0.00016348333156201988\n",
      "Iteration 937/2000, Loss: 0.0005839726654812694\n",
      "Iteration 938/2000, Loss: 0.00016872616834007204\n",
      "Iteration 939/2000, Loss: 0.0004000882036052644\n",
      "Iteration 940/2000, Loss: 0.00018072937382385135\n",
      "Iteration 941/2000, Loss: 0.00033923066803254187\n",
      "Iteration 942/2000, Loss: 0.0002658293233253062\n",
      "Iteration 943/2000, Loss: 0.0002700079348869622\n",
      "Iteration 944/2000, Loss: 0.00032938882941380143\n",
      "Iteration 945/2000, Loss: 0.0002083096478600055\n",
      "Iteration 946/2000, Loss: 0.0010147561552003026\n",
      "Iteration 947/2000, Loss: 0.0003154129663016647\n",
      "Iteration 948/2000, Loss: 0.0003282605903223157\n",
      "Iteration 949/2000, Loss: 0.0002689444809220731\n",
      "Iteration 950/2000, Loss: 0.0002739657647907734\n",
      "Iteration 951/2000, Loss: 0.0003529325476847589\n",
      "Iteration 952/2000, Loss: 0.00045454074279405177\n",
      "Iteration 953/2000, Loss: 0.00015836476814001799\n",
      "Iteration 954/2000, Loss: 0.00033858438837341964\n",
      "Iteration 955/2000, Loss: 0.00021304783876985312\n",
      "Iteration 956/2000, Loss: 0.0004421518824528903\n",
      "Iteration 957/2000, Loss: 0.0004708876658696681\n",
      "Iteration 958/2000, Loss: 0.0009324477869085968\n",
      "Iteration 959/2000, Loss: 0.00045846099965274334\n",
      "Iteration 960/2000, Loss: 0.0008428494329564273\n",
      "Iteration 961/2000, Loss: 0.00037580347270704806\n",
      "Iteration 962/2000, Loss: 0.0005866044666618109\n",
      "Iteration 963/2000, Loss: 0.00021595168800558895\n",
      "Iteration 964/2000, Loss: 0.0008425224223174155\n",
      "Iteration 965/2000, Loss: 0.00034529613913036883\n",
      "Iteration 966/2000, Loss: 0.0005327809485606849\n",
      "Iteration 967/2000, Loss: 0.0004478361224755645\n",
      "Iteration 968/2000, Loss: 0.0005038298550061882\n",
      "Iteration 969/2000, Loss: 0.0004173964844085276\n",
      "Iteration 970/2000, Loss: 0.0002943631261587143\n",
      "Iteration 971/2000, Loss: 0.00018446738249622285\n",
      "Iteration 972/2000, Loss: 0.0005440833629108965\n",
      "Iteration 973/2000, Loss: 0.00038439498166553676\n",
      "Iteration 974/2000, Loss: 0.0002025002468144521\n",
      "Iteration 975/2000, Loss: 0.000373234914150089\n",
      "Iteration 976/2000, Loss: 0.00012460668222047389\n",
      "Iteration 977/2000, Loss: 0.0005239446763880551\n",
      "Iteration 978/2000, Loss: 0.00018654516316018999\n",
      "Iteration 979/2000, Loss: 0.00031474357820115983\n",
      "Iteration 980/2000, Loss: 0.00017218675930052996\n",
      "Iteration 981/2000, Loss: 0.0006263732211664319\n",
      "Iteration 982/2000, Loss: 0.00032067904248833656\n",
      "Iteration 983/2000, Loss: 0.00048217049334198236\n",
      "Iteration 984/2000, Loss: 0.00040574188460595906\n",
      "Iteration 985/2000, Loss: 0.00040874205296859145\n",
      "Iteration 986/2000, Loss: 0.0002262079215142876\n",
      "Iteration 987/2000, Loss: 0.0002456215734127909\n",
      "Iteration 988/2000, Loss: 0.00023059311206452549\n",
      "Iteration 989/2000, Loss: 0.00033632502891123295\n",
      "Iteration 990/2000, Loss: 0.00020059834059793502\n",
      "Iteration 991/2000, Loss: 0.0003285307320766151\n",
      "Iteration 992/2000, Loss: 0.0001178311140392907\n",
      "Iteration 993/2000, Loss: 0.00038075761403888464\n",
      "Iteration 994/2000, Loss: 0.0001275061658816412\n",
      "Iteration 995/2000, Loss: 0.0001760874001774937\n",
      "Iteration 996/2000, Loss: 0.00031897652661427855\n",
      "Iteration 997/2000, Loss: 0.0003174351586494595\n",
      "Iteration 998/2000, Loss: 0.0003096739237662405\n",
      "Iteration 999/2000, Loss: 0.00019664704450406134\n",
      "Iteration 1000/2000, Loss: 0.0002204196498496458\n",
      "Iteration 1001/2000, Loss: 0.00019703980069607496\n",
      "Iteration 1002/2000, Loss: 0.0001563571859151125\n",
      "Iteration 1003/2000, Loss: 0.00021787238074466586\n",
      "Iteration 1004/2000, Loss: 0.0002493103966116905\n",
      "Iteration 1005/2000, Loss: 0.00016485381638631225\n",
      "Iteration 1006/2000, Loss: 0.00020862810197286308\n",
      "Iteration 1007/2000, Loss: 0.00013666808081325144\n",
      "Iteration 1008/2000, Loss: 0.0005136671825312078\n",
      "Iteration 1009/2000, Loss: 0.00018221867503598332\n",
      "Iteration 1010/2000, Loss: 0.00035853133886121213\n",
      "Iteration 1011/2000, Loss: 0.00016668016905896366\n",
      "Iteration 1012/2000, Loss: 0.00014330660633277148\n",
      "Iteration 1013/2000, Loss: 0.00023553943901788443\n",
      "Iteration 1014/2000, Loss: 0.0005728817195631564\n",
      "Iteration 1015/2000, Loss: 0.00033002905547618866\n",
      "Iteration 1016/2000, Loss: 0.0003779469116125256\n",
      "Iteration 1017/2000, Loss: 0.0004933715681545436\n",
      "Iteration 1018/2000, Loss: 0.00016318392590619624\n",
      "Iteration 1019/2000, Loss: 0.0003300632815808058\n",
      "Iteration 1020/2000, Loss: 0.00015057952259667218\n",
      "Iteration 1021/2000, Loss: 0.0003099391469731927\n",
      "Iteration 1022/2000, Loss: 0.0001601280237082392\n",
      "Iteration 1023/2000, Loss: 0.0003751862677745521\n",
      "Iteration 1024/2000, Loss: 0.0003402339352760464\n",
      "Iteration 1025/2000, Loss: 0.00043550299596972764\n",
      "Iteration 1026/2000, Loss: 0.000248967029619962\n",
      "Iteration 1027/2000, Loss: 0.0004898218903690577\n",
      "Iteration 1028/2000, Loss: 0.00015524367336183786\n",
      "Iteration 1029/2000, Loss: 0.0005766224930994213\n",
      "Iteration 1030/2000, Loss: 0.00042994527029804885\n",
      "Iteration 1031/2000, Loss: 0.00022913220163900405\n",
      "Iteration 1032/2000, Loss: 0.00037118297768756747\n",
      "Iteration 1033/2000, Loss: 0.00033691717544570565\n",
      "Iteration 1034/2000, Loss: 0.0003316579677630216\n",
      "Iteration 1035/2000, Loss: 0.00038298178697004914\n",
      "Iteration 1036/2000, Loss: 0.00028780262800864875\n",
      "Iteration 1037/2000, Loss: 0.0003268409345764667\n",
      "Iteration 1038/2000, Loss: 0.00024127149663399905\n",
      "Iteration 1039/2000, Loss: 0.0003100627800449729\n",
      "Iteration 1040/2000, Loss: 0.0003116401785518974\n",
      "Iteration 1041/2000, Loss: 0.0005899486714042723\n",
      "Iteration 1042/2000, Loss: 0.0002628896327223629\n",
      "Iteration 1043/2000, Loss: 0.00012166949454694986\n",
      "Iteration 1044/2000, Loss: 0.0002677967422641814\n",
      "Iteration 1045/2000, Loss: 0.0004492833686526865\n",
      "Iteration 1046/2000, Loss: 0.00042333369492553174\n",
      "Iteration 1047/2000, Loss: 0.00014980649575591087\n",
      "Iteration 1048/2000, Loss: 0.0002106020983774215\n",
      "Iteration 1049/2000, Loss: 0.00030763784889131784\n",
      "Iteration 1050/2000, Loss: 0.00023627941845916212\n",
      "Iteration 1051/2000, Loss: 0.0001511020673206076\n",
      "Iteration 1052/2000, Loss: 0.00017381017096340656\n",
      "Iteration 1053/2000, Loss: 0.0003498560399748385\n",
      "Iteration 1054/2000, Loss: 0.00016989595314953476\n",
      "Iteration 1055/2000, Loss: 0.0003459349973127246\n",
      "Iteration 1056/2000, Loss: 0.00026773245190270245\n",
      "Iteration 1057/2000, Loss: 0.0003559535834938288\n",
      "Iteration 1058/2000, Loss: 0.00025260329130105674\n",
      "Iteration 1059/2000, Loss: 0.00021150076645426452\n",
      "Iteration 1060/2000, Loss: 0.0002575426478870213\n",
      "Iteration 1061/2000, Loss: 0.0001921437942655757\n",
      "Iteration 1062/2000, Loss: 0.00024824810680001974\n",
      "Iteration 1063/2000, Loss: 0.0002756166795734316\n",
      "Iteration 1064/2000, Loss: 0.00018187148089054972\n",
      "Iteration 1065/2000, Loss: 0.00015944422921165824\n",
      "Iteration 1066/2000, Loss: 0.00023425584367942065\n",
      "Iteration 1067/2000, Loss: 0.0002649957314133644\n",
      "Iteration 1068/2000, Loss: 0.00018904438184108585\n",
      "Iteration 1069/2000, Loss: 0.0002938327379524708\n",
      "Iteration 1070/2000, Loss: 0.00022725223971065134\n",
      "Iteration 1071/2000, Loss: 0.00029902567621320486\n",
      "Iteration 1072/2000, Loss: 0.00020620242867153138\n",
      "Iteration 1073/2000, Loss: 0.0002482945565134287\n",
      "Iteration 1074/2000, Loss: 0.00015286124835256487\n",
      "Iteration 1075/2000, Loss: 0.00029334911960177124\n",
      "Iteration 1076/2000, Loss: 0.00023074411728885025\n",
      "Iteration 1077/2000, Loss: 0.0004293950041756034\n",
      "Iteration 1078/2000, Loss: 0.0002356339682592079\n",
      "Iteration 1079/2000, Loss: 0.0003700445231515914\n",
      "Iteration 1080/2000, Loss: 0.0004352069809101522\n",
      "Iteration 1081/2000, Loss: 0.0002391605667071417\n",
      "Iteration 1082/2000, Loss: 0.00016141806554514915\n",
      "Iteration 1083/2000, Loss: 0.0005085036973468959\n",
      "Iteration 1084/2000, Loss: 0.00017145441961474717\n",
      "Iteration 1085/2000, Loss: 0.00021293472673278302\n",
      "Iteration 1086/2000, Loss: 0.0001649689656915143\n",
      "Iteration 1087/2000, Loss: 0.0002888721937779337\n",
      "Iteration 1088/2000, Loss: 0.00020768394460901618\n",
      "Iteration 1089/2000, Loss: 0.0001915706234285608\n",
      "Iteration 1090/2000, Loss: 0.0002074321819236502\n",
      "Iteration 1091/2000, Loss: 0.0002594778488855809\n",
      "Iteration 1092/2000, Loss: 0.00018021494906861335\n",
      "Iteration 1093/2000, Loss: 0.0002934920194093138\n",
      "Iteration 1094/2000, Loss: 0.0002768097911030054\n",
      "Iteration 1095/2000, Loss: 0.0001430872653145343\n",
      "Iteration 1096/2000, Loss: 0.00013186359137762338\n",
      "Iteration 1097/2000, Loss: 0.00014380423817783594\n",
      "Iteration 1098/2000, Loss: 0.00013860384933650494\n",
      "Iteration 1099/2000, Loss: 0.00020169041818007827\n",
      "Iteration 1100/2000, Loss: 0.00012444051390048116\n",
      "Iteration 1101/2000, Loss: 0.00018938157882075757\n",
      "Iteration 1102/2000, Loss: 0.00024325096455868334\n",
      "Iteration 1103/2000, Loss: 0.0003212552110198885\n",
      "Iteration 1104/2000, Loss: 0.00018829757755156606\n",
      "Iteration 1105/2000, Loss: 0.00011055400682380423\n",
      "Iteration 1106/2000, Loss: 0.00019670063920784742\n",
      "Iteration 1107/2000, Loss: 0.00034750672057271004\n",
      "Iteration 1108/2000, Loss: 0.00016342733579222113\n",
      "Iteration 1109/2000, Loss: 0.000240828754613176\n",
      "Iteration 1110/2000, Loss: 0.00019489640544634312\n",
      "Iteration 1111/2000, Loss: 0.00030394011992029846\n",
      "Iteration 1112/2000, Loss: 0.00017835658218245953\n",
      "Iteration 1113/2000, Loss: 0.00021362557890824974\n",
      "Iteration 1114/2000, Loss: 0.00034321806742809713\n",
      "Iteration 1115/2000, Loss: 0.00019627147412393242\n",
      "Iteration 1116/2000, Loss: 0.00022141961380839348\n",
      "Iteration 1117/2000, Loss: 0.00021640100749209523\n",
      "Iteration 1118/2000, Loss: 0.0001914902386488393\n",
      "Iteration 1119/2000, Loss: 0.00024415311054326594\n",
      "Iteration 1120/2000, Loss: 0.00013094666064716876\n",
      "Iteration 1121/2000, Loss: 0.000394668139051646\n",
      "Iteration 1122/2000, Loss: 0.0001285807666135952\n",
      "Iteration 1123/2000, Loss: 0.00022180356609169394\n",
      "Iteration 1124/2000, Loss: 0.00025894425925798714\n",
      "Iteration 1125/2000, Loss: 0.00014735096192453057\n",
      "Iteration 1126/2000, Loss: 0.00023908348521217704\n",
      "Iteration 1127/2000, Loss: 0.0005824337713420391\n",
      "Iteration 1128/2000, Loss: 0.0002377066557528451\n",
      "Iteration 1129/2000, Loss: 0.00013990142906550318\n",
      "Iteration 1130/2000, Loss: 0.0002456628135405481\n",
      "Iteration 1131/2000, Loss: 0.0002992555673699826\n",
      "Iteration 1132/2000, Loss: 0.00016830721870064735\n",
      "Iteration 1133/2000, Loss: 0.00013652174675371498\n",
      "Iteration 1134/2000, Loss: 0.00046562281204387546\n",
      "Iteration 1135/2000, Loss: 0.0001791825343389064\n",
      "Iteration 1136/2000, Loss: 0.00022290115884970874\n",
      "Iteration 1137/2000, Loss: 0.00028911660774610937\n",
      "Iteration 1138/2000, Loss: 0.000144480902235955\n",
      "Iteration 1139/2000, Loss: 0.00019507955585140735\n",
      "Iteration 1140/2000, Loss: 0.00021034128440078348\n",
      "Iteration 1141/2000, Loss: 0.00016271676577161998\n",
      "Iteration 1142/2000, Loss: 0.0001773563417373225\n",
      "Iteration 1143/2000, Loss: 0.00020014810434076935\n",
      "Iteration 1144/2000, Loss: 0.00029246328631415963\n",
      "Iteration 1145/2000, Loss: 0.00017411324370186776\n",
      "Iteration 1146/2000, Loss: 0.00015703878307249397\n",
      "Iteration 1147/2000, Loss: 0.00017747905803844333\n",
      "Iteration 1148/2000, Loss: 0.00019980769138783216\n",
      "Iteration 1149/2000, Loss: 0.0001640688715269789\n",
      "Iteration 1150/2000, Loss: 0.00013012390991207212\n",
      "Iteration 1151/2000, Loss: 0.00013052007125224918\n",
      "Iteration 1152/2000, Loss: 0.00017693966219667345\n",
      "Iteration 1153/2000, Loss: 0.00019031522970180959\n",
      "Iteration 1154/2000, Loss: 0.0002174015244236216\n",
      "Iteration 1155/2000, Loss: 0.00017475149070378393\n",
      "Iteration 1156/2000, Loss: 0.00039743856177665293\n",
      "Iteration 1157/2000, Loss: 0.00028847833164036274\n",
      "Iteration 1158/2000, Loss: 0.00022332658409141004\n",
      "Iteration 1159/2000, Loss: 0.00019366684136912227\n",
      "Iteration 1160/2000, Loss: 0.00015897424600552768\n",
      "Iteration 1161/2000, Loss: 0.0002127258339896798\n",
      "Iteration 1162/2000, Loss: 0.00017407031555194408\n",
      "Iteration 1163/2000, Loss: 0.0001995069469558075\n",
      "Iteration 1164/2000, Loss: 0.00025245570577681065\n",
      "Iteration 1165/2000, Loss: 0.00014678282605018467\n",
      "Iteration 1166/2000, Loss: 0.00022681173868477345\n",
      "Iteration 1167/2000, Loss: 0.0001971907477127388\n",
      "Iteration 1168/2000, Loss: 0.00016643099661450833\n",
      "Iteration 1169/2000, Loss: 0.00022187914873939008\n",
      "Iteration 1170/2000, Loss: 0.00018166597874369472\n",
      "Iteration 1171/2000, Loss: 9.767789742909372e-05\n",
      "Iteration 1172/2000, Loss: 0.0001932462037075311\n",
      "Iteration 1173/2000, Loss: 0.00014712216216139495\n",
      "Iteration 1174/2000, Loss: 0.0001837786694522947\n",
      "Iteration 1175/2000, Loss: 0.00017262683832086623\n",
      "Iteration 1176/2000, Loss: 0.00013861837214790285\n",
      "Iteration 1177/2000, Loss: 9.713051258586347e-05\n",
      "Iteration 1178/2000, Loss: 0.0005779756465926766\n",
      "Iteration 1179/2000, Loss: 0.00018127012299373746\n",
      "Iteration 1180/2000, Loss: 0.00028133962769061327\n",
      "Iteration 1181/2000, Loss: 0.000179625756572932\n",
      "Iteration 1182/2000, Loss: 0.00018402271962258965\n",
      "Iteration 1183/2000, Loss: 0.0002419342054054141\n",
      "Iteration 1184/2000, Loss: 0.00013399509771261364\n",
      "Iteration 1185/2000, Loss: 0.00033528427593410015\n",
      "Iteration 1186/2000, Loss: 0.0002497272507753223\n",
      "Iteration 1187/2000, Loss: 0.0002718337927944958\n",
      "Iteration 1188/2000, Loss: 0.0002846956776920706\n",
      "Iteration 1189/2000, Loss: 0.00014910548634361476\n",
      "Iteration 1190/2000, Loss: 0.00023481242533307523\n",
      "Iteration 1191/2000, Loss: 0.00021646999812219292\n",
      "Iteration 1192/2000, Loss: 0.00031259210663847625\n",
      "Iteration 1193/2000, Loss: 0.0002591437951195985\n",
      "Iteration 1194/2000, Loss: 0.0001512832095613703\n",
      "Iteration 1195/2000, Loss: 0.0002145372418453917\n",
      "Iteration 1196/2000, Loss: 0.0001789406087482348\n",
      "Iteration 1197/2000, Loss: 0.00017007286078296602\n",
      "Iteration 1198/2000, Loss: 0.0001413476566085592\n",
      "Iteration 1199/2000, Loss: 0.00023225379118230194\n",
      "Iteration 1200/2000, Loss: 0.00040370674105361104\n",
      "Iteration 1201/2000, Loss: 0.0004002284549642354\n",
      "Iteration 1202/2000, Loss: 0.0002907153684645891\n",
      "Iteration 1203/2000, Loss: 0.00034915056312456727\n",
      "Iteration 1204/2000, Loss: 0.0011099026305601\n",
      "Iteration 1205/2000, Loss: 0.00027396660880185664\n",
      "Iteration 1206/2000, Loss: 0.0005125688621774316\n",
      "Iteration 1207/2000, Loss: 0.00031202365062199533\n",
      "Iteration 1208/2000, Loss: 0.0004185943107586354\n",
      "Iteration 1209/2000, Loss: 0.000657048833090812\n",
      "Iteration 1210/2000, Loss: 0.0003016388218384236\n",
      "Iteration 1211/2000, Loss: 0.0005181230953894556\n",
      "Iteration 1212/2000, Loss: 0.00029887427808716893\n",
      "Iteration 1213/2000, Loss: 0.00024621657212264836\n",
      "Iteration 1214/2000, Loss: 0.0002578628482297063\n",
      "Iteration 1215/2000, Loss: 0.0006879777065478265\n",
      "Iteration 1216/2000, Loss: 0.0003141865599900484\n",
      "Iteration 1217/2000, Loss: 0.0002431312605040148\n",
      "Iteration 1218/2000, Loss: 0.00029229477513581514\n",
      "Iteration 1219/2000, Loss: 0.0002924665459431708\n",
      "Iteration 1220/2000, Loss: 0.00020016021153423935\n",
      "Iteration 1221/2000, Loss: 0.0005770404823124409\n",
      "Iteration 1222/2000, Loss: 0.00015704205725342035\n",
      "Iteration 1223/2000, Loss: 0.00043725629802793264\n",
      "Iteration 1224/2000, Loss: 0.00015820171392988414\n",
      "Iteration 1225/2000, Loss: 0.00026436688494868577\n",
      "Iteration 1226/2000, Loss: 0.0002948767214547843\n",
      "Iteration 1227/2000, Loss: 0.0003142734058201313\n",
      "Iteration 1228/2000, Loss: 0.0005014360067434609\n",
      "Iteration 1229/2000, Loss: 0.00019629744929261506\n",
      "Iteration 1230/2000, Loss: 0.00020345325174275786\n",
      "Iteration 1231/2000, Loss: 0.0002802204980980605\n",
      "Iteration 1232/2000, Loss: 0.00020033359760418534\n",
      "Iteration 1233/2000, Loss: 0.00018749826995190233\n",
      "Iteration 1234/2000, Loss: 0.00018119544256478548\n",
      "Iteration 1235/2000, Loss: 0.00024194006982725114\n",
      "Iteration 1236/2000, Loss: 0.00020429189316928387\n",
      "Iteration 1237/2000, Loss: 0.00024054294044617563\n",
      "Iteration 1238/2000, Loss: 0.00015438349510077387\n",
      "Iteration 1239/2000, Loss: 0.00019747766782529652\n",
      "Iteration 1240/2000, Loss: 0.00015625276137143373\n",
      "Iteration 1241/2000, Loss: 0.00041763464105315506\n",
      "Iteration 1242/2000, Loss: 0.00021152765839360654\n",
      "Iteration 1243/2000, Loss: 0.00024845637381076813\n",
      "Iteration 1244/2000, Loss: 0.00016470556147396564\n",
      "Iteration 1245/2000, Loss: 0.00013495050370693207\n",
      "Iteration 1246/2000, Loss: 0.00023382148356176913\n",
      "Iteration 1247/2000, Loss: 0.00019169390725437552\n",
      "Iteration 1248/2000, Loss: 0.0001848780084401369\n",
      "Iteration 1249/2000, Loss: 0.00020271181710995734\n",
      "Iteration 1250/2000, Loss: 0.00013104638492222875\n",
      "Iteration 1251/2000, Loss: 0.0002359043137403205\n",
      "Iteration 1252/2000, Loss: 0.00016607459110673517\n",
      "Iteration 1253/2000, Loss: 0.00022945609816815704\n",
      "Iteration 1254/2000, Loss: 0.00023001267982181162\n",
      "Iteration 1255/2000, Loss: 0.0006342839915305376\n",
      "Iteration 1256/2000, Loss: 0.0001746002526488155\n",
      "Iteration 1257/2000, Loss: 0.0007796918507665396\n",
      "Iteration 1258/2000, Loss: 0.0002152224478777498\n",
      "Iteration 1259/2000, Loss: 0.00024729190045036376\n",
      "Iteration 1260/2000, Loss: 0.00016795232659205794\n",
      "Iteration 1261/2000, Loss: 0.00028195796767249703\n",
      "Iteration 1262/2000, Loss: 0.00022101611830294132\n",
      "Iteration 1263/2000, Loss: 0.00030607610824517906\n",
      "Iteration 1264/2000, Loss: 0.0001723382738418877\n",
      "Iteration 1265/2000, Loss: 0.0003747402224689722\n",
      "Iteration 1266/2000, Loss: 0.00017216624110005796\n",
      "Iteration 1267/2000, Loss: 0.000253745325608179\n",
      "Iteration 1268/2000, Loss: 0.0004388405941426754\n",
      "Iteration 1269/2000, Loss: 0.00025622404064051807\n",
      "Iteration 1270/2000, Loss: 0.00038426212267950177\n",
      "Iteration 1271/2000, Loss: 0.00020929415768478066\n",
      "Iteration 1272/2000, Loss: 0.0002049718314083293\n",
      "Iteration 1273/2000, Loss: 0.0002521085843909532\n",
      "Iteration 1274/2000, Loss: 0.00019197545771021396\n",
      "Iteration 1275/2000, Loss: 0.0002507217868696898\n",
      "Iteration 1276/2000, Loss: 0.0002481735427863896\n",
      "Iteration 1277/2000, Loss: 0.0002708249376155436\n",
      "Iteration 1278/2000, Loss: 0.00015174283180385828\n",
      "Iteration 1279/2000, Loss: 0.0001695035316515714\n",
      "Iteration 1280/2000, Loss: 0.00017588275659363717\n",
      "Iteration 1281/2000, Loss: 0.00017600407591089606\n",
      "Iteration 1282/2000, Loss: 0.00023090084141585976\n",
      "Iteration 1283/2000, Loss: 0.00016885617515072227\n",
      "Iteration 1284/2000, Loss: 0.0002509420446585864\n",
      "Iteration 1285/2000, Loss: 0.00013741858128923923\n",
      "Iteration 1286/2000, Loss: 0.000307181355310604\n",
      "Iteration 1287/2000, Loss: 9.97988972812891e-05\n",
      "Iteration 1288/2000, Loss: 0.0003130563418380916\n",
      "Iteration 1289/2000, Loss: 0.0002867099246941507\n",
      "Iteration 1290/2000, Loss: 0.00021111949172336608\n",
      "Iteration 1291/2000, Loss: 0.00020962739654351026\n",
      "Iteration 1292/2000, Loss: 0.0002567982592154294\n",
      "Iteration 1293/2000, Loss: 0.00022208814334589988\n",
      "Iteration 1294/2000, Loss: 0.0003096384461969137\n",
      "Iteration 1295/2000, Loss: 0.000180022805579938\n",
      "Iteration 1296/2000, Loss: 0.00025072673452086747\n",
      "Iteration 1297/2000, Loss: 0.00033132379758171737\n",
      "Iteration 1298/2000, Loss: 0.0001890207058750093\n",
      "Iteration 1299/2000, Loss: 0.0003169531119056046\n",
      "Iteration 1300/2000, Loss: 0.00039201651816256344\n",
      "Iteration 1301/2000, Loss: 0.0003576413728296757\n",
      "Iteration 1302/2000, Loss: 8.682718180352822e-05\n",
      "Iteration 1303/2000, Loss: 0.0002562726440373808\n",
      "Iteration 1304/2000, Loss: 0.0001459769409848377\n",
      "Iteration 1305/2000, Loss: 0.00039560801815241575\n",
      "Iteration 1306/2000, Loss: 0.00016490566486027092\n",
      "Iteration 1307/2000, Loss: 0.00014675174315925688\n",
      "Iteration 1308/2000, Loss: 0.00013138925714883953\n",
      "Iteration 1309/2000, Loss: 0.00014800214557908475\n",
      "Iteration 1310/2000, Loss: 0.00018349962192587554\n",
      "Iteration 1311/2000, Loss: 0.00016423349734395742\n",
      "Iteration 1312/2000, Loss: 0.00015601853374391794\n",
      "Iteration 1313/2000, Loss: 0.00017771418788470328\n",
      "Iteration 1314/2000, Loss: 0.0001571535540279001\n",
      "Iteration 1315/2000, Loss: 0.00015487486962229013\n",
      "Iteration 1316/2000, Loss: 0.00018609387916512787\n",
      "Iteration 1317/2000, Loss: 0.00024201409542001784\n",
      "Iteration 1318/2000, Loss: 0.00021853616635780782\n",
      "Iteration 1319/2000, Loss: 0.00038189045153558254\n",
      "Iteration 1320/2000, Loss: 0.00036523729795590043\n",
      "Iteration 1321/2000, Loss: 0.0002537849359214306\n",
      "Iteration 1322/2000, Loss: 0.00018724017718341202\n",
      "Iteration 1323/2000, Loss: 0.00036306254332885146\n",
      "Iteration 1324/2000, Loss: 0.00025895333965308964\n",
      "Iteration 1325/2000, Loss: 0.0003197993210051209\n",
      "Iteration 1326/2000, Loss: 0.0003082950715906918\n",
      "Iteration 1327/2000, Loss: 0.0001555600465508178\n",
      "Iteration 1328/2000, Loss: 0.00025641254615038633\n",
      "Iteration 1329/2000, Loss: 0.00016364526527468115\n",
      "Iteration 1330/2000, Loss: 0.00020011486776638776\n",
      "Iteration 1331/2000, Loss: 0.00020953612693119794\n",
      "Iteration 1332/2000, Loss: 0.00022943341173231602\n",
      "Iteration 1333/2000, Loss: 0.00026623846497386694\n",
      "Iteration 1334/2000, Loss: 0.0001894842425826937\n",
      "Iteration 1335/2000, Loss: 0.00014273158740252256\n",
      "Iteration 1336/2000, Loss: 0.00027341090026311576\n",
      "Iteration 1337/2000, Loss: 0.00022882492339704186\n",
      "Iteration 1338/2000, Loss: 0.00032889784779399633\n",
      "Iteration 1339/2000, Loss: 0.00019419299496803433\n",
      "Iteration 1340/2000, Loss: 0.0002616119454614818\n",
      "Iteration 1341/2000, Loss: 0.00023242802126333117\n",
      "Iteration 1342/2000, Loss: 0.00022431886463891715\n",
      "Iteration 1343/2000, Loss: 0.00010914514132309705\n",
      "Iteration 1344/2000, Loss: 0.0001847000530688092\n",
      "Iteration 1345/2000, Loss: 0.00023189291823655367\n",
      "Iteration 1346/2000, Loss: 0.00020298223535064608\n",
      "Iteration 1347/2000, Loss: 0.00043991307029500604\n",
      "Iteration 1348/2000, Loss: 0.0001504227111581713\n",
      "Iteration 1349/2000, Loss: 0.00013561378000304103\n",
      "Iteration 1350/2000, Loss: 0.00017203578318003565\n",
      "Iteration 1351/2000, Loss: 0.00012573867570608854\n",
      "Iteration 1352/2000, Loss: 0.00020166226022411138\n",
      "Iteration 1353/2000, Loss: 0.00021967511565890163\n",
      "Iteration 1354/2000, Loss: 0.0002476766239851713\n",
      "Iteration 1355/2000, Loss: 9.19192680157721e-05\n",
      "Iteration 1356/2000, Loss: 0.00012540481111500412\n",
      "Iteration 1357/2000, Loss: 0.00014715756697114557\n",
      "Iteration 1358/2000, Loss: 0.0001487479021307081\n",
      "Iteration 1359/2000, Loss: 0.00022647356672678143\n",
      "Iteration 1360/2000, Loss: 0.00017423013923689723\n",
      "Iteration 1361/2000, Loss: 0.00019053676805924624\n",
      "Iteration 1362/2000, Loss: 0.0002439722593408078\n",
      "Iteration 1363/2000, Loss: 0.00017006117559503764\n",
      "Iteration 1364/2000, Loss: 0.00047472171718254685\n",
      "Iteration 1365/2000, Loss: 0.00011651621025521308\n",
      "Iteration 1366/2000, Loss: 0.0003055580018553883\n",
      "Iteration 1367/2000, Loss: 0.00017058796947821975\n",
      "Iteration 1368/2000, Loss: 0.00013308784400578588\n",
      "Iteration 1369/2000, Loss: 0.00021993003610987216\n",
      "Iteration 1370/2000, Loss: 0.00018269069551024586\n",
      "Iteration 1371/2000, Loss: 0.00016411730030085891\n",
      "Iteration 1372/2000, Loss: 0.00013343959290068597\n",
      "Iteration 1373/2000, Loss: 0.00023230200167745352\n",
      "Iteration 1374/2000, Loss: 0.00014282087795436382\n",
      "Iteration 1375/2000, Loss: 0.0004221094131935388\n",
      "Iteration 1376/2000, Loss: 0.0002173680841224268\n",
      "Iteration 1377/2000, Loss: 0.00031585467513650656\n",
      "Iteration 1378/2000, Loss: 0.00011577070108614862\n",
      "Iteration 1379/2000, Loss: 0.00012364465510472655\n",
      "Iteration 1380/2000, Loss: 0.0001368176017422229\n",
      "Iteration 1381/2000, Loss: 0.00013077881885692477\n",
      "Iteration 1382/2000, Loss: 0.00011540684499777853\n",
      "Iteration 1383/2000, Loss: 0.00014314324653241783\n",
      "Iteration 1384/2000, Loss: 0.0003007426275871694\n",
      "Iteration 1385/2000, Loss: 0.00013748911442235112\n",
      "Iteration 1386/2000, Loss: 0.00017158400441985577\n",
      "Iteration 1387/2000, Loss: 0.00016365511692129076\n",
      "Iteration 1388/2000, Loss: 0.00021599355386570096\n",
      "Iteration 1389/2000, Loss: 0.00015955806884448975\n",
      "Iteration 1390/2000, Loss: 0.00020785305241588503\n",
      "Iteration 1391/2000, Loss: 0.00015989062376320362\n",
      "Iteration 1392/2000, Loss: 0.0002705164370127022\n",
      "Iteration 1393/2000, Loss: 0.0002826590498443693\n",
      "Iteration 1394/2000, Loss: 0.0002305141679244116\n",
      "Iteration 1395/2000, Loss: 0.0002303796063642949\n",
      "Iteration 1396/2000, Loss: 0.0001855153386713937\n",
      "Iteration 1397/2000, Loss: 0.0002734214358497411\n",
      "Iteration 1398/2000, Loss: 0.0001853501016739756\n",
      "Iteration 1399/2000, Loss: 0.0002362798695685342\n",
      "Iteration 1400/2000, Loss: 0.000290426192805171\n",
      "Iteration 1401/2000, Loss: 0.0005027426523156464\n",
      "Iteration 1402/2000, Loss: 0.00034227079595439136\n",
      "Iteration 1403/2000, Loss: 0.00021888942865189165\n",
      "Iteration 1404/2000, Loss: 0.00022542118676938117\n",
      "Iteration 1405/2000, Loss: 0.00032698153518140316\n",
      "Iteration 1406/2000, Loss: 0.00015219757915474474\n",
      "Iteration 1407/2000, Loss: 0.00042239943286404014\n",
      "Iteration 1408/2000, Loss: 0.0003323608834762126\n",
      "Iteration 1409/2000, Loss: 0.00023926560243126005\n",
      "Iteration 1410/2000, Loss: 0.00023749291722197086\n",
      "Iteration 1411/2000, Loss: 0.0004395324212964624\n",
      "Iteration 1412/2000, Loss: 0.0003095196734648198\n",
      "Iteration 1413/2000, Loss: 0.00019346241606399417\n",
      "Iteration 1414/2000, Loss: 0.0002079743571812287\n",
      "Iteration 1415/2000, Loss: 0.00019253010395914316\n",
      "Iteration 1416/2000, Loss: 0.00036649228422902524\n",
      "Iteration 1417/2000, Loss: 0.0003673306491691619\n",
      "Iteration 1418/2000, Loss: 0.0002093597431667149\n",
      "Iteration 1419/2000, Loss: 0.00029919896041974425\n",
      "Iteration 1420/2000, Loss: 0.00042556633707135916\n",
      "Iteration 1421/2000, Loss: 0.0002554844249971211\n",
      "Iteration 1422/2000, Loss: 0.00022748172341380268\n",
      "Iteration 1423/2000, Loss: 0.00025343202287331223\n",
      "Iteration 1424/2000, Loss: 0.00024121091701090336\n",
      "Iteration 1425/2000, Loss: 0.00030575389973819256\n",
      "Iteration 1426/2000, Loss: 0.00011521075066411868\n",
      "Iteration 1427/2000, Loss: 0.00030596638680435717\n",
      "Iteration 1428/2000, Loss: 0.00015850146883167326\n",
      "Iteration 1429/2000, Loss: 0.00027197093004360795\n",
      "Iteration 1430/2000, Loss: 0.00034394950489513576\n",
      "Iteration 1431/2000, Loss: 0.000227846932830289\n",
      "Iteration 1432/2000, Loss: 0.00028744092560373247\n",
      "Iteration 1433/2000, Loss: 0.0002166445046896115\n",
      "Iteration 1434/2000, Loss: 0.0003565703227650374\n",
      "Iteration 1435/2000, Loss: 0.0002380328660365194\n",
      "Iteration 1436/2000, Loss: 0.00026078842347487807\n",
      "Iteration 1437/2000, Loss: 0.00018119526794180274\n",
      "Iteration 1438/2000, Loss: 0.00017351098358631134\n",
      "Iteration 1439/2000, Loss: 0.00039342694799415767\n",
      "Iteration 1440/2000, Loss: 0.0001239059929503128\n",
      "Iteration 1441/2000, Loss: 0.00017492807819508016\n",
      "Iteration 1442/2000, Loss: 0.000289000483462587\n",
      "Iteration 1443/2000, Loss: 0.0003408410120755434\n",
      "Iteration 1444/2000, Loss: 0.00027626127121038735\n",
      "Iteration 1445/2000, Loss: 0.00017666378698777407\n",
      "Iteration 1446/2000, Loss: 0.00015551575052086264\n",
      "Iteration 1447/2000, Loss: 0.00018504992476664484\n",
      "Iteration 1448/2000, Loss: 0.00016362652240786701\n",
      "Iteration 1449/2000, Loss: 0.0001766345085343346\n",
      "Iteration 1450/2000, Loss: 0.0002873726771213114\n",
      "Iteration 1451/2000, Loss: 0.00027808474260382354\n",
      "Iteration 1452/2000, Loss: 0.000138987394166179\n",
      "Iteration 1453/2000, Loss: 0.0001438497129129246\n",
      "Iteration 1454/2000, Loss: 0.00010798380390042439\n",
      "Iteration 1455/2000, Loss: 0.0001351621322100982\n",
      "Iteration 1456/2000, Loss: 0.00017348899564240128\n",
      "Iteration 1457/2000, Loss: 0.00033450123737566173\n",
      "Iteration 1458/2000, Loss: 0.00012388628965709358\n",
      "Iteration 1459/2000, Loss: 0.0003038681170437485\n",
      "Iteration 1460/2000, Loss: 0.00020034423505421728\n",
      "Iteration 1461/2000, Loss: 0.00011338086187606677\n",
      "Iteration 1462/2000, Loss: 0.00017920220852829516\n",
      "Iteration 1463/2000, Loss: 0.0002081766870105639\n",
      "Iteration 1464/2000, Loss: 0.0001534361654194072\n",
      "Iteration 1465/2000, Loss: 0.00024011178174987435\n",
      "Iteration 1466/2000, Loss: 0.00017140289128292352\n",
      "Iteration 1467/2000, Loss: 0.0002676305884961039\n",
      "Iteration 1468/2000, Loss: 0.00019208135199733078\n",
      "Iteration 1469/2000, Loss: 0.00012662906374316663\n",
      "Iteration 1470/2000, Loss: 0.0001468566624680534\n",
      "Iteration 1471/2000, Loss: 0.00017022753308992833\n",
      "Iteration 1472/2000, Loss: 0.00013337428390514106\n",
      "Iteration 1473/2000, Loss: 0.00017406142433173954\n",
      "Iteration 1474/2000, Loss: 0.00020763050997629762\n",
      "Iteration 1475/2000, Loss: 0.00020191662770230323\n",
      "Iteration 1476/2000, Loss: 0.00012559822062030435\n",
      "Iteration 1477/2000, Loss: 0.0001804406347218901\n",
      "Iteration 1478/2000, Loss: 0.000291153002763167\n",
      "Iteration 1479/2000, Loss: 0.00025012422702275217\n",
      "Iteration 1480/2000, Loss: 0.0001286705519305542\n",
      "Iteration 1481/2000, Loss: 0.00023806643730495125\n",
      "Iteration 1482/2000, Loss: 0.0002129068598151207\n",
      "Iteration 1483/2000, Loss: 0.00036059634294360876\n",
      "Iteration 1484/2000, Loss: 0.00014626423944719136\n",
      "Iteration 1485/2000, Loss: 0.00020281354954931885\n",
      "Iteration 1486/2000, Loss: 0.00019227151642553508\n",
      "Iteration 1487/2000, Loss: 0.00019914060248993337\n",
      "Iteration 1488/2000, Loss: 8.936627273214981e-05\n",
      "Iteration 1489/2000, Loss: 0.0001406490773661062\n",
      "Iteration 1490/2000, Loss: 0.00032177334651350975\n",
      "Iteration 1491/2000, Loss: 0.00022654268832411617\n",
      "Iteration 1492/2000, Loss: 0.0001186294830404222\n",
      "Iteration 1493/2000, Loss: 0.00013461823982652277\n",
      "Iteration 1494/2000, Loss: 0.00011698448361130431\n",
      "Iteration 1495/2000, Loss: 0.0001621445844648406\n",
      "Iteration 1496/2000, Loss: 0.00012682011583819985\n",
      "Iteration 1497/2000, Loss: 0.00014814335736446083\n",
      "Iteration 1498/2000, Loss: 0.00018610156257636845\n",
      "Iteration 1499/2000, Loss: 0.00028829422080889344\n",
      "Iteration 1500/2000, Loss: 0.0003475972043816\n",
      "Iteration 1501/2000, Loss: 0.000153364147990942\n",
      "Iteration 1502/2000, Loss: 0.00022248535242397338\n",
      "Iteration 1503/2000, Loss: 0.00025568093406036496\n",
      "Iteration 1504/2000, Loss: 0.0001799413003027439\n",
      "Iteration 1505/2000, Loss: 0.00036593846743926406\n",
      "Iteration 1506/2000, Loss: 0.00024115719133988023\n",
      "Iteration 1507/2000, Loss: 0.00013652672350872308\n",
      "Iteration 1508/2000, Loss: 0.00022281739802565426\n",
      "Iteration 1509/2000, Loss: 0.00021456461399793625\n",
      "Iteration 1510/2000, Loss: 0.00014362561341840774\n",
      "Iteration 1511/2000, Loss: 0.0002435296482872218\n",
      "Iteration 1512/2000, Loss: 0.0001133422993007116\n",
      "Iteration 1513/2000, Loss: 0.0001753813266986981\n",
      "Iteration 1514/2000, Loss: 0.0002973806404042989\n",
      "Iteration 1515/2000, Loss: 0.00019218596571590751\n",
      "Iteration 1516/2000, Loss: 0.00022917242313269526\n",
      "Iteration 1517/2000, Loss: 0.0002666057553142309\n",
      "Iteration 1518/2000, Loss: 0.00020580067939590663\n",
      "Iteration 1519/2000, Loss: 0.00012584569049067795\n",
      "Iteration 1520/2000, Loss: 0.00019131909357383847\n",
      "Iteration 1521/2000, Loss: 0.0001568259031046182\n",
      "Iteration 1522/2000, Loss: 0.0002599368744995445\n",
      "Iteration 1523/2000, Loss: 0.00020943339040968567\n",
      "Iteration 1524/2000, Loss: 0.00023073851480148733\n",
      "Iteration 1525/2000, Loss: 0.00012688081187661737\n",
      "Iteration 1526/2000, Loss: 0.00019783260358963162\n",
      "Iteration 1527/2000, Loss: 0.0004989500739611685\n",
      "Iteration 1528/2000, Loss: 0.0001896713802125305\n",
      "Iteration 1529/2000, Loss: 0.0002079870755551383\n",
      "Iteration 1530/2000, Loss: 0.0002984225866384804\n",
      "Iteration 1531/2000, Loss: 0.00031396662234328687\n",
      "Iteration 1532/2000, Loss: 0.00012880784925073385\n",
      "Iteration 1533/2000, Loss: 0.00029615501989610493\n",
      "Iteration 1534/2000, Loss: 0.00020120492263231426\n",
      "Iteration 1535/2000, Loss: 0.00018418842228129506\n",
      "Iteration 1536/2000, Loss: 0.000142931574373506\n",
      "Iteration 1537/2000, Loss: 0.00037760319537483156\n",
      "Iteration 1538/2000, Loss: 0.00015100989548955113\n",
      "Iteration 1539/2000, Loss: 0.0001507110137026757\n",
      "Iteration 1540/2000, Loss: 0.0002976838150061667\n",
      "Iteration 1541/2000, Loss: 0.0001404251961503178\n",
      "Iteration 1542/2000, Loss: 0.0001379562309011817\n",
      "Iteration 1543/2000, Loss: 0.00012080182932550088\n",
      "Iteration 1544/2000, Loss: 0.00022988920682109892\n",
      "Iteration 1545/2000, Loss: 0.00028783074230886996\n",
      "Iteration 1546/2000, Loss: 0.0002364840474911034\n",
      "Iteration 1547/2000, Loss: 0.00020177931583020836\n",
      "Iteration 1548/2000, Loss: 0.0004139117372687906\n",
      "Iteration 1549/2000, Loss: 0.00011782883666455746\n",
      "Iteration 1550/2000, Loss: 0.00038862478686496615\n",
      "Iteration 1551/2000, Loss: 0.00017198857676703483\n",
      "Iteration 1552/2000, Loss: 0.0004808730154763907\n",
      "Iteration 1553/2000, Loss: 0.0004094578616786748\n",
      "Iteration 1554/2000, Loss: 0.00028788254712708294\n",
      "Iteration 1555/2000, Loss: 0.0004827085940632969\n",
      "Iteration 1556/2000, Loss: 0.00011959928087890148\n",
      "Iteration 1557/2000, Loss: 0.00035832604044117033\n",
      "Iteration 1558/2000, Loss: 0.00023730787506792694\n",
      "Iteration 1559/2000, Loss: 0.0002156476693926379\n",
      "Iteration 1560/2000, Loss: 0.00028231964097358286\n",
      "Iteration 1561/2000, Loss: 0.00044377383892424405\n",
      "Iteration 1562/2000, Loss: 0.00012894810060970485\n",
      "Iteration 1563/2000, Loss: 0.00027454301016405225\n",
      "Iteration 1564/2000, Loss: 0.00013091914297547191\n",
      "Iteration 1565/2000, Loss: 0.0003649204154498875\n",
      "Iteration 1566/2000, Loss: 0.0001549160951981321\n",
      "Iteration 1567/2000, Loss: 0.00044242944568395615\n",
      "Iteration 1568/2000, Loss: 0.0002886111324187368\n",
      "Iteration 1569/2000, Loss: 0.00029249582439661026\n",
      "Iteration 1570/2000, Loss: 0.0002996271359734237\n",
      "Iteration 1571/2000, Loss: 0.00020625760953407735\n",
      "Iteration 1572/2000, Loss: 0.00021630959236063063\n",
      "Iteration 1573/2000, Loss: 0.0001807281660148874\n",
      "Iteration 1574/2000, Loss: 0.0003107338852714747\n",
      "Iteration 1575/2000, Loss: 0.00013617452350445092\n",
      "Iteration 1576/2000, Loss: 0.0002921702398452908\n",
      "Iteration 1577/2000, Loss: 0.00014416909834835678\n",
      "Iteration 1578/2000, Loss: 0.00030649296240881085\n",
      "Iteration 1579/2000, Loss: 0.00034253267222084105\n",
      "Iteration 1580/2000, Loss: 0.00025125188403762877\n",
      "Iteration 1581/2000, Loss: 0.00018837727839127183\n",
      "Iteration 1582/2000, Loss: 0.00031535516609437764\n",
      "Iteration 1583/2000, Loss: 0.00034290459007024765\n",
      "Iteration 1584/2000, Loss: 0.00029748142696917057\n",
      "Iteration 1585/2000, Loss: 0.0004948138957843184\n",
      "Iteration 1586/2000, Loss: 0.0005365872057154775\n",
      "Iteration 1587/2000, Loss: 0.00045267585664987564\n",
      "Iteration 1588/2000, Loss: 0.0007176210638135672\n",
      "Iteration 1589/2000, Loss: 0.00032324279891327024\n",
      "Iteration 1590/2000, Loss: 0.0009681559749878943\n",
      "Iteration 1591/2000, Loss: 0.0002588684146758169\n",
      "Iteration 1592/2000, Loss: 0.000590588606428355\n",
      "Iteration 1593/2000, Loss: 0.00030555424746125937\n",
      "Iteration 1594/2000, Loss: 0.0009245986002497375\n",
      "Iteration 1595/2000, Loss: 0.0006956447614356875\n",
      "Iteration 1596/2000, Loss: 0.0006420144345611334\n",
      "Iteration 1597/2000, Loss: 0.000605217763222754\n",
      "Iteration 1598/2000, Loss: 0.0002845630224328488\n",
      "Iteration 1599/2000, Loss: 0.0003103863273281604\n",
      "Iteration 1600/2000, Loss: 0.0004440892080310732\n",
      "Iteration 1601/2000, Loss: 0.00042723340447992086\n",
      "Iteration 1602/2000, Loss: 0.0004475457244552672\n",
      "Iteration 1603/2000, Loss: 0.0007452855934388936\n",
      "Iteration 1604/2000, Loss: 0.0003942041366826743\n",
      "Iteration 1605/2000, Loss: 0.0005452369805425406\n",
      "Iteration 1606/2000, Loss: 0.00046306289732456207\n",
      "Iteration 1607/2000, Loss: 0.00041712637175805867\n",
      "Iteration 1608/2000, Loss: 0.0002874341153074056\n",
      "Iteration 1609/2000, Loss: 0.0004053139709867537\n",
      "Iteration 1610/2000, Loss: 0.000338996818754822\n",
      "Iteration 1611/2000, Loss: 0.0004111761227250099\n",
      "Iteration 1612/2000, Loss: 0.0002479909162502736\n",
      "Iteration 1613/2000, Loss: 0.0004302967572584748\n",
      "Iteration 1614/2000, Loss: 0.0005147218471392989\n",
      "Iteration 1615/2000, Loss: 0.00053325918270275\n",
      "Iteration 1616/2000, Loss: 0.00034707694430835545\n",
      "Iteration 1617/2000, Loss: 0.0007205209694802761\n",
      "Iteration 1618/2000, Loss: 0.00011811437434516847\n",
      "Iteration 1619/2000, Loss: 0.0004929574788548052\n",
      "Iteration 1620/2000, Loss: 0.0001583560515427962\n",
      "Iteration 1621/2000, Loss: 0.00046033639227971435\n",
      "Iteration 1622/2000, Loss: 0.00038803668576292694\n",
      "Iteration 1623/2000, Loss: 0.0004975245683453977\n",
      "Iteration 1624/2000, Loss: 0.0003270623565185815\n",
      "Iteration 1625/2000, Loss: 0.00031249920721165836\n",
      "Iteration 1626/2000, Loss: 0.0007783953915350139\n",
      "Iteration 1627/2000, Loss: 0.0008477607043460011\n",
      "Iteration 1628/2000, Loss: 0.0007425536750815809\n",
      "Iteration 1629/2000, Loss: 0.00044952501775696874\n",
      "Iteration 1630/2000, Loss: 0.0005496047087945044\n",
      "Iteration 1631/2000, Loss: 0.0005793272284790874\n",
      "Iteration 1632/2000, Loss: 0.0009116152068600059\n",
      "Iteration 1633/2000, Loss: 0.00041340503958053887\n",
      "Iteration 1634/2000, Loss: 0.000761321047320962\n",
      "Iteration 1635/2000, Loss: 0.00038892868906259537\n",
      "Iteration 1636/2000, Loss: 0.0004449394764378667\n",
      "Iteration 1637/2000, Loss: 0.00040880293818190694\n",
      "Iteration 1638/2000, Loss: 0.0004455799062270671\n",
      "Iteration 1639/2000, Loss: 0.00035572456545196474\n",
      "Iteration 1640/2000, Loss: 0.0002483900752849877\n",
      "Iteration 1641/2000, Loss: 0.0003555821895133704\n",
      "Iteration 1642/2000, Loss: 0.000241490313783288\n",
      "Iteration 1643/2000, Loss: 0.00024886883329600096\n",
      "Iteration 1644/2000, Loss: 0.0003136912710033357\n",
      "Iteration 1645/2000, Loss: 0.00030614377465099096\n",
      "Iteration 1646/2000, Loss: 0.0002625480992719531\n",
      "Iteration 1647/2000, Loss: 0.0003523959603626281\n",
      "Iteration 1648/2000, Loss: 0.00024002068676054478\n",
      "Iteration 1649/2000, Loss: 0.0003969589015468955\n",
      "Iteration 1650/2000, Loss: 0.00033769060974009335\n",
      "Iteration 1651/2000, Loss: 0.0007797779398970306\n",
      "Iteration 1652/2000, Loss: 0.000260365690337494\n",
      "Iteration 1653/2000, Loss: 0.0002341657818760723\n",
      "Iteration 1654/2000, Loss: 0.00025177045608870685\n",
      "Iteration 1655/2000, Loss: 0.00024220971681643277\n",
      "Iteration 1656/2000, Loss: 0.00020191010844428092\n",
      "Iteration 1657/2000, Loss: 0.00025917813763953745\n",
      "Iteration 1658/2000, Loss: 0.0003106528311036527\n",
      "Iteration 1659/2000, Loss: 0.000578859995584935\n",
      "Iteration 1660/2000, Loss: 0.0002712046552915126\n",
      "Iteration 1661/2000, Loss: 0.00029646631446667016\n",
      "Iteration 1662/2000, Loss: 0.0002140678116120398\n",
      "Iteration 1663/2000, Loss: 0.00026186538161709905\n",
      "Iteration 1664/2000, Loss: 0.0006159160402603447\n",
      "Iteration 1665/2000, Loss: 0.00018074808758683503\n",
      "Iteration 1666/2000, Loss: 0.0002098544646287337\n",
      "Iteration 1667/2000, Loss: 0.00020368947298265994\n",
      "Iteration 1668/2000, Loss: 0.0005376580520533025\n",
      "Iteration 1669/2000, Loss: 0.00023478885123040527\n",
      "Iteration 1670/2000, Loss: 0.00020745370420627296\n",
      "Iteration 1671/2000, Loss: 0.00023018474166747183\n",
      "Iteration 1672/2000, Loss: 0.00021729526633862406\n",
      "Iteration 1673/2000, Loss: 0.0002924809814430773\n",
      "Iteration 1674/2000, Loss: 0.00018428484327159822\n",
      "Iteration 1675/2000, Loss: 0.0001684091257629916\n",
      "Iteration 1676/2000, Loss: 0.00029028396238572896\n",
      "Iteration 1677/2000, Loss: 0.00019519355555530638\n",
      "Iteration 1678/2000, Loss: 0.000259656721027568\n",
      "Iteration 1679/2000, Loss: 0.00013191619655117393\n",
      "Iteration 1680/2000, Loss: 0.0002883124107029289\n",
      "Iteration 1681/2000, Loss: 0.0002129984350176528\n",
      "Iteration 1682/2000, Loss: 0.0003712403995450586\n",
      "Iteration 1683/2000, Loss: 0.00023291097022593021\n",
      "Iteration 1684/2000, Loss: 0.000315831508487463\n",
      "Iteration 1685/2000, Loss: 0.00023033897741697729\n",
      "Iteration 1686/2000, Loss: 0.0002531363570597023\n",
      "Iteration 1687/2000, Loss: 0.00030605783103965223\n",
      "Iteration 1688/2000, Loss: 0.00016643230628687888\n",
      "Iteration 1689/2000, Loss: 0.0002185006596846506\n",
      "Iteration 1690/2000, Loss: 0.000234314298722893\n",
      "Iteration 1691/2000, Loss: 0.00024116675194818527\n",
      "Iteration 1692/2000, Loss: 0.00017699984891805798\n",
      "Iteration 1693/2000, Loss: 0.00018011440988630056\n",
      "Iteration 1694/2000, Loss: 0.0002890785108320415\n",
      "Iteration 1695/2000, Loss: 0.00015478022396564484\n",
      "Iteration 1696/2000, Loss: 0.0002487200254108757\n",
      "Iteration 1697/2000, Loss: 0.00018697579798754305\n",
      "Iteration 1698/2000, Loss: 0.00018332111358176917\n",
      "Iteration 1699/2000, Loss: 0.00030282101943157613\n",
      "Iteration 1700/2000, Loss: 0.00016584628610871732\n",
      "Iteration 1701/2000, Loss: 0.00021937466226518154\n",
      "Iteration 1702/2000, Loss: 0.00022017932496964931\n",
      "Iteration 1703/2000, Loss: 0.0001888271508505568\n",
      "Iteration 1704/2000, Loss: 0.00023101401166059077\n",
      "Iteration 1705/2000, Loss: 0.00030303679523058236\n",
      "Iteration 1706/2000, Loss: 0.00019445849466137588\n",
      "Iteration 1707/2000, Loss: 0.0002101726713590324\n",
      "Iteration 1708/2000, Loss: 0.00023180794960353523\n",
      "Iteration 1709/2000, Loss: 0.00022144813556224108\n",
      "Iteration 1710/2000, Loss: 0.00014685509086120874\n",
      "Iteration 1711/2000, Loss: 0.0002114884409820661\n",
      "Iteration 1712/2000, Loss: 0.0002660910249687731\n",
      "Iteration 1713/2000, Loss: 0.00018628612451720983\n",
      "Iteration 1714/2000, Loss: 0.00019377762509975582\n",
      "Iteration 1715/2000, Loss: 0.00013556935300584882\n",
      "Iteration 1716/2000, Loss: 0.0001403645728714764\n",
      "Iteration 1717/2000, Loss: 0.00027678965125232935\n",
      "Iteration 1718/2000, Loss: 0.00013402261538431048\n",
      "Iteration 1719/2000, Loss: 0.00014723226195201278\n",
      "Iteration 1720/2000, Loss: 0.0001813344715628773\n",
      "Iteration 1721/2000, Loss: 0.0003066321660298854\n",
      "Iteration 1722/2000, Loss: 0.00023688234796281904\n",
      "Iteration 1723/2000, Loss: 0.00018844295118469745\n",
      "Iteration 1724/2000, Loss: 0.00017153237422462553\n",
      "Iteration 1725/2000, Loss: 0.00019517503096722066\n",
      "Iteration 1726/2000, Loss: 0.00021656136959791183\n",
      "Iteration 1727/2000, Loss: 0.00028736761305481195\n",
      "Iteration 1728/2000, Loss: 0.0002681663609109819\n",
      "Iteration 1729/2000, Loss: 0.00013693326036445796\n",
      "Iteration 1730/2000, Loss: 0.00022965965035837144\n",
      "Iteration 1731/2000, Loss: 0.00010727443441282958\n",
      "Iteration 1732/2000, Loss: 0.00018159818137064576\n",
      "Iteration 1733/2000, Loss: 0.0001578073570271954\n",
      "Iteration 1734/2000, Loss: 0.00018021494906861335\n",
      "Iteration 1735/2000, Loss: 0.00025661353720352054\n",
      "Iteration 1736/2000, Loss: 0.00019859908206854016\n",
      "Iteration 1737/2000, Loss: 0.0001523430400993675\n",
      "Iteration 1738/2000, Loss: 0.0003109991957899183\n",
      "Iteration 1739/2000, Loss: 0.00018614732834976166\n",
      "Iteration 1740/2000, Loss: 0.00018994994752574712\n",
      "Iteration 1741/2000, Loss: 0.00013977316848468035\n",
      "Iteration 1742/2000, Loss: 0.00027337149367667735\n",
      "Iteration 1743/2000, Loss: 0.0001379205787088722\n",
      "Iteration 1744/2000, Loss: 0.00021021800057496876\n",
      "Iteration 1745/2000, Loss: 0.00017076285439543426\n",
      "Iteration 1746/2000, Loss: 0.00016182362742256373\n",
      "Iteration 1747/2000, Loss: 0.0001807170920073986\n",
      "Iteration 1748/2000, Loss: 0.00020989382755942643\n",
      "Iteration 1749/2000, Loss: 0.00020983033755328506\n",
      "Iteration 1750/2000, Loss: 0.0003382600552868098\n",
      "Iteration 1751/2000, Loss: 0.00012262184463907033\n",
      "Iteration 1752/2000, Loss: 0.0003366649616509676\n",
      "Iteration 1753/2000, Loss: 0.0002750504936557263\n",
      "Iteration 1754/2000, Loss: 0.0002721945638768375\n",
      "Iteration 1755/2000, Loss: 0.00015273092139977962\n",
      "Iteration 1756/2000, Loss: 0.00017702985496725887\n",
      "Iteration 1757/2000, Loss: 0.00034755258820950985\n",
      "Iteration 1758/2000, Loss: 0.00012404164590407163\n",
      "Iteration 1759/2000, Loss: 0.00015524134505540133\n",
      "Iteration 1760/2000, Loss: 0.00018128946248907596\n",
      "Iteration 1761/2000, Loss: 0.0003538120654411614\n",
      "Iteration 1762/2000, Loss: 0.0004163778212387115\n",
      "Iteration 1763/2000, Loss: 0.00016856618458405137\n",
      "Iteration 1764/2000, Loss: 0.00020633914391510189\n",
      "Iteration 1765/2000, Loss: 0.0003753542259801179\n",
      "Iteration 1766/2000, Loss: 0.0002174313267460093\n",
      "Iteration 1767/2000, Loss: 0.00014924209972377867\n",
      "Iteration 1768/2000, Loss: 0.00023480085656046867\n",
      "Iteration 1769/2000, Loss: 0.00015759962843731046\n",
      "Iteration 1770/2000, Loss: 0.0001480376231484115\n",
      "Iteration 1771/2000, Loss: 0.00011386628466425464\n",
      "Iteration 1772/2000, Loss: 0.00019983784295618534\n",
      "Iteration 1773/2000, Loss: 0.00021417999232653528\n",
      "Iteration 1774/2000, Loss: 0.0001949711877387017\n",
      "Iteration 1775/2000, Loss: 0.0003464162291493267\n",
      "Iteration 1776/2000, Loss: 0.00020891767053399235\n",
      "Iteration 1777/2000, Loss: 0.00028777599800378084\n",
      "Iteration 1778/2000, Loss: 0.00022950899438001215\n",
      "Iteration 1779/2000, Loss: 0.00022819028527010232\n",
      "Iteration 1780/2000, Loss: 0.00023781096388120204\n",
      "Iteration 1781/2000, Loss: 0.0005272903363220394\n",
      "Iteration 1782/2000, Loss: 0.000523326569236815\n",
      "Iteration 1783/2000, Loss: 0.00019825174240395427\n",
      "Iteration 1784/2000, Loss: 0.00025236286455765367\n",
      "Iteration 1785/2000, Loss: 0.00017540657427161932\n",
      "Iteration 1786/2000, Loss: 0.0001799619203666225\n",
      "Iteration 1787/2000, Loss: 0.0001319058210356161\n",
      "Iteration 1788/2000, Loss: 0.00020533757924567908\n",
      "Iteration 1789/2000, Loss: 0.00013619143282994628\n",
      "Iteration 1790/2000, Loss: 0.00028057032614015043\n",
      "Iteration 1791/2000, Loss: 0.00024959060829132795\n",
      "Iteration 1792/2000, Loss: 0.00018853334768209606\n",
      "Iteration 1793/2000, Loss: 0.0002449986059218645\n",
      "Iteration 1794/2000, Loss: 0.0001877404429251328\n",
      "Iteration 1795/2000, Loss: 0.00018757239740807563\n",
      "Iteration 1796/2000, Loss: 0.0004570324963424355\n",
      "Iteration 1797/2000, Loss: 0.0001249171473318711\n",
      "Iteration 1798/2000, Loss: 0.00013415078865364194\n",
      "Iteration 1799/2000, Loss: 0.0003963740891776979\n",
      "Iteration 1800/2000, Loss: 0.0001900061033666134\n",
      "Iteration 1801/2000, Loss: 0.00015984647325240076\n",
      "Iteration 1802/2000, Loss: 0.00013844738714396954\n",
      "Iteration 1803/2000, Loss: 0.0002295841259183362\n",
      "Iteration 1804/2000, Loss: 0.00013071043940726668\n",
      "Iteration 1805/2000, Loss: 0.00010906357783824205\n",
      "Iteration 1806/2000, Loss: 0.00025544536765664816\n",
      "Iteration 1807/2000, Loss: 0.00018255422764923424\n",
      "Iteration 1808/2000, Loss: 0.00012155427975812927\n",
      "Iteration 1809/2000, Loss: 0.00012703207903541625\n",
      "Iteration 1810/2000, Loss: 0.00015834439545869827\n",
      "Iteration 1811/2000, Loss: 0.0001711394143057987\n",
      "Iteration 1812/2000, Loss: 0.00017734916764311492\n",
      "Iteration 1813/2000, Loss: 0.00012058782886015251\n",
      "Iteration 1814/2000, Loss: 0.00013356142153497785\n",
      "Iteration 1815/2000, Loss: 0.00010704527085181326\n",
      "Iteration 1816/2000, Loss: 0.00010870791447814554\n",
      "Iteration 1817/2000, Loss: 0.00015668988635297865\n",
      "Iteration 1818/2000, Loss: 0.000126170227304101\n",
      "Iteration 1819/2000, Loss: 0.00015701209485996515\n",
      "Iteration 1820/2000, Loss: 0.00021103666222188622\n",
      "Iteration 1821/2000, Loss: 0.00011872244067490101\n",
      "Iteration 1822/2000, Loss: 0.0001987003197427839\n",
      "Iteration 1823/2000, Loss: 8.453658665530384e-05\n",
      "Iteration 1824/2000, Loss: 0.00015945512859616429\n",
      "Iteration 1825/2000, Loss: 0.00022284913575276732\n",
      "Iteration 1826/2000, Loss: 9.60776669671759e-05\n",
      "Iteration 1827/2000, Loss: 0.00025273868232034147\n",
      "Iteration 1828/2000, Loss: 0.00020347116515040398\n",
      "Iteration 1829/2000, Loss: 0.00015410299238283187\n",
      "Iteration 1830/2000, Loss: 0.000228627395699732\n",
      "Iteration 1831/2000, Loss: 0.00024770607706159353\n",
      "Iteration 1832/2000, Loss: 0.0001930848666233942\n",
      "Iteration 1833/2000, Loss: 0.00018693461606744677\n",
      "Iteration 1834/2000, Loss: 0.00016079502529464662\n",
      "Iteration 1835/2000, Loss: 0.00015341276593971997\n",
      "Iteration 1836/2000, Loss: 0.0002268812240799889\n",
      "Iteration 1837/2000, Loss: 0.00043960160110145807\n",
      "Iteration 1838/2000, Loss: 0.0001270937646040693\n",
      "Iteration 1839/2000, Loss: 0.00018429706688039005\n",
      "Iteration 1840/2000, Loss: 0.0004704166785813868\n",
      "Iteration 1841/2000, Loss: 0.0007786017376929522\n",
      "Iteration 1842/2000, Loss: 0.00019859519670717418\n",
      "Iteration 1843/2000, Loss: 0.00018324254779145122\n",
      "Iteration 1844/2000, Loss: 0.00026129535399377346\n",
      "Iteration 1845/2000, Loss: 0.00025468412786722183\n",
      "Iteration 1846/2000, Loss: 0.0002805765252560377\n",
      "Iteration 1847/2000, Loss: 0.00022165615519043058\n",
      "Iteration 1848/2000, Loss: 0.00027093131211586297\n",
      "Iteration 1849/2000, Loss: 0.00024495430989190936\n",
      "Iteration 1850/2000, Loss: 0.00016874686116352677\n",
      "Iteration 1851/2000, Loss: 0.00017056708747986704\n",
      "Iteration 1852/2000, Loss: 0.00011982789146713912\n",
      "Iteration 1853/2000, Loss: 0.00023395511379931122\n",
      "Iteration 1854/2000, Loss: 0.0001415072474628687\n",
      "Iteration 1855/2000, Loss: 0.00018540357996243984\n",
      "Iteration 1856/2000, Loss: 0.00022923469077795744\n",
      "Iteration 1857/2000, Loss: 0.00021642408682964742\n",
      "Iteration 1858/2000, Loss: 0.00032249928335659206\n",
      "Iteration 1859/2000, Loss: 0.00022302295838017017\n",
      "Iteration 1860/2000, Loss: 0.00022889710089657456\n",
      "Iteration 1861/2000, Loss: 0.00014916546933818609\n",
      "Iteration 1862/2000, Loss: 0.0002163329627364874\n",
      "Iteration 1863/2000, Loss: 0.00013105680409353226\n",
      "Iteration 1864/2000, Loss: 0.00018487214401829988\n",
      "Iteration 1865/2000, Loss: 0.00019575374608393759\n",
      "Iteration 1866/2000, Loss: 0.00018166995141655207\n",
      "Iteration 1867/2000, Loss: 0.00017357310571242124\n",
      "Iteration 1868/2000, Loss: 0.00020289533131290227\n",
      "Iteration 1869/2000, Loss: 0.00021341629326343536\n",
      "Iteration 1870/2000, Loss: 0.00016981737280730158\n",
      "Iteration 1871/2000, Loss: 0.00025545316748321056\n",
      "Iteration 1872/2000, Loss: 0.00020389117707964033\n",
      "Iteration 1873/2000, Loss: 0.0003140133630950004\n",
      "Iteration 1874/2000, Loss: 0.0001455820311093703\n",
      "Iteration 1875/2000, Loss: 0.0002793839084915817\n",
      "Iteration 1876/2000, Loss: 0.00022034418361727148\n",
      "Iteration 1877/2000, Loss: 0.0002419390220893547\n",
      "Iteration 1878/2000, Loss: 0.00022336230904329568\n",
      "Iteration 1879/2000, Loss: 0.0002525111776776612\n",
      "Iteration 1880/2000, Loss: 0.00026281698956154287\n",
      "Iteration 1881/2000, Loss: 0.0003345911973156035\n",
      "Iteration 1882/2000, Loss: 0.0004369946545921266\n",
      "Iteration 1883/2000, Loss: 0.00018173422722611576\n",
      "Iteration 1884/2000, Loss: 0.0003310109314043075\n",
      "Iteration 1885/2000, Loss: 0.0001678005646681413\n",
      "Iteration 1886/2000, Loss: 0.00030724331736564636\n",
      "Iteration 1887/2000, Loss: 0.00019968301057815552\n",
      "Iteration 1888/2000, Loss: 0.000177254609297961\n",
      "Iteration 1889/2000, Loss: 0.00022529435227625072\n",
      "Iteration 1890/2000, Loss: 0.0001913566084112972\n",
      "Iteration 1891/2000, Loss: 0.0002214702544733882\n",
      "Iteration 1892/2000, Loss: 0.0001554529881104827\n",
      "Iteration 1893/2000, Loss: 0.0001277523988392204\n",
      "Iteration 1894/2000, Loss: 0.0002430193853797391\n",
      "Iteration 1895/2000, Loss: 0.0002155430120183155\n",
      "Iteration 1896/2000, Loss: 0.00017088637105189264\n",
      "Iteration 1897/2000, Loss: 9.841252904152498e-05\n",
      "Iteration 1898/2000, Loss: 0.00022800483566243201\n",
      "Iteration 1899/2000, Loss: 0.00023517711088061333\n",
      "Iteration 1900/2000, Loss: 0.00017031028983183205\n",
      "Iteration 1901/2000, Loss: 0.0001965231931535527\n",
      "Iteration 1902/2000, Loss: 0.00018177254241891205\n",
      "Iteration 1903/2000, Loss: 0.0002492551284376532\n",
      "Iteration 1904/2000, Loss: 0.00018661920330487192\n",
      "Iteration 1905/2000, Loss: 0.00023981633421499282\n",
      "Iteration 1906/2000, Loss: 0.00015500276640523225\n",
      "Iteration 1907/2000, Loss: 0.0005504171713255346\n",
      "Iteration 1908/2000, Loss: 0.00015372087364085019\n",
      "Iteration 1909/2000, Loss: 0.0002876731741707772\n",
      "Iteration 1910/2000, Loss: 0.0002084809821099043\n",
      "Iteration 1911/2000, Loss: 0.00015541694301646203\n",
      "Iteration 1912/2000, Loss: 0.00020863251120317727\n",
      "Iteration 1913/2000, Loss: 9.450768266106024e-05\n",
      "Iteration 1914/2000, Loss: 0.00015600182814523578\n",
      "Iteration 1915/2000, Loss: 9.039688302436844e-05\n",
      "Iteration 1916/2000, Loss: 0.00015647782129235566\n",
      "Iteration 1917/2000, Loss: 0.00012413080548867583\n",
      "Iteration 1918/2000, Loss: 0.00011921930126845837\n",
      "Iteration 1919/2000, Loss: 0.00021582173940259963\n",
      "Iteration 1920/2000, Loss: 0.00019769695063587278\n",
      "Iteration 1921/2000, Loss: 0.00021424041187856346\n",
      "Iteration 1922/2000, Loss: 0.00010054068843601272\n",
      "Iteration 1923/2000, Loss: 0.00019318981503602117\n",
      "Iteration 1924/2000, Loss: 0.00017137295799329877\n",
      "Iteration 1925/2000, Loss: 0.0007058487390168011\n",
      "Iteration 1926/2000, Loss: 0.00011680721945594996\n",
      "Iteration 1927/2000, Loss: 0.00023288116790354252\n",
      "Iteration 1928/2000, Loss: 0.00010455241135787219\n",
      "Iteration 1929/2000, Loss: 0.00020232815586496145\n",
      "Iteration 1930/2000, Loss: 0.00026999731198884547\n",
      "Iteration 1931/2000, Loss: 0.00018055753025691956\n",
      "Iteration 1932/2000, Loss: 0.00020520515681710094\n",
      "Iteration 1933/2000, Loss: 0.00021092839597258717\n",
      "Iteration 1934/2000, Loss: 0.00036183357588015497\n",
      "Iteration 1935/2000, Loss: 0.00017427273269277066\n",
      "Iteration 1936/2000, Loss: 0.00019378542492631823\n",
      "Iteration 1937/2000, Loss: 0.00022077866015024483\n",
      "Iteration 1938/2000, Loss: 0.0002831358287949115\n",
      "Iteration 1939/2000, Loss: 0.00014174489479046315\n",
      "Iteration 1940/2000, Loss: 0.00033267258550040424\n",
      "Iteration 1941/2000, Loss: 0.0002922002167906612\n",
      "Iteration 1942/2000, Loss: 0.00012894076644442976\n",
      "Iteration 1943/2000, Loss: 0.00032955207279883325\n",
      "Iteration 1944/2000, Loss: 0.00027681642677634954\n",
      "Iteration 1945/2000, Loss: 0.00017969109467230737\n",
      "Iteration 1946/2000, Loss: 0.00027908760239370167\n",
      "Iteration 1947/2000, Loss: 0.0003176027094013989\n",
      "Iteration 1948/2000, Loss: 0.00023146656167227775\n",
      "Iteration 1949/2000, Loss: 0.000182824456715025\n",
      "Iteration 1950/2000, Loss: 0.0005586874904111028\n",
      "Iteration 1951/2000, Loss: 0.00019810297817457467\n",
      "Iteration 1952/2000, Loss: 0.00013960684009362012\n",
      "Iteration 1953/2000, Loss: 0.00015970758977346122\n",
      "Iteration 1954/2000, Loss: 0.0002736737369559705\n",
      "Iteration 1955/2000, Loss: 0.0001945861877175048\n",
      "Iteration 1956/2000, Loss: 0.0001311996456934139\n",
      "Iteration 1957/2000, Loss: 0.0002442961558699608\n",
      "Iteration 1958/2000, Loss: 0.00021879501582589\n",
      "Iteration 1959/2000, Loss: 0.00018535768322180957\n",
      "Iteration 1960/2000, Loss: 0.00021702985395677388\n",
      "Iteration 1961/2000, Loss: 0.00020081375259906054\n",
      "Iteration 1962/2000, Loss: 0.0002257295127492398\n",
      "Iteration 1963/2000, Loss: 0.00022533941955771297\n",
      "Iteration 1964/2000, Loss: 0.0002336147299502045\n",
      "Iteration 1965/2000, Loss: 0.00012974200944881886\n",
      "Iteration 1966/2000, Loss: 0.00020805651729460806\n",
      "Iteration 1967/2000, Loss: 0.00017777015455067158\n",
      "Iteration 1968/2000, Loss: 0.00018071403610520065\n",
      "Iteration 1969/2000, Loss: 0.00014552824723068625\n",
      "Iteration 1970/2000, Loss: 0.0002072817733278498\n",
      "Iteration 1971/2000, Loss: 0.00013398464943747967\n",
      "Iteration 1972/2000, Loss: 0.00014936509251128882\n",
      "Iteration 1973/2000, Loss: 0.00012837689428124577\n",
      "Iteration 1974/2000, Loss: 0.00012971334217581898\n",
      "Iteration 1975/2000, Loss: 0.00015715634799562395\n",
      "Iteration 1976/2000, Loss: 0.00011984303273493424\n",
      "Iteration 1977/2000, Loss: 0.00012405253073666245\n",
      "Iteration 1978/2000, Loss: 0.0001235971722053364\n",
      "Iteration 1979/2000, Loss: 0.00020726681395899504\n",
      "Iteration 1980/2000, Loss: 0.00012395993690006435\n",
      "Iteration 1981/2000, Loss: 0.0001484503736719489\n",
      "Iteration 1982/2000, Loss: 0.00016334702377207577\n",
      "Iteration 1983/2000, Loss: 0.0001443431683583185\n",
      "Iteration 1984/2000, Loss: 0.0001279281423194334\n",
      "Iteration 1985/2000, Loss: 0.00025580302462913096\n",
      "Iteration 1986/2000, Loss: 0.00019954524759668857\n",
      "Iteration 1987/2000, Loss: 0.00013100841897539794\n",
      "Iteration 1988/2000, Loss: 0.00024227618996519595\n",
      "Iteration 1989/2000, Loss: 0.0004664114094339311\n",
      "Iteration 1990/2000, Loss: 0.00017677743744570762\n",
      "Iteration 1991/2000, Loss: 0.0003656986227724701\n",
      "Iteration 1992/2000, Loss: 0.00020299074822105467\n",
      "Iteration 1993/2000, Loss: 0.0001814667193684727\n",
      "Iteration 1994/2000, Loss: 0.00013720068091060966\n",
      "Iteration 1995/2000, Loss: 0.00018034374807029963\n",
      "Iteration 1996/2000, Loss: 0.00018195722077507526\n",
      "Iteration 1997/2000, Loss: 0.00015358033124357462\n",
      "Iteration 1998/2000, Loss: 0.0001959111395990476\n",
      "Iteration 1999/2000, Loss: 0.00016171502647921443\n",
      "Iteration 2000/2000, Loss: 0.00021435304370243102\n",
      "Model weights saved after training and testing with linewidth 100000.0 Hz.\n",
      "\n",
      "\n",
      "Testing with linewidth: 100000.0 Hz and Distance: 4000.0 km\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Testing MSE - Linewidth: 100000.0, Link Distance: 4000.0, Original: 0.005705490242689848, Neural Network: 0.005117381457239389\n",
      "\n",
      "Training with linewidth: 100000.0 Hz and Distance: 5000.0 km\n",
      "Iteration 1/2000, Loss: 0.00011208871728740633\n",
      "Iteration 2/2000, Loss: 0.00043665943667292595\n",
      "Iteration 3/2000, Loss: 0.00018362264381721616\n",
      "Iteration 4/2000, Loss: 0.0001881891948869452\n",
      "Iteration 5/2000, Loss: 0.0001865835947683081\n",
      "Iteration 6/2000, Loss: 8.792157314019278e-05\n",
      "Iteration 7/2000, Loss: 0.0002502947172615677\n",
      "Iteration 8/2000, Loss: 0.00012218795018270612\n",
      "Iteration 9/2000, Loss: 0.0001668488694122061\n",
      "Iteration 10/2000, Loss: 0.0001364995405310765\n",
      "Iteration 11/2000, Loss: 0.0002648509980645031\n",
      "Iteration 12/2000, Loss: 0.00019585223344620317\n",
      "Iteration 13/2000, Loss: 0.00015552862896583974\n",
      "Iteration 14/2000, Loss: 0.00018240480858366936\n",
      "Iteration 15/2000, Loss: 0.00021586894581560045\n",
      "Iteration 16/2000, Loss: 0.0002644246560521424\n",
      "Iteration 17/2000, Loss: 0.0002669004607014358\n",
      "Iteration 18/2000, Loss: 0.00023301804321818054\n",
      "Iteration 19/2000, Loss: 0.00019127997802570462\n",
      "Iteration 20/2000, Loss: 0.0001502888189861551\n",
      "Iteration 21/2000, Loss: 0.00022365596669260412\n",
      "Iteration 22/2000, Loss: 0.00015555458958260715\n",
      "Iteration 23/2000, Loss: 0.00020521823898889124\n",
      "Iteration 24/2000, Loss: 0.00019121720106340945\n",
      "Iteration 25/2000, Loss: 0.00021965270570944995\n",
      "Iteration 26/2000, Loss: 0.00012724717089440674\n",
      "Iteration 27/2000, Loss: 0.00022366733173839748\n",
      "Iteration 28/2000, Loss: 0.0001090131772798486\n",
      "Iteration 29/2000, Loss: 0.00020858741481788456\n",
      "Iteration 30/2000, Loss: 9.837545803748071e-05\n",
      "Iteration 31/2000, Loss: 0.0003338928800076246\n",
      "Iteration 32/2000, Loss: 0.00028229941381141543\n",
      "Iteration 33/2000, Loss: 0.0003102920891251415\n",
      "Iteration 34/2000, Loss: 0.000132547109387815\n",
      "Iteration 35/2000, Loss: 0.00011676385474856943\n",
      "Iteration 36/2000, Loss: 0.00018600332259666175\n",
      "Iteration 37/2000, Loss: 0.00018027413170784712\n",
      "Iteration 38/2000, Loss: 0.00016587949357926846\n",
      "Iteration 39/2000, Loss: 0.00018733556498773396\n",
      "Iteration 40/2000, Loss: 0.000157800386659801\n",
      "Iteration 41/2000, Loss: 0.00040735420770943165\n",
      "Iteration 42/2000, Loss: 0.00015192670980468392\n",
      "Iteration 43/2000, Loss: 0.00015978162991814315\n",
      "Iteration 44/2000, Loss: 0.0004322676104493439\n",
      "Iteration 45/2000, Loss: 0.00014888752775732428\n",
      "Iteration 46/2000, Loss: 0.00022388569777831435\n",
      "Iteration 47/2000, Loss: 0.00017855646729003638\n",
      "Iteration 48/2000, Loss: 0.00032884429674595594\n",
      "Iteration 49/2000, Loss: 0.0001595135690877214\n",
      "Iteration 50/2000, Loss: 0.00020025958656333387\n",
      "Iteration 51/2000, Loss: 0.0002617094141896814\n",
      "Iteration 52/2000, Loss: 0.00013055767340119928\n",
      "Iteration 53/2000, Loss: 0.0003523917403072119\n",
      "Iteration 54/2000, Loss: 0.00016375165432691574\n",
      "Iteration 55/2000, Loss: 0.00027860584668815136\n",
      "Iteration 56/2000, Loss: 0.0002816816559061408\n",
      "Iteration 57/2000, Loss: 0.00026637024711817503\n",
      "Iteration 58/2000, Loss: 0.0001873146538855508\n",
      "Iteration 59/2000, Loss: 0.00022447190713137388\n",
      "Iteration 60/2000, Loss: 0.00020614132517948747\n",
      "Iteration 61/2000, Loss: 0.00016618157678749412\n",
      "Iteration 62/2000, Loss: 0.0001619896647753194\n",
      "Iteration 63/2000, Loss: 0.00020771680283360183\n",
      "Iteration 64/2000, Loss: 0.0002541610738262534\n",
      "Iteration 65/2000, Loss: 0.00023030400916468352\n",
      "Iteration 66/2000, Loss: 0.00014846515841782093\n",
      "Iteration 67/2000, Loss: 0.00020328302343841642\n",
      "Iteration 68/2000, Loss: 0.00024848984321579337\n",
      "Iteration 69/2000, Loss: 0.0002895684738177806\n",
      "Iteration 70/2000, Loss: 0.0004674411902669817\n",
      "Iteration 71/2000, Loss: 0.00019612527103163302\n",
      "Iteration 72/2000, Loss: 0.00037535553565248847\n",
      "Iteration 73/2000, Loss: 0.0002758371701929718\n",
      "Iteration 74/2000, Loss: 0.0002431118191452697\n",
      "Iteration 75/2000, Loss: 0.0004968770081177354\n",
      "Iteration 76/2000, Loss: 0.00026163316215388477\n",
      "Iteration 77/2000, Loss: 0.00012316004722379148\n",
      "Iteration 78/2000, Loss: 0.00029455419280566275\n",
      "Iteration 79/2000, Loss: 0.00013497429608833045\n",
      "Iteration 80/2000, Loss: 0.00016761236474849284\n",
      "Iteration 81/2000, Loss: 0.00019189792510587722\n",
      "Iteration 82/2000, Loss: 0.00014110097254160792\n",
      "Iteration 83/2000, Loss: 0.00018013000953942537\n",
      "Iteration 84/2000, Loss: 0.00017767914687283337\n",
      "Iteration 85/2000, Loss: 0.00013001901970710605\n",
      "Iteration 86/2000, Loss: 0.0001846136146923527\n",
      "Iteration 87/2000, Loss: 0.00013343010505195707\n",
      "Iteration 88/2000, Loss: 0.00014867703430354595\n",
      "Iteration 89/2000, Loss: 0.00021463618031702936\n",
      "Iteration 90/2000, Loss: 0.00021521534654311836\n",
      "Iteration 91/2000, Loss: 9.412279177922755e-05\n",
      "Iteration 92/2000, Loss: 0.00013402150943875313\n",
      "Iteration 93/2000, Loss: 0.00022143835667520761\n",
      "Iteration 94/2000, Loss: 0.0001597130612935871\n",
      "Iteration 95/2000, Loss: 0.0002260859910165891\n",
      "Iteration 96/2000, Loss: 0.00015446072211489081\n",
      "Iteration 97/2000, Loss: 0.00019258211250416934\n",
      "Iteration 98/2000, Loss: 0.0001228759065270424\n",
      "Iteration 99/2000, Loss: 0.0001542291574878618\n",
      "Iteration 100/2000, Loss: 0.00016579331713728607\n",
      "Iteration 101/2000, Loss: 0.00017570570344105363\n",
      "Iteration 102/2000, Loss: 0.0001465626119170338\n",
      "Iteration 103/2000, Loss: 0.0003696272906381637\n",
      "Iteration 104/2000, Loss: 0.00018868863116949797\n",
      "Iteration 105/2000, Loss: 0.00015065629850141704\n",
      "Iteration 106/2000, Loss: 0.00029558874666690826\n",
      "Iteration 107/2000, Loss: 0.00018657997134141624\n",
      "Iteration 108/2000, Loss: 0.0001681459543760866\n",
      "Iteration 109/2000, Loss: 0.0001870277337729931\n",
      "Iteration 110/2000, Loss: 0.00016705876623746008\n",
      "Iteration 111/2000, Loss: 0.0001556346396682784\n",
      "Iteration 112/2000, Loss: 0.00025302288122475147\n",
      "Iteration 113/2000, Loss: 0.0002243553171865642\n",
      "Iteration 114/2000, Loss: 0.00014039386587683111\n",
      "Iteration 115/2000, Loss: 0.0003063729382120073\n",
      "Iteration 116/2000, Loss: 0.00040981482015922666\n",
      "Iteration 117/2000, Loss: 0.0002820306981448084\n",
      "Iteration 118/2000, Loss: 0.00022854184499010444\n",
      "Iteration 119/2000, Loss: 0.0001977833017008379\n",
      "Iteration 120/2000, Loss: 0.00026881397934630513\n",
      "Iteration 121/2000, Loss: 0.0001429577387170866\n",
      "Iteration 122/2000, Loss: 0.00024209580442402512\n",
      "Iteration 123/2000, Loss: 0.00017148797633126378\n",
      "Iteration 124/2000, Loss: 0.0003489537339191884\n",
      "Iteration 125/2000, Loss: 0.00018015764362644404\n",
      "Iteration 126/2000, Loss: 0.00033947217161767185\n",
      "Iteration 127/2000, Loss: 0.00032651625224389136\n",
      "Iteration 128/2000, Loss: 0.00015293747128453106\n",
      "Iteration 129/2000, Loss: 0.00034184043761342764\n",
      "Iteration 130/2000, Loss: 0.00012027526827296242\n",
      "Iteration 131/2000, Loss: 0.00019447431259322912\n",
      "Iteration 132/2000, Loss: 0.0001377399021293968\n",
      "Iteration 133/2000, Loss: 0.00019720967975445092\n",
      "Iteration 134/2000, Loss: 0.00012755912030115724\n",
      "Iteration 135/2000, Loss: 0.0001315773552050814\n",
      "Iteration 136/2000, Loss: 0.0001609897444723174\n",
      "Iteration 137/2000, Loss: 0.00021538989676628262\n",
      "Iteration 138/2000, Loss: 0.00033248704858124256\n",
      "Iteration 139/2000, Loss: 0.00012056405830662698\n",
      "Iteration 140/2000, Loss: 0.00012152326235082\n",
      "Iteration 141/2000, Loss: 0.00014443833788391203\n",
      "Iteration 142/2000, Loss: 0.00020194034732412547\n",
      "Iteration 143/2000, Loss: 0.00013635454524774104\n",
      "Iteration 144/2000, Loss: 0.00016157061327248812\n",
      "Iteration 145/2000, Loss: 0.00017424488032702357\n",
      "Iteration 146/2000, Loss: 0.00022017917945049703\n",
      "Iteration 147/2000, Loss: 0.0002717929019127041\n",
      "Iteration 148/2000, Loss: 0.0001880106865428388\n",
      "Iteration 149/2000, Loss: 0.0002528266631998122\n",
      "Iteration 150/2000, Loss: 0.00025055426522158086\n",
      "Iteration 151/2000, Loss: 0.00025200832169502974\n",
      "Iteration 152/2000, Loss: 0.00017999825649894774\n",
      "Iteration 153/2000, Loss: 0.0001372581027681008\n",
      "Iteration 154/2000, Loss: 0.00017366923566441983\n",
      "Iteration 155/2000, Loss: 0.00016479486657772213\n",
      "Iteration 156/2000, Loss: 0.0003560443001333624\n",
      "Iteration 157/2000, Loss: 0.0002884080749936402\n",
      "Iteration 158/2000, Loss: 0.00019408845400903374\n",
      "Iteration 159/2000, Loss: 0.00018763648404274136\n",
      "Iteration 160/2000, Loss: 0.00020597416732925922\n",
      "Iteration 161/2000, Loss: 0.00021743762772530317\n",
      "Iteration 162/2000, Loss: 0.00019836473802570254\n",
      "Iteration 163/2000, Loss: 0.00017338187899440527\n",
      "Iteration 164/2000, Loss: 0.0002523621660657227\n",
      "Iteration 165/2000, Loss: 0.0005204947665333748\n",
      "Iteration 166/2000, Loss: 0.0003945511707570404\n",
      "Iteration 167/2000, Loss: 0.00043474091216921806\n",
      "Iteration 168/2000, Loss: 0.00031842055614106357\n",
      "Iteration 169/2000, Loss: 0.00042997716809622943\n",
      "Iteration 170/2000, Loss: 0.0003423140151426196\n",
      "Iteration 171/2000, Loss: 0.0004313260142225772\n",
      "Iteration 172/2000, Loss: 0.00024537742137908936\n",
      "Iteration 173/2000, Loss: 0.00044384063221514225\n",
      "Iteration 174/2000, Loss: 0.00028899862081743777\n",
      "Iteration 175/2000, Loss: 0.0003814634692389518\n",
      "Iteration 176/2000, Loss: 0.0004363661864772439\n",
      "Iteration 177/2000, Loss: 0.00014352539437822998\n",
      "Iteration 178/2000, Loss: 0.0003981369372922927\n",
      "Iteration 179/2000, Loss: 0.00015944988990668207\n",
      "Iteration 180/2000, Loss: 0.0004234368389006704\n",
      "Iteration 181/2000, Loss: 0.0002607597562018782\n",
      "Iteration 182/2000, Loss: 0.001295217894949019\n",
      "Iteration 183/2000, Loss: 0.0001892830041470006\n",
      "Iteration 184/2000, Loss: 0.0005955024971626699\n",
      "Iteration 185/2000, Loss: 0.0002503891009837389\n",
      "Iteration 186/2000, Loss: 0.00020485123968683183\n",
      "Iteration 187/2000, Loss: 0.00045998310088180006\n",
      "Iteration 188/2000, Loss: 0.0001722701999824494\n",
      "Iteration 189/2000, Loss: 0.00043411197839304805\n",
      "Iteration 190/2000, Loss: 0.000492110732011497\n",
      "Iteration 191/2000, Loss: 0.0004104262334294617\n",
      "Iteration 192/2000, Loss: 0.00023446304840035737\n",
      "Iteration 193/2000, Loss: 0.0005042787524871528\n",
      "Iteration 194/2000, Loss: 0.0004542878014035523\n",
      "Iteration 195/2000, Loss: 0.00019932871509809047\n",
      "Iteration 196/2000, Loss: 0.0002671346010174602\n",
      "Iteration 197/2000, Loss: 0.0002900094841606915\n",
      "Iteration 198/2000, Loss: 0.00018628797261044383\n",
      "Iteration 199/2000, Loss: 0.0003536705917213112\n",
      "Iteration 200/2000, Loss: 0.00028352299705147743\n",
      "Iteration 201/2000, Loss: 0.00024796059005893767\n",
      "Iteration 202/2000, Loss: 0.00029407849069684744\n",
      "Iteration 203/2000, Loss: 0.00017978415417019278\n",
      "Iteration 204/2000, Loss: 0.00021138110605534166\n",
      "Iteration 205/2000, Loss: 0.0003053137334063649\n",
      "Iteration 206/2000, Loss: 0.00036441755946725607\n",
      "Iteration 207/2000, Loss: 0.00020421417139004916\n",
      "Iteration 208/2000, Loss: 0.00022092887957114726\n",
      "Iteration 209/2000, Loss: 0.0004727965861093253\n",
      "Iteration 210/2000, Loss: 0.00027546219644136727\n",
      "Iteration 211/2000, Loss: 0.00031582292285747826\n",
      "Iteration 212/2000, Loss: 0.0002112418442266062\n",
      "Iteration 213/2000, Loss: 0.0002233535487903282\n",
      "Iteration 214/2000, Loss: 0.00026969987084157765\n",
      "Iteration 215/2000, Loss: 0.00021113100228831172\n",
      "Iteration 216/2000, Loss: 0.0004510720609687269\n",
      "Iteration 217/2000, Loss: 0.00021587091032415628\n",
      "Iteration 218/2000, Loss: 0.00016287672042381018\n",
      "Iteration 219/2000, Loss: 0.00010424525680718943\n",
      "Iteration 220/2000, Loss: 0.0002375267940806225\n",
      "Iteration 221/2000, Loss: 0.00025838316651061177\n",
      "Iteration 222/2000, Loss: 0.00021158184972591698\n",
      "Iteration 223/2000, Loss: 0.00014057137013878673\n",
      "Iteration 224/2000, Loss: 0.0001297969720326364\n",
      "Iteration 225/2000, Loss: 0.0002350277209188789\n",
      "Iteration 226/2000, Loss: 0.00023524854623246938\n",
      "Iteration 227/2000, Loss: 0.00021963661129120737\n",
      "Iteration 228/2000, Loss: 0.0001842756028054282\n",
      "Iteration 229/2000, Loss: 0.00016929104458540678\n",
      "Iteration 230/2000, Loss: 0.00011077066301368177\n",
      "Iteration 231/2000, Loss: 0.00015521151362918317\n",
      "Iteration 232/2000, Loss: 0.0001401790214003995\n",
      "Iteration 233/2000, Loss: 0.00025849873782135546\n",
      "Iteration 234/2000, Loss: 0.0007120200898498297\n",
      "Iteration 235/2000, Loss: 0.00021309340081643313\n",
      "Iteration 236/2000, Loss: 0.0001454781013308093\n",
      "Iteration 237/2000, Loss: 0.00023314738064073026\n",
      "Iteration 238/2000, Loss: 0.00017875159392133355\n",
      "Iteration 239/2000, Loss: 0.00020646207849495113\n",
      "Iteration 240/2000, Loss: 0.00022331011132337153\n",
      "Iteration 241/2000, Loss: 0.0002700712066143751\n",
      "Iteration 242/2000, Loss: 0.00019091853755526245\n",
      "Iteration 243/2000, Loss: 0.00025378793361596763\n",
      "Iteration 244/2000, Loss: 8.798553608357906e-05\n",
      "Iteration 245/2000, Loss: 0.00028735879459418356\n",
      "Iteration 246/2000, Loss: 0.00022065258235670626\n",
      "Iteration 247/2000, Loss: 0.00022695741790812463\n",
      "Iteration 248/2000, Loss: 0.00022743706358596683\n",
      "Iteration 249/2000, Loss: 0.0003146627568639815\n",
      "Iteration 250/2000, Loss: 0.00011246422218391672\n",
      "Iteration 251/2000, Loss: 0.00030572598916478455\n",
      "Iteration 252/2000, Loss: 0.00013831767137162387\n",
      "Iteration 253/2000, Loss: 0.0002782992087304592\n",
      "Iteration 254/2000, Loss: 0.0001820512261474505\n",
      "Iteration 255/2000, Loss: 0.00018317124340683222\n",
      "Iteration 256/2000, Loss: 0.00012547714868560433\n",
      "Iteration 257/2000, Loss: 0.00038311450043693185\n",
      "Iteration 258/2000, Loss: 0.00045348377898335457\n",
      "Iteration 259/2000, Loss: 0.00012268991849850863\n",
      "Iteration 260/2000, Loss: 0.0002482539275661111\n",
      "Iteration 261/2000, Loss: 0.0001706034818198532\n",
      "Iteration 262/2000, Loss: 0.00022839599114377052\n",
      "Iteration 263/2000, Loss: 0.0002919737307820469\n",
      "Iteration 264/2000, Loss: 0.00017377632320858538\n",
      "Iteration 265/2000, Loss: 0.0002023735287366435\n",
      "Iteration 266/2000, Loss: 0.0001869956322479993\n",
      "Iteration 267/2000, Loss: 0.00011812050070147961\n",
      "Iteration 268/2000, Loss: 0.0002237840963061899\n",
      "Iteration 269/2000, Loss: 0.00020970280456822366\n",
      "Iteration 270/2000, Loss: 0.00025164204998873174\n",
      "Iteration 271/2000, Loss: 0.0001694223319645971\n",
      "Iteration 272/2000, Loss: 0.000213112827623263\n",
      "Iteration 273/2000, Loss: 0.000256681494647637\n",
      "Iteration 274/2000, Loss: 0.00031967394170351326\n",
      "Iteration 275/2000, Loss: 0.00017541172564961016\n",
      "Iteration 276/2000, Loss: 0.00013964988465886563\n",
      "Iteration 277/2000, Loss: 0.0002581266453489661\n",
      "Iteration 278/2000, Loss: 0.0002455548965372145\n",
      "Iteration 279/2000, Loss: 0.00014766845561098307\n",
      "Iteration 280/2000, Loss: 0.00015859196719247848\n",
      "Iteration 281/2000, Loss: 0.00015382032142952085\n",
      "Iteration 282/2000, Loss: 0.00028791133081540465\n",
      "Iteration 283/2000, Loss: 0.00025221734540537\n",
      "Iteration 284/2000, Loss: 0.00017137927352450788\n",
      "Iteration 285/2000, Loss: 0.00019540693028829992\n",
      "Iteration 286/2000, Loss: 0.0001114171973313205\n",
      "Iteration 287/2000, Loss: 0.00011437280045356601\n",
      "Iteration 288/2000, Loss: 0.00014224817277863622\n",
      "Iteration 289/2000, Loss: 0.00015652761794626713\n",
      "Iteration 290/2000, Loss: 0.00013080821372568607\n",
      "Iteration 291/2000, Loss: 0.00025269013713113964\n",
      "Iteration 292/2000, Loss: 0.00018432199431117624\n",
      "Iteration 293/2000, Loss: 0.0002676969161257148\n",
      "Iteration 294/2000, Loss: 0.0002886683796532452\n",
      "Iteration 295/2000, Loss: 0.00014081479457672685\n",
      "Iteration 296/2000, Loss: 0.0001417465682607144\n",
      "Iteration 297/2000, Loss: 0.00020105922885704786\n",
      "Iteration 298/2000, Loss: 9.014930401463062e-05\n",
      "Iteration 299/2000, Loss: 0.00017835537437349558\n",
      "Iteration 300/2000, Loss: 0.00029083326808176935\n",
      "Iteration 301/2000, Loss: 0.0004500719078350812\n",
      "Iteration 302/2000, Loss: 0.00012710430019069463\n",
      "Iteration 303/2000, Loss: 0.00019006518414244056\n",
      "Iteration 304/2000, Loss: 0.00013774287072010338\n",
      "Iteration 305/2000, Loss: 0.00019136740593239665\n",
      "Iteration 306/2000, Loss: 0.00011552724754437804\n",
      "Iteration 307/2000, Loss: 0.0001059432397596538\n",
      "Iteration 308/2000, Loss: 0.0001715768885333091\n",
      "Iteration 309/2000, Loss: 0.00023801549104973674\n",
      "Iteration 310/2000, Loss: 0.00021833846403751522\n",
      "Iteration 311/2000, Loss: 0.00022584102407563478\n",
      "Iteration 312/2000, Loss: 0.00015802560665179044\n",
      "Iteration 313/2000, Loss: 0.00014795119932387024\n",
      "Iteration 314/2000, Loss: 0.00016458025493193418\n",
      "Iteration 315/2000, Loss: 0.0001659062982071191\n",
      "Iteration 316/2000, Loss: 0.00012363366840872914\n",
      "Iteration 317/2000, Loss: 0.00017625131295062602\n",
      "Iteration 318/2000, Loss: 0.000231144018471241\n",
      "Iteration 319/2000, Loss: 0.00019071610586252064\n",
      "Iteration 320/2000, Loss: 0.00012870997306890786\n",
      "Iteration 321/2000, Loss: 0.00022208952577784657\n",
      "Iteration 322/2000, Loss: 0.00026305275969207287\n",
      "Iteration 323/2000, Loss: 0.00015385114238597453\n",
      "Iteration 324/2000, Loss: 0.0001439684710931033\n",
      "Iteration 325/2000, Loss: 0.00015268185234162956\n",
      "Iteration 326/2000, Loss: 0.00020220718579366803\n",
      "Iteration 327/2000, Loss: 0.00019152338791172951\n",
      "Iteration 328/2000, Loss: 0.0001416680170223117\n",
      "Iteration 329/2000, Loss: 0.00017711370310280472\n",
      "Iteration 330/2000, Loss: 0.0002514413499739021\n",
      "Iteration 331/2000, Loss: 0.00015160134353209287\n",
      "Iteration 332/2000, Loss: 0.00014716287842020392\n",
      "Iteration 333/2000, Loss: 0.00013779709115624428\n",
      "Iteration 334/2000, Loss: 0.0003022141463588923\n",
      "Iteration 335/2000, Loss: 0.00015800468099769205\n",
      "Iteration 336/2000, Loss: 0.0005006259889341891\n",
      "Iteration 337/2000, Loss: 0.00037320275441743433\n",
      "Iteration 338/2000, Loss: 0.00022989630815573037\n",
      "Iteration 339/2000, Loss: 0.00041996515938080847\n",
      "Iteration 340/2000, Loss: 0.0002329574344912544\n",
      "Iteration 341/2000, Loss: 0.0005951934144832194\n",
      "Iteration 342/2000, Loss: 0.0003249532310292125\n",
      "Iteration 343/2000, Loss: 0.0002612930547911674\n",
      "Iteration 344/2000, Loss: 0.0002517565153539181\n",
      "Iteration 345/2000, Loss: 0.00020883737306576222\n",
      "Iteration 346/2000, Loss: 0.00034151197178289294\n",
      "Iteration 347/2000, Loss: 0.00026569064357317984\n",
      "Iteration 348/2000, Loss: 0.0004059585917275399\n",
      "Iteration 349/2000, Loss: 0.00021224579541012645\n",
      "Iteration 350/2000, Loss: 0.00021972370450384915\n",
      "Iteration 351/2000, Loss: 0.0004262112488504499\n",
      "Iteration 352/2000, Loss: 0.00012520367454271764\n",
      "Iteration 353/2000, Loss: 0.00015023908053990453\n",
      "Iteration 354/2000, Loss: 0.00029602745780721307\n",
      "Iteration 355/2000, Loss: 0.0002685348445083946\n",
      "Iteration 356/2000, Loss: 0.00016615691129118204\n",
      "Iteration 357/2000, Loss: 0.0002055339573416859\n",
      "Iteration 358/2000, Loss: 0.00019509757112246007\n",
      "Iteration 359/2000, Loss: 0.00029297894798219204\n",
      "Iteration 360/2000, Loss: 0.0005407988210208714\n",
      "Iteration 361/2000, Loss: 0.00015816990344319493\n",
      "Iteration 362/2000, Loss: 0.0005345228128135204\n",
      "Iteration 363/2000, Loss: 0.0005741843488067389\n",
      "Iteration 364/2000, Loss: 0.0002281383058289066\n",
      "Iteration 365/2000, Loss: 0.0003886947815772146\n",
      "Iteration 366/2000, Loss: 0.00021312107855919749\n",
      "Iteration 367/2000, Loss: 0.00018348314915783703\n",
      "Iteration 368/2000, Loss: 0.00024086916528176516\n",
      "Iteration 369/2000, Loss: 0.00022181845270097256\n",
      "Iteration 370/2000, Loss: 0.0002371138980379328\n",
      "Iteration 371/2000, Loss: 0.00018613191787153482\n",
      "Iteration 372/2000, Loss: 0.0003064307093154639\n",
      "Iteration 373/2000, Loss: 0.00048017597873695195\n",
      "Iteration 374/2000, Loss: 0.00021369046589825302\n",
      "Iteration 375/2000, Loss: 0.0003457164275459945\n",
      "Iteration 376/2000, Loss: 0.0002717429888434708\n",
      "Iteration 377/2000, Loss: 0.00021330497111193836\n",
      "Iteration 378/2000, Loss: 0.00021799831301905215\n",
      "Iteration 379/2000, Loss: 0.00026416414766572416\n",
      "Iteration 380/2000, Loss: 0.00012850627535954118\n",
      "Iteration 381/2000, Loss: 0.0006060042069293559\n",
      "Iteration 382/2000, Loss: 0.0004181376425549388\n",
      "Iteration 383/2000, Loss: 0.00017677093273960054\n",
      "Iteration 384/2000, Loss: 0.00037397924461402\n",
      "Iteration 385/2000, Loss: 0.0002647986984811723\n",
      "Iteration 386/2000, Loss: 0.0002973207738250494\n",
      "Iteration 387/2000, Loss: 0.000527822005096823\n",
      "Iteration 388/2000, Loss: 0.00020126646268181503\n",
      "Iteration 389/2000, Loss: 0.00021898314298596233\n",
      "Iteration 390/2000, Loss: 0.00028471063706092536\n",
      "Iteration 391/2000, Loss: 0.00014946064038667828\n",
      "Iteration 392/2000, Loss: 0.00018431454373057932\n",
      "Iteration 393/2000, Loss: 0.0001771369279595092\n",
      "Iteration 394/2000, Loss: 0.00017463484255131334\n",
      "Iteration 395/2000, Loss: 0.00018855344387702644\n",
      "Iteration 396/2000, Loss: 0.000517874606885016\n",
      "Iteration 397/2000, Loss: 0.0001645134179852903\n",
      "Iteration 398/2000, Loss: 0.00031916642910800874\n",
      "Iteration 399/2000, Loss: 0.00023242144379764795\n",
      "Iteration 400/2000, Loss: 0.00024127851065713912\n",
      "Iteration 401/2000, Loss: 0.00028886087238788605\n",
      "Iteration 402/2000, Loss: 0.00012699309445451945\n",
      "Iteration 403/2000, Loss: 0.00024344425764866173\n",
      "Iteration 404/2000, Loss: 0.00012877106200903654\n",
      "Iteration 405/2000, Loss: 0.00026114474167115986\n",
      "Iteration 406/2000, Loss: 0.00023398427583742887\n",
      "Iteration 407/2000, Loss: 0.00021684107196051627\n",
      "Iteration 408/2000, Loss: 0.00015230766439344734\n",
      "Iteration 409/2000, Loss: 0.00024523408501408994\n",
      "Iteration 410/2000, Loss: 0.00021165012731216848\n",
      "Iteration 411/2000, Loss: 0.00016228888125624508\n",
      "Iteration 412/2000, Loss: 0.0001336981076747179\n",
      "Iteration 413/2000, Loss: 0.0003838905831798911\n",
      "Iteration 414/2000, Loss: 0.00018839533731807023\n",
      "Iteration 415/2000, Loss: 0.00013441355258692056\n",
      "Iteration 416/2000, Loss: 0.0001254954986507073\n",
      "Iteration 417/2000, Loss: 0.0001867987884907052\n",
      "Iteration 418/2000, Loss: 0.00024548135115765035\n",
      "Iteration 419/2000, Loss: 0.00016527845582459122\n",
      "Iteration 420/2000, Loss: 0.0001538837095722556\n",
      "Iteration 421/2000, Loss: 9.408502228325233e-05\n",
      "Iteration 422/2000, Loss: 0.00016842079639900476\n",
      "Iteration 423/2000, Loss: 0.00025158782955259085\n",
      "Iteration 424/2000, Loss: 0.00014712447591591626\n",
      "Iteration 425/2000, Loss: 0.00013489734556060284\n",
      "Iteration 426/2000, Loss: 0.00020637339912354946\n",
      "Iteration 427/2000, Loss: 0.00026002980303019285\n",
      "Iteration 428/2000, Loss: 0.00021218416804913431\n",
      "Iteration 429/2000, Loss: 0.00019290017371531576\n",
      "Iteration 430/2000, Loss: 0.00026673104730434716\n",
      "Iteration 431/2000, Loss: 0.00023090525064617395\n",
      "Iteration 432/2000, Loss: 0.00016883231000974774\n",
      "Iteration 433/2000, Loss: 0.00019440021424088627\n",
      "Iteration 434/2000, Loss: 0.0001866519742179662\n",
      "Iteration 435/2000, Loss: 0.0005128827760927379\n",
      "Iteration 436/2000, Loss: 0.00011841885861940682\n",
      "Iteration 437/2000, Loss: 0.00033795341732911766\n",
      "Iteration 438/2000, Loss: 0.00019097226322628558\n",
      "Iteration 439/2000, Loss: 0.00027178446180187166\n",
      "Iteration 440/2000, Loss: 0.00040986519888974726\n",
      "Iteration 441/2000, Loss: 0.00021844754519406706\n",
      "Iteration 442/2000, Loss: 0.00028029177337884903\n",
      "Iteration 443/2000, Loss: 0.0005039687384851277\n",
      "Iteration 444/2000, Loss: 0.00033469105255790055\n",
      "Iteration 445/2000, Loss: 0.0002232341794297099\n",
      "Iteration 446/2000, Loss: 0.0001631612831261009\n",
      "Iteration 447/2000, Loss: 0.0002477961534168571\n",
      "Iteration 448/2000, Loss: 0.0001702636363916099\n",
      "Iteration 449/2000, Loss: 0.0001631210616324097\n",
      "Iteration 450/2000, Loss: 0.00020840304205194116\n",
      "Iteration 451/2000, Loss: 0.0002447135339025408\n",
      "Iteration 452/2000, Loss: 0.00018093510880135\n",
      "Iteration 453/2000, Loss: 0.0001577755028847605\n",
      "Iteration 454/2000, Loss: 0.00031067224335856736\n",
      "Iteration 455/2000, Loss: 0.00024757941719144583\n",
      "Iteration 456/2000, Loss: 0.00034690278698690236\n",
      "Iteration 457/2000, Loss: 0.00023951959155965596\n",
      "Iteration 458/2000, Loss: 0.0003071661340072751\n",
      "Iteration 459/2000, Loss: 0.0002562875743024051\n",
      "Iteration 460/2000, Loss: 0.0003629382117651403\n",
      "Iteration 461/2000, Loss: 0.00037288249586708844\n",
      "Iteration 462/2000, Loss: 0.00013171727187000215\n",
      "Iteration 463/2000, Loss: 0.00038691333611495793\n",
      "Iteration 464/2000, Loss: 0.0001872010761871934\n",
      "Iteration 465/2000, Loss: 0.00018672458827495575\n",
      "Iteration 466/2000, Loss: 0.00014945633301977068\n",
      "Iteration 467/2000, Loss: 0.0001858439645729959\n",
      "Iteration 468/2000, Loss: 0.00032162098796106875\n",
      "Iteration 469/2000, Loss: 0.00016495124145876616\n",
      "Iteration 470/2000, Loss: 0.00020765211957041174\n",
      "Iteration 471/2000, Loss: 0.00030849501490592957\n",
      "Iteration 472/2000, Loss: 0.0003592143184505403\n",
      "Iteration 473/2000, Loss: 0.00028002599719911814\n",
      "Iteration 474/2000, Loss: 0.0003307582810521126\n",
      "Iteration 475/2000, Loss: 0.00022578549396712333\n",
      "Iteration 476/2000, Loss: 0.0002685059153009206\n",
      "Iteration 477/2000, Loss: 0.00010966651461785659\n",
      "Iteration 478/2000, Loss: 0.00022429127420764416\n",
      "Iteration 479/2000, Loss: 0.00020925559510942549\n",
      "Iteration 480/2000, Loss: 0.0004881978966295719\n",
      "Iteration 481/2000, Loss: 0.00028571096481755376\n",
      "Iteration 482/2000, Loss: 0.0001791608810890466\n",
      "Iteration 483/2000, Loss: 0.0002394020848441869\n",
      "Iteration 484/2000, Loss: 0.00020307717204559594\n",
      "Iteration 485/2000, Loss: 0.0002839119697455317\n",
      "Iteration 486/2000, Loss: 0.0001825216313591227\n",
      "Iteration 487/2000, Loss: 0.0002130997454514727\n",
      "Iteration 488/2000, Loss: 0.00061462027952075\n",
      "Iteration 489/2000, Loss: 0.00014621748414356261\n",
      "Iteration 490/2000, Loss: 0.00039395972271449864\n",
      "Iteration 491/2000, Loss: 0.00027864755247719586\n",
      "Iteration 492/2000, Loss: 0.00019090283603873104\n",
      "Iteration 493/2000, Loss: 0.00025719456607475877\n",
      "Iteration 494/2000, Loss: 0.00023010566656012088\n",
      "Iteration 495/2000, Loss: 0.0002725639205891639\n",
      "Iteration 496/2000, Loss: 0.00022511430142913014\n",
      "Iteration 497/2000, Loss: 0.00033925383468158543\n",
      "Iteration 498/2000, Loss: 0.00037808698834851384\n",
      "Iteration 499/2000, Loss: 0.00028752879006788135\n",
      "Iteration 500/2000, Loss: 0.00016264125588349998\n",
      "Iteration 501/2000, Loss: 0.0002929752808995545\n",
      "Iteration 502/2000, Loss: 0.0002720274496823549\n",
      "Iteration 503/2000, Loss: 0.00015423636068589985\n",
      "Iteration 504/2000, Loss: 0.0002293014549650252\n",
      "Iteration 505/2000, Loss: 0.00023604197485838085\n",
      "Iteration 506/2000, Loss: 0.00019631953909993172\n",
      "Iteration 507/2000, Loss: 0.00020462210522964597\n",
      "Iteration 508/2000, Loss: 0.00026509197778068483\n",
      "Iteration 509/2000, Loss: 0.0001574484194861725\n",
      "Iteration 510/2000, Loss: 0.0002831153979059309\n",
      "Iteration 511/2000, Loss: 0.0002458694507367909\n",
      "Iteration 512/2000, Loss: 0.0003442611778154969\n",
      "Iteration 513/2000, Loss: 0.0003119170432910323\n",
      "Iteration 514/2000, Loss: 0.00025811803061515093\n",
      "Iteration 515/2000, Loss: 0.00017663792823441327\n",
      "Iteration 516/2000, Loss: 0.0002632175455801189\n",
      "Iteration 517/2000, Loss: 0.00033111401717178524\n",
      "Iteration 518/2000, Loss: 0.0003922680043615401\n",
      "Iteration 519/2000, Loss: 0.0002015448990277946\n",
      "Iteration 520/2000, Loss: 0.0002237989247078076\n",
      "Iteration 521/2000, Loss: 0.0002983252634294331\n",
      "Iteration 522/2000, Loss: 0.00032363191712647676\n",
      "Iteration 523/2000, Loss: 0.00025706092128530145\n",
      "Iteration 524/2000, Loss: 0.00021635145822074264\n",
      "Iteration 525/2000, Loss: 0.00023459389922209084\n",
      "Iteration 526/2000, Loss: 0.0002693510614335537\n",
      "Iteration 527/2000, Loss: 0.0002808722492773086\n",
      "Iteration 528/2000, Loss: 0.00020884571131318808\n",
      "Iteration 529/2000, Loss: 0.0001613711065147072\n",
      "Iteration 530/2000, Loss: 0.00014936209481675178\n",
      "Iteration 531/2000, Loss: 0.000279177853371948\n",
      "Iteration 532/2000, Loss: 0.0002687503001652658\n",
      "Iteration 533/2000, Loss: 0.0001687618059804663\n",
      "Iteration 534/2000, Loss: 0.00024673715233802795\n",
      "Iteration 535/2000, Loss: 0.00011061404802603647\n",
      "Iteration 536/2000, Loss: 0.00032403296791017056\n",
      "Iteration 537/2000, Loss: 0.0002582343004178256\n",
      "Iteration 538/2000, Loss: 0.00020074557687621564\n",
      "Iteration 539/2000, Loss: 0.0002356099575990811\n",
      "Iteration 540/2000, Loss: 0.0002329496928723529\n",
      "Iteration 541/2000, Loss: 0.00016553123714402318\n",
      "Iteration 542/2000, Loss: 9.593514550942928e-05\n",
      "Iteration 543/2000, Loss: 0.00024690700229257345\n",
      "Iteration 544/2000, Loss: 0.0001199610487674363\n",
      "Iteration 545/2000, Loss: 0.0002538984699640423\n",
      "Iteration 546/2000, Loss: 0.0001952064922079444\n",
      "Iteration 547/2000, Loss: 0.000321789993904531\n",
      "Iteration 548/2000, Loss: 0.0001849128311732784\n",
      "Iteration 549/2000, Loss: 0.00020080215472262353\n",
      "Iteration 550/2000, Loss: 0.0002748666738625616\n",
      "Iteration 551/2000, Loss: 0.0003549735702108592\n",
      "Iteration 552/2000, Loss: 0.00032581144478172064\n",
      "Iteration 553/2000, Loss: 0.00016362273890990764\n",
      "Iteration 554/2000, Loss: 0.00039804415428079665\n",
      "Iteration 555/2000, Loss: 0.00014514628855977207\n",
      "Iteration 556/2000, Loss: 0.00040042505133897066\n",
      "Iteration 557/2000, Loss: 0.00019558028725441545\n",
      "Iteration 558/2000, Loss: 0.0002399239019723609\n",
      "Iteration 559/2000, Loss: 0.0002957309188786894\n",
      "Iteration 560/2000, Loss: 0.00026088563026860356\n",
      "Iteration 561/2000, Loss: 0.00020670999947469682\n",
      "Iteration 562/2000, Loss: 0.0001261027791770175\n",
      "Iteration 563/2000, Loss: 0.00016977048653643578\n",
      "Iteration 564/2000, Loss: 0.0002841551322489977\n",
      "Iteration 565/2000, Loss: 0.00041549233719706535\n",
      "Iteration 566/2000, Loss: 0.0002711524721235037\n",
      "Iteration 567/2000, Loss: 0.00026040663942694664\n",
      "Iteration 568/2000, Loss: 0.0001660006819292903\n",
      "Iteration 569/2000, Loss: 0.00022396135318558663\n",
      "Iteration 570/2000, Loss: 0.00035921510425396264\n",
      "Iteration 571/2000, Loss: 0.0003440537548158318\n",
      "Iteration 572/2000, Loss: 0.000261828739894554\n",
      "Iteration 573/2000, Loss: 0.0002355945180170238\n",
      "Iteration 574/2000, Loss: 0.00012917190906591713\n",
      "Iteration 575/2000, Loss: 0.0002890320320148021\n",
      "Iteration 576/2000, Loss: 0.0002148996281903237\n",
      "Iteration 577/2000, Loss: 0.0002445638820063323\n",
      "Iteration 578/2000, Loss: 0.0003273560432717204\n",
      "Iteration 579/2000, Loss: 0.00019402419275138527\n",
      "Iteration 580/2000, Loss: 0.0007384468917734921\n",
      "Iteration 581/2000, Loss: 0.00025522385840304196\n",
      "Iteration 582/2000, Loss: 0.0007314225658774376\n",
      "Iteration 583/2000, Loss: 0.00035557764931581914\n",
      "Iteration 584/2000, Loss: 0.00017939743702299893\n",
      "Iteration 585/2000, Loss: 0.0003040644805878401\n",
      "Iteration 586/2000, Loss: 0.00018761841056402773\n",
      "Iteration 587/2000, Loss: 0.00029016504413448274\n",
      "Iteration 588/2000, Loss: 0.00013286503963172436\n",
      "Iteration 589/2000, Loss: 0.00028703725547529757\n",
      "Iteration 590/2000, Loss: 0.00017682461475487798\n",
      "Iteration 591/2000, Loss: 0.00031669222516939044\n",
      "Iteration 592/2000, Loss: 0.0004905549576506019\n",
      "Iteration 593/2000, Loss: 0.000217790890019387\n",
      "Iteration 594/2000, Loss: 0.0002338216727366671\n",
      "Iteration 595/2000, Loss: 0.00015579728642478585\n",
      "Iteration 596/2000, Loss: 0.000254565617069602\n",
      "Iteration 597/2000, Loss: 0.00015606990200467408\n",
      "Iteration 598/2000, Loss: 0.0003275679482612759\n",
      "Iteration 599/2000, Loss: 0.00039641745388507843\n",
      "Iteration 600/2000, Loss: 0.00012687890557572246\n",
      "Iteration 601/2000, Loss: 0.0003742070111911744\n",
      "Iteration 602/2000, Loss: 0.00026923505356535316\n",
      "Iteration 603/2000, Loss: 0.0006686360575258732\n",
      "Iteration 604/2000, Loss: 0.0002543608716223389\n",
      "Iteration 605/2000, Loss: 0.0004130746820010245\n",
      "Iteration 606/2000, Loss: 0.00040185751277022064\n",
      "Iteration 607/2000, Loss: 0.0004479339113458991\n",
      "Iteration 608/2000, Loss: 0.0005117720575071871\n",
      "Iteration 609/2000, Loss: 0.0002184116601711139\n",
      "Iteration 610/2000, Loss: 0.0004309140203986317\n",
      "Iteration 611/2000, Loss: 0.00021992376423440874\n",
      "Iteration 612/2000, Loss: 0.0007271915674209595\n",
      "Iteration 613/2000, Loss: 0.00010871302947634831\n",
      "Iteration 614/2000, Loss: 0.0006833629449829459\n",
      "Iteration 615/2000, Loss: 0.0001283129386138171\n",
      "Iteration 616/2000, Loss: 0.00033117993734776974\n",
      "Iteration 617/2000, Loss: 0.00024033163208514452\n",
      "Iteration 618/2000, Loss: 0.00017449977167416364\n",
      "Iteration 619/2000, Loss: 0.00020412378944456577\n",
      "Iteration 620/2000, Loss: 0.00025092560099437833\n",
      "Iteration 621/2000, Loss: 0.00016795277770143002\n",
      "Iteration 622/2000, Loss: 0.00024599157040938735\n",
      "Iteration 623/2000, Loss: 0.00014219143486116081\n",
      "Iteration 624/2000, Loss: 0.0002544684975873679\n",
      "Iteration 625/2000, Loss: 0.0002485364966560155\n",
      "Iteration 626/2000, Loss: 0.0003289574524387717\n",
      "Iteration 627/2000, Loss: 0.0004164752899669111\n",
      "Iteration 628/2000, Loss: 0.00020142381254117936\n",
      "Iteration 629/2000, Loss: 0.0004178602248430252\n",
      "Iteration 630/2000, Loss: 0.00021673993614967912\n",
      "Iteration 631/2000, Loss: 0.00031825320911593735\n",
      "Iteration 632/2000, Loss: 0.00016527707339264452\n",
      "Iteration 633/2000, Loss: 0.00022307754261419177\n",
      "Iteration 634/2000, Loss: 0.00016936790780164301\n",
      "Iteration 635/2000, Loss: 0.00023699068697169423\n",
      "Iteration 636/2000, Loss: 0.00030273586162365973\n",
      "Iteration 637/2000, Loss: 0.00016976227925624698\n",
      "Iteration 638/2000, Loss: 0.00017897198267746717\n",
      "Iteration 639/2000, Loss: 0.00016744519234634936\n",
      "Iteration 640/2000, Loss: 0.00011994018859695643\n",
      "Iteration 641/2000, Loss: 0.00021381149417720735\n",
      "Iteration 642/2000, Loss: 0.0001496543554821983\n",
      "Iteration 643/2000, Loss: 0.0001987868017749861\n",
      "Iteration 644/2000, Loss: 0.00021893873054068536\n",
      "Iteration 645/2000, Loss: 0.0002206098142778501\n",
      "Iteration 646/2000, Loss: 0.00014801329234614968\n",
      "Iteration 647/2000, Loss: 0.0001302600430790335\n",
      "Iteration 648/2000, Loss: 0.00019845545466523618\n",
      "Iteration 649/2000, Loss: 0.00020497951481956989\n",
      "Iteration 650/2000, Loss: 0.00024001793644856662\n",
      "Iteration 651/2000, Loss: 0.00011429991718614474\n",
      "Iteration 652/2000, Loss: 0.0002900741819757968\n",
      "Iteration 653/2000, Loss: 0.00021839600231032819\n",
      "Iteration 654/2000, Loss: 0.00021509651560336351\n",
      "Iteration 655/2000, Loss: 0.00017254847625736147\n",
      "Iteration 656/2000, Loss: 0.0001887253310997039\n",
      "Iteration 657/2000, Loss: 0.00017132163338828832\n",
      "Iteration 658/2000, Loss: 0.00016442629566881806\n",
      "Iteration 659/2000, Loss: 0.00017969092004932463\n",
      "Iteration 660/2000, Loss: 0.0001635903463466093\n",
      "Iteration 661/2000, Loss: 0.00025006753276102245\n",
      "Iteration 662/2000, Loss: 0.00023057436919771135\n",
      "Iteration 663/2000, Loss: 0.00014733945135958493\n",
      "Iteration 664/2000, Loss: 0.0003177949402015656\n",
      "Iteration 665/2000, Loss: 0.00018844075384549797\n",
      "Iteration 666/2000, Loss: 0.00015798828098922968\n",
      "Iteration 667/2000, Loss: 0.00018951682432089\n",
      "Iteration 668/2000, Loss: 0.00014199342695064843\n",
      "Iteration 669/2000, Loss: 0.00016801772289909422\n",
      "Iteration 670/2000, Loss: 0.00017469697922933847\n",
      "Iteration 671/2000, Loss: 0.0002280369953950867\n",
      "Iteration 672/2000, Loss: 0.00025461954646743834\n",
      "Iteration 673/2000, Loss: 0.00017197945271618664\n",
      "Iteration 674/2000, Loss: 0.00011812255252152681\n",
      "Iteration 675/2000, Loss: 0.0002656092110555619\n",
      "Iteration 676/2000, Loss: 9.76265873759985e-05\n",
      "Iteration 677/2000, Loss: 0.00021014027879573405\n",
      "Iteration 678/2000, Loss: 0.00026169701595790684\n",
      "Iteration 679/2000, Loss: 0.00019213592167943716\n",
      "Iteration 680/2000, Loss: 0.00013881093764211982\n",
      "Iteration 681/2000, Loss: 0.00019257493840996176\n",
      "Iteration 682/2000, Loss: 0.00043967372039332986\n",
      "Iteration 683/2000, Loss: 0.00019844126654788852\n",
      "Iteration 684/2000, Loss: 0.00021893324446864426\n",
      "Iteration 685/2000, Loss: 0.0001313728716922924\n",
      "Iteration 686/2000, Loss: 0.0008136098622344434\n",
      "Iteration 687/2000, Loss: 0.00010807313083205372\n",
      "Iteration 688/2000, Loss: 0.00023658981081098318\n",
      "Iteration 689/2000, Loss: 0.0002939951082225889\n",
      "Iteration 690/2000, Loss: 0.0001848385581979528\n",
      "Iteration 691/2000, Loss: 0.00017812801524996758\n",
      "Iteration 692/2000, Loss: 0.00016768441128078848\n",
      "Iteration 693/2000, Loss: 0.0003800205886363983\n",
      "Iteration 694/2000, Loss: 0.0003763930290006101\n",
      "Iteration 695/2000, Loss: 0.0006189876585267484\n",
      "Iteration 696/2000, Loss: 0.0002138880518032238\n",
      "Iteration 697/2000, Loss: 0.0008145153988152742\n",
      "Iteration 698/2000, Loss: 0.0003248141147196293\n",
      "Iteration 699/2000, Loss: 0.00037415933911688626\n",
      "Iteration 700/2000, Loss: 0.0003275166673120111\n",
      "Iteration 701/2000, Loss: 0.0005251751281321049\n",
      "Iteration 702/2000, Loss: 0.0002885730646084994\n",
      "Iteration 703/2000, Loss: 0.00030636569135822356\n",
      "Iteration 704/2000, Loss: 0.00035662477603182197\n",
      "Iteration 705/2000, Loss: 0.00030796052305959165\n",
      "Iteration 706/2000, Loss: 0.0001651540951570496\n",
      "Iteration 707/2000, Loss: 0.0002309624251211062\n",
      "Iteration 708/2000, Loss: 0.00029376731254160404\n",
      "Iteration 709/2000, Loss: 0.0005144497845321894\n",
      "Iteration 710/2000, Loss: 0.00016075832536444068\n",
      "Iteration 711/2000, Loss: 0.00030564406188204885\n",
      "Iteration 712/2000, Loss: 0.0002083584840875119\n",
      "Iteration 713/2000, Loss: 0.00017743730859365314\n",
      "Iteration 714/2000, Loss: 0.00021704897517338395\n",
      "Iteration 715/2000, Loss: 0.0002650149108376354\n",
      "Iteration 716/2000, Loss: 0.00021023365843575448\n",
      "Iteration 717/2000, Loss: 0.00018204972730018198\n",
      "Iteration 718/2000, Loss: 0.00036813263432122767\n",
      "Iteration 719/2000, Loss: 0.00014440782251767814\n",
      "Iteration 720/2000, Loss: 0.000426963233621791\n",
      "Iteration 721/2000, Loss: 0.000205693650059402\n",
      "Iteration 722/2000, Loss: 0.0003037987044081092\n",
      "Iteration 723/2000, Loss: 0.0002612857206258923\n",
      "Iteration 724/2000, Loss: 0.000250461685936898\n",
      "Iteration 725/2000, Loss: 0.0002449667372275144\n",
      "Iteration 726/2000, Loss: 0.00014411695883609354\n",
      "Iteration 727/2000, Loss: 0.00015851264470256865\n",
      "Iteration 728/2000, Loss: 0.0001930023863678798\n",
      "Iteration 729/2000, Loss: 0.00022053057909943163\n",
      "Iteration 730/2000, Loss: 0.00021284868125803769\n",
      "Iteration 731/2000, Loss: 0.00019430056272540241\n",
      "Iteration 732/2000, Loss: 0.00020071535254828632\n",
      "Iteration 733/2000, Loss: 0.00046366601600311697\n",
      "Iteration 734/2000, Loss: 0.0001839211763581261\n",
      "Iteration 735/2000, Loss: 0.0003124066861346364\n",
      "Iteration 736/2000, Loss: 0.0002622359897941351\n",
      "Iteration 737/2000, Loss: 0.0001332076353719458\n",
      "Iteration 738/2000, Loss: 0.00011043704580515623\n",
      "Iteration 739/2000, Loss: 0.00022266292944550514\n",
      "Iteration 740/2000, Loss: 0.00013269238115753978\n",
      "Iteration 741/2000, Loss: 0.00017344100342597812\n",
      "Iteration 742/2000, Loss: 0.0001441869098925963\n",
      "Iteration 743/2000, Loss: 0.00020332196436356753\n",
      "Iteration 744/2000, Loss: 0.00017346856475342065\n",
      "Iteration 745/2000, Loss: 0.00015556177822872996\n",
      "Iteration 746/2000, Loss: 0.00021307950373739004\n",
      "Iteration 747/2000, Loss: 0.00024973100516945124\n",
      "Iteration 748/2000, Loss: 0.0002623464970383793\n",
      "Iteration 749/2000, Loss: 0.0002651604008860886\n",
      "Iteration 750/2000, Loss: 0.0003350117476657033\n",
      "Iteration 751/2000, Loss: 0.00026245557819493115\n",
      "Iteration 752/2000, Loss: 0.0001510524598415941\n",
      "Iteration 753/2000, Loss: 0.00016862254415173084\n",
      "Iteration 754/2000, Loss: 0.00023561737907584757\n",
      "Iteration 755/2000, Loss: 0.00024789475719444454\n",
      "Iteration 756/2000, Loss: 0.00017895572818815708\n",
      "Iteration 757/2000, Loss: 0.000530033023096621\n",
      "Iteration 758/2000, Loss: 0.00023977005912456661\n",
      "Iteration 759/2000, Loss: 0.00039295051828958094\n",
      "Iteration 760/2000, Loss: 0.00020961380505468696\n",
      "Iteration 761/2000, Loss: 0.0005467993905767798\n",
      "Iteration 762/2000, Loss: 0.00019084071391262114\n",
      "Iteration 763/2000, Loss: 0.00039687324897386134\n",
      "Iteration 764/2000, Loss: 0.00028550648130476475\n",
      "Iteration 765/2000, Loss: 0.00026448717107996345\n",
      "Iteration 766/2000, Loss: 0.00028700698749162257\n",
      "Iteration 767/2000, Loss: 0.00018304945842828602\n",
      "Iteration 768/2000, Loss: 0.00028689319151453674\n",
      "Iteration 769/2000, Loss: 0.00037048119702376425\n",
      "Iteration 770/2000, Loss: 0.0003983053029514849\n",
      "Iteration 771/2000, Loss: 0.0003959831956308335\n",
      "Iteration 772/2000, Loss: 0.0005252464325167239\n",
      "Iteration 773/2000, Loss: 0.00018176365119870752\n",
      "Iteration 774/2000, Loss: 0.0005760971107520163\n",
      "Iteration 775/2000, Loss: 0.00017163381562568247\n",
      "Iteration 776/2000, Loss: 0.0005737175233662128\n",
      "Iteration 777/2000, Loss: 0.00017722461780067533\n",
      "Iteration 778/2000, Loss: 0.0005922754062339664\n",
      "Iteration 779/2000, Loss: 0.00014232919784262776\n",
      "Iteration 780/2000, Loss: 0.00036257252213545144\n",
      "Iteration 781/2000, Loss: 0.0002736784517765045\n",
      "Iteration 782/2000, Loss: 0.0003429801145102829\n",
      "Iteration 783/2000, Loss: 0.0002751615538727492\n",
      "Iteration 784/2000, Loss: 0.0002585427719168365\n",
      "Iteration 785/2000, Loss: 0.00032435113098472357\n",
      "Iteration 786/2000, Loss: 0.0001693140948191285\n",
      "Iteration 787/2000, Loss: 0.00026286806678399444\n",
      "Iteration 788/2000, Loss: 0.00015771429752931\n",
      "Iteration 789/2000, Loss: 0.0002735887246672064\n",
      "Iteration 790/2000, Loss: 0.00022647666628472507\n",
      "Iteration 791/2000, Loss: 0.00012488014181144536\n",
      "Iteration 792/2000, Loss: 0.00011248384544160217\n",
      "Iteration 793/2000, Loss: 0.00019263406284153461\n",
      "Iteration 794/2000, Loss: 0.0004520746588241309\n",
      "Iteration 795/2000, Loss: 0.0001944625546457246\n",
      "Iteration 796/2000, Loss: 0.000162893658853136\n",
      "Iteration 797/2000, Loss: 0.00015159478061832488\n",
      "Iteration 798/2000, Loss: 0.0002497861860319972\n",
      "Iteration 799/2000, Loss: 0.00013873544230591506\n",
      "Iteration 800/2000, Loss: 0.00017125961312558502\n",
      "Iteration 801/2000, Loss: 0.0001884034718386829\n",
      "Iteration 802/2000, Loss: 0.00015279998478945345\n",
      "Iteration 803/2000, Loss: 0.0004883483634330332\n",
      "Iteration 804/2000, Loss: 0.0001391199475619942\n",
      "Iteration 805/2000, Loss: 0.00020110544573981315\n",
      "Iteration 806/2000, Loss: 0.00011069161701016128\n",
      "Iteration 807/2000, Loss: 0.00013400147145148367\n",
      "Iteration 808/2000, Loss: 0.00017142464639618993\n",
      "Iteration 809/2000, Loss: 0.0001350405946141109\n",
      "Iteration 810/2000, Loss: 0.00014980140258558095\n",
      "Iteration 811/2000, Loss: 0.00021005049347877502\n",
      "Iteration 812/2000, Loss: 0.00019482315110508353\n",
      "Iteration 813/2000, Loss: 0.00012178636825410649\n",
      "Iteration 814/2000, Loss: 0.0006072643445804715\n",
      "Iteration 815/2000, Loss: 0.00017476432549301535\n",
      "Iteration 816/2000, Loss: 0.0002379175421083346\n",
      "Iteration 817/2000, Loss: 0.00027793197659775615\n",
      "Iteration 818/2000, Loss: 0.00024337277864106\n",
      "Iteration 819/2000, Loss: 0.00030283769592642784\n",
      "Iteration 820/2000, Loss: 0.00028336094692349434\n",
      "Iteration 821/2000, Loss: 0.0001235606468981132\n",
      "Iteration 822/2000, Loss: 0.0001961773814400658\n",
      "Iteration 823/2000, Loss: 0.0001248064509127289\n",
      "Iteration 824/2000, Loss: 0.0004947833367623389\n",
      "Iteration 825/2000, Loss: 0.00021945996559225023\n",
      "Iteration 826/2000, Loss: 0.00017849853611551225\n",
      "Iteration 827/2000, Loss: 0.00023943719861563295\n",
      "Iteration 828/2000, Loss: 0.00016710624913685024\n",
      "Iteration 829/2000, Loss: 0.0002267173258587718\n",
      "Iteration 830/2000, Loss: 0.00010964307875838131\n",
      "Iteration 831/2000, Loss: 0.00019391450041439384\n",
      "Iteration 832/2000, Loss: 0.0001560939708724618\n",
      "Iteration 833/2000, Loss: 0.00021719046344514936\n",
      "Iteration 834/2000, Loss: 0.00027422638959251344\n",
      "Iteration 835/2000, Loss: 0.00017978023970499635\n",
      "Iteration 836/2000, Loss: 0.00026648861239664257\n",
      "Iteration 837/2000, Loss: 0.00012031343067064881\n",
      "Iteration 838/2000, Loss: 0.00030913413502275944\n",
      "Iteration 839/2000, Loss: 0.00015281394007615745\n",
      "Iteration 840/2000, Loss: 0.0001851606648415327\n",
      "Iteration 841/2000, Loss: 0.00011226572678424418\n",
      "Iteration 842/2000, Loss: 0.00020705028146039695\n",
      "Iteration 843/2000, Loss: 0.00011163287126692012\n",
      "Iteration 844/2000, Loss: 0.0001934876199811697\n",
      "Iteration 845/2000, Loss: 0.0002253788261441514\n",
      "Iteration 846/2000, Loss: 0.0001134268386522308\n",
      "Iteration 847/2000, Loss: 0.00011113390792161226\n",
      "Iteration 848/2000, Loss: 0.0001699635322438553\n",
      "Iteration 849/2000, Loss: 0.00016147295536939055\n",
      "Iteration 850/2000, Loss: 0.00013826112262904644\n",
      "Iteration 851/2000, Loss: 0.00019377315766178071\n",
      "Iteration 852/2000, Loss: 0.00017036483041010797\n",
      "Iteration 853/2000, Loss: 0.00016777143173385412\n",
      "Iteration 854/2000, Loss: 0.000238335007452406\n",
      "Iteration 855/2000, Loss: 0.00025684470892883837\n",
      "Iteration 856/2000, Loss: 0.0002594507532194257\n",
      "Iteration 857/2000, Loss: 0.0002794817555695772\n",
      "Iteration 858/2000, Loss: 0.00016092472651507705\n",
      "Iteration 859/2000, Loss: 0.0001877689646789804\n",
      "Iteration 860/2000, Loss: 0.00010349125659558922\n",
      "Iteration 861/2000, Loss: 0.00018799914687406272\n",
      "Iteration 862/2000, Loss: 0.00011146078031742945\n",
      "Iteration 863/2000, Loss: 0.0002482607960700989\n",
      "Iteration 864/2000, Loss: 0.00017427655984647572\n",
      "Iteration 865/2000, Loss: 0.00027252337895333767\n",
      "Iteration 866/2000, Loss: 0.00029627326875925064\n",
      "Iteration 867/2000, Loss: 0.0001776666467776522\n",
      "Iteration 868/2000, Loss: 0.0002835375489667058\n",
      "Iteration 869/2000, Loss: 0.00012183286889921874\n",
      "Iteration 870/2000, Loss: 0.00031539512565359473\n",
      "Iteration 871/2000, Loss: 0.0003583779325708747\n",
      "Iteration 872/2000, Loss: 0.00040000915760174394\n",
      "Iteration 873/2000, Loss: 0.0002828120777849108\n",
      "Iteration 874/2000, Loss: 0.00012088654330000281\n",
      "Iteration 875/2000, Loss: 0.0001791894610505551\n",
      "Iteration 876/2000, Loss: 0.00017136191308964044\n",
      "Iteration 877/2000, Loss: 0.00017934510833583772\n",
      "Iteration 878/2000, Loss: 0.00021338720398489386\n",
      "Iteration 879/2000, Loss: 0.00020279547607060522\n",
      "Iteration 880/2000, Loss: 0.0001592874905327335\n",
      "Iteration 881/2000, Loss: 0.00015206816897261888\n",
      "Iteration 882/2000, Loss: 0.00027470706845633686\n",
      "Iteration 883/2000, Loss: 0.0001926475524669513\n",
      "Iteration 884/2000, Loss: 0.00015136795991566032\n",
      "Iteration 885/2000, Loss: 0.00014296421431936324\n",
      "Iteration 886/2000, Loss: 0.00021347851725295186\n",
      "Iteration 887/2000, Loss: 0.00017511204350739717\n",
      "Iteration 888/2000, Loss: 0.00015994526620488614\n",
      "Iteration 889/2000, Loss: 0.00020776251039933413\n",
      "Iteration 890/2000, Loss: 0.000404571823310107\n",
      "Iteration 891/2000, Loss: 0.00014400907093659043\n",
      "Iteration 892/2000, Loss: 0.00016968858835753053\n",
      "Iteration 893/2000, Loss: 0.00017418096831534058\n",
      "Iteration 894/2000, Loss: 0.00040797892143018544\n",
      "Iteration 895/2000, Loss: 0.00019945776148233563\n",
      "Iteration 896/2000, Loss: 0.00023963132116477937\n",
      "Iteration 897/2000, Loss: 0.0001514873729320243\n",
      "Iteration 898/2000, Loss: 0.0002733455621637404\n",
      "Iteration 899/2000, Loss: 0.0001627766905585304\n",
      "Iteration 900/2000, Loss: 0.00024508943897672\n",
      "Iteration 901/2000, Loss: 0.00019759588758461177\n",
      "Iteration 902/2000, Loss: 0.00013859951286576688\n",
      "Iteration 903/2000, Loss: 0.0001394112769048661\n",
      "Iteration 904/2000, Loss: 0.0001756443380145356\n",
      "Iteration 905/2000, Loss: 0.00014857869246043265\n",
      "Iteration 906/2000, Loss: 0.0002552783989813179\n",
      "Iteration 907/2000, Loss: 0.0001127003415604122\n",
      "Iteration 908/2000, Loss: 0.00021268382261041552\n",
      "Iteration 909/2000, Loss: 0.00033995919511653483\n",
      "Iteration 910/2000, Loss: 0.00013764164759777486\n",
      "Iteration 911/2000, Loss: 0.0001229363988386467\n",
      "Iteration 912/2000, Loss: 0.00021232070866972208\n",
      "Iteration 913/2000, Loss: 0.00013621398829855025\n",
      "Iteration 914/2000, Loss: 0.0002152291708625853\n",
      "Iteration 915/2000, Loss: 0.0008483772980980575\n",
      "Iteration 916/2000, Loss: 0.00015538057778030634\n",
      "Iteration 917/2000, Loss: 0.00016323385352734476\n",
      "Iteration 918/2000, Loss: 0.00013203723938204348\n",
      "Iteration 919/2000, Loss: 0.00022327873739413917\n",
      "Iteration 920/2000, Loss: 0.0001614767243154347\n",
      "Iteration 921/2000, Loss: 0.00013188565208110958\n",
      "Iteration 922/2000, Loss: 0.00018357358931098133\n",
      "Iteration 923/2000, Loss: 0.000204893498448655\n",
      "Iteration 924/2000, Loss: 0.00017363439837936312\n",
      "Iteration 925/2000, Loss: 0.00016575520567130297\n",
      "Iteration 926/2000, Loss: 0.00035229517379775643\n",
      "Iteration 927/2000, Loss: 0.00020196502737235278\n",
      "Iteration 928/2000, Loss: 0.0005001574172638357\n",
      "Iteration 929/2000, Loss: 0.00016223336569964886\n",
      "Iteration 930/2000, Loss: 0.0002425187558401376\n",
      "Iteration 931/2000, Loss: 0.00020338172907941043\n",
      "Iteration 932/2000, Loss: 0.0002095255913445726\n",
      "Iteration 933/2000, Loss: 0.00027277719345875084\n",
      "Iteration 934/2000, Loss: 0.0002996886323671788\n",
      "Iteration 935/2000, Loss: 0.00020670251979026943\n",
      "Iteration 936/2000, Loss: 0.00021052066585980356\n",
      "Iteration 937/2000, Loss: 0.00018176478624809533\n",
      "Iteration 938/2000, Loss: 0.00039255019510164857\n",
      "Iteration 939/2000, Loss: 0.00021800532704219222\n",
      "Iteration 940/2000, Loss: 0.0002769611310213804\n",
      "Iteration 941/2000, Loss: 0.00022525271924678236\n",
      "Iteration 942/2000, Loss: 0.0001491956354584545\n",
      "Iteration 943/2000, Loss: 0.00017068129091057926\n",
      "Iteration 944/2000, Loss: 0.00029714417178183794\n",
      "Iteration 945/2000, Loss: 0.0001914909662446007\n",
      "Iteration 946/2000, Loss: 0.00018773988995235413\n",
      "Iteration 947/2000, Loss: 0.0003477675491012633\n",
      "Iteration 948/2000, Loss: 0.00019066163804382086\n",
      "Iteration 949/2000, Loss: 0.0001505992840975523\n",
      "Iteration 950/2000, Loss: 0.00034032692201435566\n",
      "Iteration 951/2000, Loss: 0.00018103951879311353\n",
      "Iteration 952/2000, Loss: 0.00030075680115260184\n",
      "Iteration 953/2000, Loss: 0.0002702173369470984\n",
      "Iteration 954/2000, Loss: 0.00043416532571427524\n",
      "Iteration 955/2000, Loss: 0.00018465805624146014\n",
      "Iteration 956/2000, Loss: 0.00019307121692690998\n",
      "Iteration 957/2000, Loss: 0.00023474237241316587\n",
      "Iteration 958/2000, Loss: 0.00031939311884343624\n",
      "Iteration 959/2000, Loss: 0.00018274957255925983\n",
      "Iteration 960/2000, Loss: 0.00023077182413544506\n",
      "Iteration 961/2000, Loss: 0.00018063627067022026\n",
      "Iteration 962/2000, Loss: 0.000153735963976942\n",
      "Iteration 963/2000, Loss: 0.00016215888899751008\n",
      "Iteration 964/2000, Loss: 0.0002765075187198818\n",
      "Iteration 965/2000, Loss: 0.00015298557991627604\n",
      "Iteration 966/2000, Loss: 0.00011758993787225336\n",
      "Iteration 967/2000, Loss: 0.0002479669055901468\n",
      "Iteration 968/2000, Loss: 0.00028114437009207904\n",
      "Iteration 969/2000, Loss: 0.00022019457537680864\n",
      "Iteration 970/2000, Loss: 0.0002430368185741827\n",
      "Iteration 971/2000, Loss: 0.000358755438355729\n",
      "Iteration 972/2000, Loss: 0.0002621865423861891\n",
      "Iteration 973/2000, Loss: 0.00040128082036972046\n",
      "Iteration 974/2000, Loss: 0.00018399332475382835\n",
      "Iteration 975/2000, Loss: 0.0002087112661683932\n",
      "Iteration 976/2000, Loss: 0.00018607333186082542\n",
      "Iteration 977/2000, Loss: 0.0001926876575453207\n",
      "Iteration 978/2000, Loss: 0.0001349790400126949\n",
      "Iteration 979/2000, Loss: 0.0001631155319046229\n",
      "Iteration 980/2000, Loss: 0.00016416401194874197\n",
      "Iteration 981/2000, Loss: 0.00036867352901026607\n",
      "Iteration 982/2000, Loss: 0.00026679039001464844\n",
      "Iteration 983/2000, Loss: 0.00014156488759908825\n",
      "Iteration 984/2000, Loss: 0.00021081184968352318\n",
      "Iteration 985/2000, Loss: 0.0002835396444424987\n",
      "Iteration 986/2000, Loss: 0.00018274500325787812\n",
      "Iteration 987/2000, Loss: 0.0001714164245640859\n",
      "Iteration 988/2000, Loss: 0.00012893792882096022\n",
      "Iteration 989/2000, Loss: 0.00017272138211410493\n",
      "Iteration 990/2000, Loss: 0.00013791827950626612\n",
      "Iteration 991/2000, Loss: 0.00014899570669513196\n",
      "Iteration 992/2000, Loss: 0.0001947790151461959\n",
      "Iteration 993/2000, Loss: 9.337505616713315e-05\n",
      "Iteration 994/2000, Loss: 0.00015379286196548492\n",
      "Iteration 995/2000, Loss: 0.0001275263784918934\n",
      "Iteration 996/2000, Loss: 0.0001239331904798746\n",
      "Iteration 997/2000, Loss: 0.0001279347634408623\n",
      "Iteration 998/2000, Loss: 0.0001442109642084688\n",
      "Iteration 999/2000, Loss: 0.00026907981373369694\n",
      "Iteration 1000/2000, Loss: 0.00023158395197242498\n",
      "Iteration 1001/2000, Loss: 0.00012062385212630033\n",
      "Iteration 1002/2000, Loss: 0.0002797749184537679\n",
      "Iteration 1003/2000, Loss: 9.371111082145944e-05\n",
      "Iteration 1004/2000, Loss: 0.0001416451996192336\n",
      "Iteration 1005/2000, Loss: 0.00013238537940196693\n",
      "Iteration 1006/2000, Loss: 0.00023889279691502452\n",
      "Iteration 1007/2000, Loss: 0.0003433226083870977\n",
      "Iteration 1008/2000, Loss: 0.00035929589648731053\n",
      "Iteration 1009/2000, Loss: 0.0002442409167997539\n",
      "Iteration 1010/2000, Loss: 0.0001939837384270504\n",
      "Iteration 1011/2000, Loss: 0.00019099685596302152\n",
      "Iteration 1012/2000, Loss: 0.00016618368681520224\n",
      "Iteration 1013/2000, Loss: 0.000174615066498518\n",
      "Iteration 1014/2000, Loss: 0.00015228100528474897\n",
      "Iteration 1015/2000, Loss: 0.00043271807953715324\n",
      "Iteration 1016/2000, Loss: 0.0006967608351260424\n",
      "Iteration 1017/2000, Loss: 0.0003822563448920846\n",
      "Iteration 1018/2000, Loss: 0.00017600897990632802\n",
      "Iteration 1019/2000, Loss: 0.0003688174474518746\n",
      "Iteration 1020/2000, Loss: 0.001573042245581746\n",
      "Iteration 1021/2000, Loss: 0.0002221069298684597\n",
      "Iteration 1022/2000, Loss: 0.0002644584164954722\n",
      "Iteration 1023/2000, Loss: 0.00022172476747073233\n",
      "Iteration 1024/2000, Loss: 0.00037292009801603854\n",
      "Iteration 1025/2000, Loss: 0.0002632104733493179\n",
      "Iteration 1026/2000, Loss: 0.00020429831056389958\n",
      "Iteration 1027/2000, Loss: 0.0002478098904248327\n",
      "Iteration 1028/2000, Loss: 0.00022929506667423993\n",
      "Iteration 1029/2000, Loss: 0.0002611308009363711\n",
      "Iteration 1030/2000, Loss: 0.00016952271107584238\n",
      "Iteration 1031/2000, Loss: 0.00018596957670524716\n",
      "Iteration 1032/2000, Loss: 0.0002008647716138512\n",
      "Iteration 1033/2000, Loss: 0.00021980520978104323\n",
      "Iteration 1034/2000, Loss: 0.00018023069424089044\n",
      "Iteration 1035/2000, Loss: 0.0001413957797922194\n",
      "Iteration 1036/2000, Loss: 0.0001720673608360812\n",
      "Iteration 1037/2000, Loss: 0.00015380646800622344\n",
      "Iteration 1038/2000, Loss: 0.00027845692238770425\n",
      "Iteration 1039/2000, Loss: 0.0003799645055551082\n",
      "Iteration 1040/2000, Loss: 0.000201967908651568\n",
      "Iteration 1041/2000, Loss: 0.00038418054464273155\n",
      "Iteration 1042/2000, Loss: 0.0003395772073417902\n",
      "Iteration 1043/2000, Loss: 0.0002524517185520381\n",
      "Iteration 1044/2000, Loss: 0.0002191111125284806\n",
      "Iteration 1045/2000, Loss: 0.0003852377994917333\n",
      "Iteration 1046/2000, Loss: 0.00028435472631826997\n",
      "Iteration 1047/2000, Loss: 0.0002669717650860548\n",
      "Iteration 1048/2000, Loss: 0.00029575705411843956\n",
      "Iteration 1049/2000, Loss: 0.00027469638735055923\n",
      "Iteration 1050/2000, Loss: 0.00026597146643325686\n",
      "Iteration 1051/2000, Loss: 0.0002606652560643852\n",
      "Iteration 1052/2000, Loss: 0.00022200126841198653\n",
      "Iteration 1053/2000, Loss: 0.0002284020883962512\n",
      "Iteration 1054/2000, Loss: 0.00023347479873336852\n",
      "Iteration 1055/2000, Loss: 0.00022122712107375264\n",
      "Iteration 1056/2000, Loss: 0.00028375061810947955\n",
      "Iteration 1057/2000, Loss: 0.0003157455357722938\n",
      "Iteration 1058/2000, Loss: 0.0002870588796213269\n",
      "Iteration 1059/2000, Loss: 0.00028555048629641533\n",
      "Iteration 1060/2000, Loss: 0.00029563074349425733\n",
      "Iteration 1061/2000, Loss: 0.0002225204516435042\n",
      "Iteration 1062/2000, Loss: 0.00020661682356148958\n",
      "Iteration 1063/2000, Loss: 0.00028862105682492256\n",
      "Iteration 1064/2000, Loss: 0.00020968765602447093\n",
      "Iteration 1065/2000, Loss: 0.00016672027413733304\n",
      "Iteration 1066/2000, Loss: 0.00015081059245858341\n",
      "Iteration 1067/2000, Loss: 0.00019126894767396152\n",
      "Iteration 1068/2000, Loss: 0.0006000820430926979\n",
      "Iteration 1069/2000, Loss: 0.00029908271972090006\n",
      "Iteration 1070/2000, Loss: 0.00026658657588995993\n",
      "Iteration 1071/2000, Loss: 0.00018156750593334436\n",
      "Iteration 1072/2000, Loss: 0.0002482041309121996\n",
      "Iteration 1073/2000, Loss: 0.0002565999166108668\n",
      "Iteration 1074/2000, Loss: 0.00022849261586088687\n",
      "Iteration 1075/2000, Loss: 0.00029836356407031417\n",
      "Iteration 1076/2000, Loss: 0.0003056779969483614\n",
      "Iteration 1077/2000, Loss: 0.0003440264845266938\n",
      "Iteration 1078/2000, Loss: 0.000260196509771049\n",
      "Iteration 1079/2000, Loss: 0.00038778973976150155\n",
      "Iteration 1080/2000, Loss: 0.0005913763889111578\n",
      "Iteration 1081/2000, Loss: 0.0003519055317156017\n",
      "Iteration 1082/2000, Loss: 0.00019036493904422969\n",
      "Iteration 1083/2000, Loss: 0.0005358777125366032\n",
      "Iteration 1084/2000, Loss: 0.00024253033916465938\n",
      "Iteration 1085/2000, Loss: 0.0006328524905256927\n",
      "Iteration 1086/2000, Loss: 0.00018492092203814536\n",
      "Iteration 1087/2000, Loss: 0.0007857450400479138\n",
      "Iteration 1088/2000, Loss: 0.00025293140788562596\n",
      "Iteration 1089/2000, Loss: 0.0003957213193643838\n",
      "Iteration 1090/2000, Loss: 0.0002881433174479753\n",
      "Iteration 1091/2000, Loss: 0.00022865133360028267\n",
      "Iteration 1092/2000, Loss: 0.00022918781905900687\n",
      "Iteration 1093/2000, Loss: 0.00017924954590853304\n",
      "Iteration 1094/2000, Loss: 0.0002074430522043258\n",
      "Iteration 1095/2000, Loss: 0.00021474891400430351\n",
      "Iteration 1096/2000, Loss: 0.00025925837690010667\n",
      "Iteration 1097/2000, Loss: 0.00016534594760742038\n",
      "Iteration 1098/2000, Loss: 0.00022649824677500874\n",
      "Iteration 1099/2000, Loss: 0.00031266562291420996\n",
      "Iteration 1100/2000, Loss: 0.00029528539744205773\n",
      "Iteration 1101/2000, Loss: 0.0002821981906890869\n",
      "Iteration 1102/2000, Loss: 0.0002225440985057503\n",
      "Iteration 1103/2000, Loss: 0.0004128730797674507\n",
      "Iteration 1104/2000, Loss: 0.0001513034658273682\n",
      "Iteration 1105/2000, Loss: 0.0005499464459717274\n",
      "Iteration 1106/2000, Loss: 0.0003805785672739148\n",
      "Iteration 1107/2000, Loss: 0.00032917805947363377\n",
      "Iteration 1108/2000, Loss: 0.00010359727457398549\n",
      "Iteration 1109/2000, Loss: 0.0004949706490151584\n",
      "Iteration 1110/2000, Loss: 0.0002642397303134203\n",
      "Iteration 1111/2000, Loss: 0.00024317970382981002\n",
      "Iteration 1112/2000, Loss: 0.0007971819723024964\n",
      "Iteration 1113/2000, Loss: 0.0001883632648969069\n",
      "Iteration 1114/2000, Loss: 0.0003073564439546317\n",
      "Iteration 1115/2000, Loss: 0.00028823388856835663\n",
      "Iteration 1116/2000, Loss: 0.00031040748581290245\n",
      "Iteration 1117/2000, Loss: 0.00029076149803586304\n",
      "Iteration 1118/2000, Loss: 0.00014772321446798742\n",
      "Iteration 1119/2000, Loss: 0.00044920321670360863\n",
      "Iteration 1120/2000, Loss: 0.0001873251167126\n",
      "Iteration 1121/2000, Loss: 0.00020602039876393974\n",
      "Iteration 1122/2000, Loss: 0.00017034730990417302\n",
      "Iteration 1123/2000, Loss: 0.0002756283211056143\n",
      "Iteration 1124/2000, Loss: 0.00020112023048568517\n",
      "Iteration 1125/2000, Loss: 0.00016060158668551594\n",
      "Iteration 1126/2000, Loss: 0.0001998971711145714\n",
      "Iteration 1127/2000, Loss: 0.00023926816356834024\n",
      "Iteration 1128/2000, Loss: 0.0002941128332167864\n",
      "Iteration 1129/2000, Loss: 0.00020086138101760298\n",
      "Iteration 1130/2000, Loss: 0.0002640007878653705\n",
      "Iteration 1131/2000, Loss: 0.00018374079081695527\n",
      "Iteration 1132/2000, Loss: 0.00015324885316658765\n",
      "Iteration 1133/2000, Loss: 0.0002234485582448542\n",
      "Iteration 1134/2000, Loss: 0.00012603276991285384\n",
      "Iteration 1135/2000, Loss: 0.00030393365886993706\n",
      "Iteration 1136/2000, Loss: 0.0001636958186281845\n",
      "Iteration 1137/2000, Loss: 0.00023755278380122036\n",
      "Iteration 1138/2000, Loss: 0.00033603538759052753\n",
      "Iteration 1139/2000, Loss: 0.00023928425798658282\n",
      "Iteration 1140/2000, Loss: 0.00018425157759338617\n",
      "Iteration 1141/2000, Loss: 0.00023407244589179754\n",
      "Iteration 1142/2000, Loss: 0.00018113745318260044\n",
      "Iteration 1143/2000, Loss: 0.0002055195509456098\n",
      "Iteration 1144/2000, Loss: 0.00015718420036137104\n",
      "Iteration 1145/2000, Loss: 9.353877976536751e-05\n",
      "Iteration 1146/2000, Loss: 0.0003191320283804089\n",
      "Iteration 1147/2000, Loss: 0.00016672721540089697\n",
      "Iteration 1148/2000, Loss: 0.0002307952381670475\n",
      "Iteration 1149/2000, Loss: 0.0002013899793382734\n",
      "Iteration 1150/2000, Loss: 0.00016582284297328442\n",
      "Iteration 1151/2000, Loss: 0.00030065575265325606\n",
      "Iteration 1152/2000, Loss: 0.00012797884119208902\n",
      "Iteration 1153/2000, Loss: 0.00021920609287917614\n",
      "Iteration 1154/2000, Loss: 0.00018658435146789998\n",
      "Iteration 1155/2000, Loss: 0.00017404774553142488\n",
      "Iteration 1156/2000, Loss: 0.00030589295784011483\n",
      "Iteration 1157/2000, Loss: 0.00010601492249406874\n",
      "Iteration 1158/2000, Loss: 0.0003698497894220054\n",
      "Iteration 1159/2000, Loss: 0.0002881522523239255\n",
      "Iteration 1160/2000, Loss: 0.0002580528671387583\n",
      "Iteration 1161/2000, Loss: 0.00044971503666602075\n",
      "Iteration 1162/2000, Loss: 0.00024714451865293086\n",
      "Iteration 1163/2000, Loss: 0.0001966417912626639\n",
      "Iteration 1164/2000, Loss: 0.00011268161324551329\n",
      "Iteration 1165/2000, Loss: 0.00024129461962729692\n",
      "Iteration 1166/2000, Loss: 0.00016233276983257383\n",
      "Iteration 1167/2000, Loss: 0.00018232867296319455\n",
      "Iteration 1168/2000, Loss: 0.00015273421013262123\n",
      "Iteration 1169/2000, Loss: 0.0002078075922327116\n",
      "Iteration 1170/2000, Loss: 0.000281668413663283\n",
      "Iteration 1171/2000, Loss: 0.00015433582302648574\n",
      "Iteration 1172/2000, Loss: 0.0001626270532142371\n",
      "Iteration 1173/2000, Loss: 0.00023130836780183017\n",
      "Iteration 1174/2000, Loss: 0.00017293480050284415\n",
      "Iteration 1175/2000, Loss: 0.00017433121684007347\n",
      "Iteration 1176/2000, Loss: 0.00025438121519982815\n",
      "Iteration 1177/2000, Loss: 0.00012612473801709712\n",
      "Iteration 1178/2000, Loss: 0.0003012656816281378\n",
      "Iteration 1179/2000, Loss: 0.00042342316010035574\n",
      "Iteration 1180/2000, Loss: 0.00013841006148140877\n",
      "Iteration 1181/2000, Loss: 0.00023674665135331452\n",
      "Iteration 1182/2000, Loss: 0.00019646504370030016\n",
      "Iteration 1183/2000, Loss: 0.0002754411252681166\n",
      "Iteration 1184/2000, Loss: 0.00024378698435612023\n",
      "Iteration 1185/2000, Loss: 0.00023836980108171701\n",
      "Iteration 1186/2000, Loss: 0.00018442643340677023\n",
      "Iteration 1187/2000, Loss: 0.00025304718292318285\n",
      "Iteration 1188/2000, Loss: 0.00016642396803945303\n",
      "Iteration 1189/2000, Loss: 0.00025438208831474185\n",
      "Iteration 1190/2000, Loss: 0.0004036191094201058\n",
      "Iteration 1191/2000, Loss: 0.00031030617537908256\n",
      "Iteration 1192/2000, Loss: 0.0002509357000235468\n",
      "Iteration 1193/2000, Loss: 0.0002667121007107198\n",
      "Iteration 1194/2000, Loss: 0.00035482863313518465\n",
      "Iteration 1195/2000, Loss: 0.0002919078688137233\n",
      "Iteration 1196/2000, Loss: 0.00018961881869472563\n",
      "Iteration 1197/2000, Loss: 0.00020457048958633095\n",
      "Iteration 1198/2000, Loss: 0.00027918387786485255\n",
      "Iteration 1199/2000, Loss: 0.00015063020691741258\n",
      "Iteration 1200/2000, Loss: 0.00021282896341290325\n",
      "Iteration 1201/2000, Loss: 0.00018351856851950288\n",
      "Iteration 1202/2000, Loss: 0.00030388677259907126\n",
      "Iteration 1203/2000, Loss: 0.0002629387308843434\n",
      "Iteration 1204/2000, Loss: 0.0004305404145270586\n",
      "Iteration 1205/2000, Loss: 0.0002941132406704128\n",
      "Iteration 1206/2000, Loss: 0.0006212728330865502\n",
      "Iteration 1207/2000, Loss: 0.00024231323914136738\n",
      "Iteration 1208/2000, Loss: 0.00043754035141319036\n",
      "Iteration 1209/2000, Loss: 0.00026375046581961215\n",
      "Iteration 1210/2000, Loss: 0.00042510568164289\n",
      "Iteration 1211/2000, Loss: 0.00048313895240426064\n",
      "Iteration 1212/2000, Loss: 0.00034498635795898736\n",
      "Iteration 1213/2000, Loss: 0.000953697192016989\n",
      "Iteration 1214/2000, Loss: 0.00029252367676235735\n",
      "Iteration 1215/2000, Loss: 0.0007523302338086069\n",
      "Iteration 1216/2000, Loss: 0.00022912898566573858\n",
      "Iteration 1217/2000, Loss: 0.0008136489777825773\n",
      "Iteration 1218/2000, Loss: 0.00033659779001027346\n",
      "Iteration 1219/2000, Loss: 0.00020388953271321952\n",
      "Iteration 1220/2000, Loss: 0.0002602436870802194\n",
      "Iteration 1221/2000, Loss: 0.00022531190188601613\n",
      "Iteration 1222/2000, Loss: 0.0002798374043777585\n",
      "Iteration 1223/2000, Loss: 0.00014795735478401184\n",
      "Iteration 1224/2000, Loss: 0.00024665214004926383\n",
      "Iteration 1225/2000, Loss: 0.00013841447071172297\n",
      "Iteration 1226/2000, Loss: 0.00018506459309719503\n",
      "Iteration 1227/2000, Loss: 0.00020864886755589396\n",
      "Iteration 1228/2000, Loss: 0.00122094398830086\n",
      "Iteration 1229/2000, Loss: 0.00044109768350608647\n",
      "Iteration 1230/2000, Loss: 0.0002431075117783621\n",
      "Iteration 1231/2000, Loss: 0.00033764613908715546\n",
      "Iteration 1232/2000, Loss: 0.0002838006184902042\n",
      "Iteration 1233/2000, Loss: 0.00038029300048947334\n",
      "Iteration 1234/2000, Loss: 0.000321822939440608\n",
      "Iteration 1235/2000, Loss: 0.00020984669390600175\n",
      "Iteration 1236/2000, Loss: 0.0003960035683121532\n",
      "Iteration 1237/2000, Loss: 0.00023294403217732906\n",
      "Iteration 1238/2000, Loss: 0.0002993983798660338\n",
      "Iteration 1239/2000, Loss: 0.00024800279061309993\n",
      "Iteration 1240/2000, Loss: 0.00040723985875956714\n",
      "Iteration 1241/2000, Loss: 0.00034748841426335275\n",
      "Iteration 1242/2000, Loss: 0.00032738520530983806\n",
      "Iteration 1243/2000, Loss: 0.00022470140538644046\n",
      "Iteration 1244/2000, Loss: 0.00026282796170562506\n",
      "Iteration 1245/2000, Loss: 0.00019879387400578707\n",
      "Iteration 1246/2000, Loss: 0.0002686335938051343\n",
      "Iteration 1247/2000, Loss: 0.0002451182226650417\n",
      "Iteration 1248/2000, Loss: 0.00022749698837287724\n",
      "Iteration 1249/2000, Loss: 0.00018216119497083127\n",
      "Iteration 1250/2000, Loss: 0.0003369932237546891\n",
      "Iteration 1251/2000, Loss: 0.0001702431618468836\n",
      "Iteration 1252/2000, Loss: 0.0001642288698349148\n",
      "Iteration 1253/2000, Loss: 0.00031703111017122865\n",
      "Iteration 1254/2000, Loss: 0.00013931444846093655\n",
      "Iteration 1255/2000, Loss: 0.000212801547604613\n",
      "Iteration 1256/2000, Loss: 0.00017714450950734317\n",
      "Iteration 1257/2000, Loss: 0.0001723596069496125\n",
      "Iteration 1258/2000, Loss: 0.0002771319996099919\n",
      "Iteration 1259/2000, Loss: 0.0002010342141147703\n",
      "Iteration 1260/2000, Loss: 0.00037118804175406694\n",
      "Iteration 1261/2000, Loss: 0.0003828239568974823\n",
      "Iteration 1262/2000, Loss: 0.00019883934874087572\n",
      "Iteration 1263/2000, Loss: 0.0002060020196950063\n",
      "Iteration 1264/2000, Loss: 0.00022645060380455106\n",
      "Iteration 1265/2000, Loss: 0.00026685770717449486\n",
      "Iteration 1266/2000, Loss: 0.00014991271018516272\n",
      "Iteration 1267/2000, Loss: 0.0002600443840492517\n",
      "Iteration 1268/2000, Loss: 0.00026902995887212455\n",
      "Iteration 1269/2000, Loss: 0.00012206417159177363\n",
      "Iteration 1270/2000, Loss: 0.00035422304063104093\n",
      "Iteration 1271/2000, Loss: 0.000363449624273926\n",
      "Iteration 1272/2000, Loss: 0.0002769850252661854\n",
      "Iteration 1273/2000, Loss: 0.00024132916587404907\n",
      "Iteration 1274/2000, Loss: 0.00015613609866704792\n",
      "Iteration 1275/2000, Loss: 0.00020679795125033706\n",
      "Iteration 1276/2000, Loss: 0.00014630219084210694\n",
      "Iteration 1277/2000, Loss: 0.0002044950524577871\n",
      "Iteration 1278/2000, Loss: 0.00024501601001247764\n",
      "Iteration 1279/2000, Loss: 0.00019145608530379832\n",
      "Iteration 1280/2000, Loss: 0.00016854515706654638\n",
      "Iteration 1281/2000, Loss: 0.00015119057206902653\n",
      "Iteration 1282/2000, Loss: 0.00023545236035715789\n",
      "Iteration 1283/2000, Loss: 0.00014777616888750345\n",
      "Iteration 1284/2000, Loss: 0.00011665985221043229\n",
      "Iteration 1285/2000, Loss: 0.0003142681089229882\n",
      "Iteration 1286/2000, Loss: 0.0001729584182612598\n",
      "Iteration 1287/2000, Loss: 0.00019201324903406203\n",
      "Iteration 1288/2000, Loss: 0.00016239218530245125\n",
      "Iteration 1289/2000, Loss: 0.00017903445404954255\n",
      "Iteration 1290/2000, Loss: 0.0001376517757307738\n",
      "Iteration 1291/2000, Loss: 0.00011192636156920344\n",
      "Iteration 1292/2000, Loss: 0.00011572601943043992\n",
      "Iteration 1293/2000, Loss: 0.00026992650236934423\n",
      "Iteration 1294/2000, Loss: 0.00018495973199605942\n",
      "Iteration 1295/2000, Loss: 0.00021387822926044464\n",
      "Iteration 1296/2000, Loss: 0.0005125178722664714\n",
      "Iteration 1297/2000, Loss: 0.0003044744662474841\n",
      "Iteration 1298/2000, Loss: 0.00033299074857495725\n",
      "Iteration 1299/2000, Loss: 0.0002694184659048915\n",
      "Iteration 1300/2000, Loss: 0.0002884990826714784\n",
      "Iteration 1301/2000, Loss: 0.0002018819359363988\n",
      "Iteration 1302/2000, Loss: 0.00026448280550539494\n",
      "Iteration 1303/2000, Loss: 0.00011250088573433459\n",
      "Iteration 1304/2000, Loss: 0.00028812879463657737\n",
      "Iteration 1305/2000, Loss: 0.00028655873029492795\n",
      "Iteration 1306/2000, Loss: 0.0002891776093747467\n",
      "Iteration 1307/2000, Loss: 0.00017847109120339155\n",
      "Iteration 1308/2000, Loss: 0.00022489743423648179\n",
      "Iteration 1309/2000, Loss: 0.0002458516974002123\n",
      "Iteration 1310/2000, Loss: 0.0002425678976578638\n",
      "Iteration 1311/2000, Loss: 0.000193274681805633\n",
      "Iteration 1312/2000, Loss: 0.00023830639838706702\n",
      "Iteration 1313/2000, Loss: 0.0001912484731292352\n",
      "Iteration 1314/2000, Loss: 0.0003601267235353589\n",
      "Iteration 1315/2000, Loss: 0.0001778115693014115\n",
      "Iteration 1316/2000, Loss: 0.0002224073396064341\n",
      "Iteration 1317/2000, Loss: 0.000275633588898927\n",
      "Iteration 1318/2000, Loss: 0.0002142476150766015\n",
      "Iteration 1319/2000, Loss: 0.00041334916022606194\n",
      "Iteration 1320/2000, Loss: 0.00019537482876330614\n",
      "Iteration 1321/2000, Loss: 0.00034858626895584166\n",
      "Iteration 1322/2000, Loss: 0.00033504317980259657\n",
      "Iteration 1323/2000, Loss: 0.0004917287733405828\n",
      "Iteration 1324/2000, Loss: 0.00021536195708904415\n",
      "Iteration 1325/2000, Loss: 0.00045592759852297604\n",
      "Iteration 1326/2000, Loss: 0.00022066307428758591\n",
      "Iteration 1327/2000, Loss: 0.000462106429040432\n",
      "Iteration 1328/2000, Loss: 0.0003925772034563124\n",
      "Iteration 1329/2000, Loss: 0.00021494059183169156\n",
      "Iteration 1330/2000, Loss: 0.00024997739819809794\n",
      "Iteration 1331/2000, Loss: 0.0002017140359384939\n",
      "Iteration 1332/2000, Loss: 0.0003288458101451397\n",
      "Iteration 1333/2000, Loss: 0.00017683263286016881\n",
      "Iteration 1334/2000, Loss: 0.0002472233900334686\n",
      "Iteration 1335/2000, Loss: 0.00017344376828987151\n",
      "Iteration 1336/2000, Loss: 0.00020601034339051694\n",
      "Iteration 1337/2000, Loss: 0.00017277305596508086\n",
      "Iteration 1338/2000, Loss: 0.00027576767024584115\n",
      "Iteration 1339/2000, Loss: 0.00018016538524534553\n",
      "Iteration 1340/2000, Loss: 0.00026202481240034103\n",
      "Iteration 1341/2000, Loss: 0.00016608848818577826\n",
      "Iteration 1342/2000, Loss: 0.00028903339989483356\n",
      "Iteration 1343/2000, Loss: 0.00021448321058414876\n",
      "Iteration 1344/2000, Loss: 0.0003140759654343128\n",
      "Iteration 1345/2000, Loss: 0.00010005812509916723\n",
      "Iteration 1346/2000, Loss: 0.0001787016080925241\n",
      "Iteration 1347/2000, Loss: 0.00016509798297192901\n",
      "Iteration 1348/2000, Loss: 0.00014644453767687082\n",
      "Iteration 1349/2000, Loss: 0.00025757894036360085\n",
      "Iteration 1350/2000, Loss: 0.00014136760728433728\n",
      "Iteration 1351/2000, Loss: 0.00030719503411091864\n",
      "Iteration 1352/2000, Loss: 0.00010039050539489836\n",
      "Iteration 1353/2000, Loss: 0.00017857665079645813\n",
      "Iteration 1354/2000, Loss: 0.00014445144915953279\n",
      "Iteration 1355/2000, Loss: 0.00027283839881420135\n",
      "Iteration 1356/2000, Loss: 0.00030723633244633675\n",
      "Iteration 1357/2000, Loss: 0.00015347755106631666\n",
      "Iteration 1358/2000, Loss: 0.00025538119371049106\n",
      "Iteration 1359/2000, Loss: 0.00018064976029563695\n",
      "Iteration 1360/2000, Loss: 0.00015977809380274266\n",
      "Iteration 1361/2000, Loss: 0.00022331290529109538\n",
      "Iteration 1362/2000, Loss: 0.000514891988132149\n",
      "Iteration 1363/2000, Loss: 8.951995550887659e-05\n",
      "Iteration 1364/2000, Loss: 0.0002669293317012489\n",
      "Iteration 1365/2000, Loss: 0.00016482309729326516\n",
      "Iteration 1366/2000, Loss: 0.0002063142746919766\n",
      "Iteration 1367/2000, Loss: 0.0002486332377884537\n",
      "Iteration 1368/2000, Loss: 0.0002961258578579873\n",
      "Iteration 1369/2000, Loss: 0.00012646210961975157\n",
      "Iteration 1370/2000, Loss: 0.00023659170255996287\n",
      "Iteration 1371/2000, Loss: 0.00030418584356084466\n",
      "Iteration 1372/2000, Loss: 0.00016465575026813895\n",
      "Iteration 1373/2000, Loss: 0.00018308723520021886\n",
      "Iteration 1374/2000, Loss: 0.00016748196503613144\n",
      "Iteration 1375/2000, Loss: 0.00044849139521829784\n",
      "Iteration 1376/2000, Loss: 0.00015300800441764295\n",
      "Iteration 1377/2000, Loss: 0.0003204264212399721\n",
      "Iteration 1378/2000, Loss: 0.00015584213542751968\n",
      "Iteration 1379/2000, Loss: 0.00015866542526055127\n",
      "Iteration 1380/2000, Loss: 0.00027212908025830984\n",
      "Iteration 1381/2000, Loss: 0.0002491349296178669\n",
      "Iteration 1382/2000, Loss: 0.00018661547801457345\n",
      "Iteration 1383/2000, Loss: 0.00025048325187526643\n",
      "Iteration 1384/2000, Loss: 0.0001500912185292691\n",
      "Iteration 1385/2000, Loss: 0.00015185904339887202\n",
      "Iteration 1386/2000, Loss: 0.00027828215388581157\n",
      "Iteration 1387/2000, Loss: 0.00016206289001274854\n",
      "Iteration 1388/2000, Loss: 0.0004783045151270926\n",
      "Iteration 1389/2000, Loss: 0.0001892311411211267\n",
      "Iteration 1390/2000, Loss: 0.0001643652212806046\n",
      "Iteration 1391/2000, Loss: 0.00015166596858762205\n",
      "Iteration 1392/2000, Loss: 0.00019519489433150738\n",
      "Iteration 1393/2000, Loss: 0.00026279425946995616\n",
      "Iteration 1394/2000, Loss: 0.00012247134873177856\n",
      "Iteration 1395/2000, Loss: 0.00012925763439852744\n",
      "Iteration 1396/2000, Loss: 0.00012267680722288787\n",
      "Iteration 1397/2000, Loss: 0.00014625911717303097\n",
      "Iteration 1398/2000, Loss: 0.00018505891785025597\n",
      "Iteration 1399/2000, Loss: 0.00018343408009968698\n",
      "Iteration 1400/2000, Loss: 0.00015072965470608324\n",
      "Iteration 1401/2000, Loss: 0.00011843800893984735\n",
      "Iteration 1402/2000, Loss: 0.0001607253507245332\n",
      "Iteration 1403/2000, Loss: 0.00023981332196854055\n",
      "Iteration 1404/2000, Loss: 0.0001351394603261724\n",
      "Iteration 1405/2000, Loss: 0.00016269652405753732\n",
      "Iteration 1406/2000, Loss: 0.0001532443129690364\n",
      "Iteration 1407/2000, Loss: 0.0001745670015225187\n",
      "Iteration 1408/2000, Loss: 0.0001827980304369703\n",
      "Iteration 1409/2000, Loss: 0.00018149783136323094\n",
      "Iteration 1410/2000, Loss: 0.0002293906727572903\n",
      "Iteration 1411/2000, Loss: 0.0003143924113828689\n",
      "Iteration 1412/2000, Loss: 0.00019337798585183918\n",
      "Iteration 1413/2000, Loss: 0.00014152424409985542\n",
      "Iteration 1414/2000, Loss: 0.0003124045906588435\n",
      "Iteration 1415/2000, Loss: 0.00029012898448854685\n",
      "Iteration 1416/2000, Loss: 0.00020063435658812523\n",
      "Iteration 1417/2000, Loss: 0.00012461721780709922\n",
      "Iteration 1418/2000, Loss: 0.0002359257487114519\n",
      "Iteration 1419/2000, Loss: 0.00034989905543625355\n",
      "Iteration 1420/2000, Loss: 0.0001483341766288504\n",
      "Iteration 1421/2000, Loss: 0.00019123355741612613\n",
      "Iteration 1422/2000, Loss: 0.0002598926657810807\n",
      "Iteration 1423/2000, Loss: 0.00020924329874105752\n",
      "Iteration 1424/2000, Loss: 0.00016749031783547252\n",
      "Iteration 1425/2000, Loss: 0.00022463097411673516\n",
      "Iteration 1426/2000, Loss: 0.000286365975625813\n",
      "Iteration 1427/2000, Loss: 0.00019682337006088346\n",
      "Iteration 1428/2000, Loss: 0.0002619643637444824\n",
      "Iteration 1429/2000, Loss: 0.0003019524156115949\n",
      "Iteration 1430/2000, Loss: 0.00017806203686632216\n",
      "Iteration 1431/2000, Loss: 0.00021240851492621005\n",
      "Iteration 1432/2000, Loss: 0.00014858643407933414\n",
      "Iteration 1433/2000, Loss: 0.00011489615280879661\n",
      "Iteration 1434/2000, Loss: 0.00019995111506432295\n",
      "Iteration 1435/2000, Loss: 0.0002542925940360874\n",
      "Iteration 1436/2000, Loss: 0.0002539640408940613\n",
      "Iteration 1437/2000, Loss: 0.0002018593077082187\n",
      "Iteration 1438/2000, Loss: 0.0002023101260419935\n",
      "Iteration 1439/2000, Loss: 0.00037102127680554986\n",
      "Iteration 1440/2000, Loss: 0.00017393630696460605\n",
      "Iteration 1441/2000, Loss: 0.00025584615650586784\n",
      "Iteration 1442/2000, Loss: 0.00021194873261265457\n",
      "Iteration 1443/2000, Loss: 0.0002947371976915747\n",
      "Iteration 1444/2000, Loss: 0.0002666715590748936\n",
      "Iteration 1445/2000, Loss: 0.0003265381383243948\n",
      "Iteration 1446/2000, Loss: 0.00022425770293921232\n",
      "Iteration 1447/2000, Loss: 0.0002417989308014512\n",
      "Iteration 1448/2000, Loss: 0.00044335355050861835\n",
      "Iteration 1449/2000, Loss: 0.0002890012401621789\n",
      "Iteration 1450/2000, Loss: 0.0001936412154464051\n",
      "Iteration 1451/2000, Loss: 0.0004603839770425111\n",
      "Iteration 1452/2000, Loss: 0.00046843799646012485\n",
      "Iteration 1453/2000, Loss: 0.00022229537717066705\n",
      "Iteration 1454/2000, Loss: 0.00021499133436009288\n",
      "Iteration 1455/2000, Loss: 0.0005920971161685884\n",
      "Iteration 1456/2000, Loss: 0.0004713532398454845\n",
      "Iteration 1457/2000, Loss: 0.000225597876124084\n",
      "Iteration 1458/2000, Loss: 0.0002229634701507166\n",
      "Iteration 1459/2000, Loss: 0.0003687324351631105\n",
      "Iteration 1460/2000, Loss: 0.00033392017940059304\n",
      "Iteration 1461/2000, Loss: 0.0008128606132231653\n",
      "Iteration 1462/2000, Loss: 0.0002761439245659858\n",
      "Iteration 1463/2000, Loss: 0.00020705009228549898\n",
      "Iteration 1464/2000, Loss: 0.0002577210543677211\n",
      "Iteration 1465/2000, Loss: 0.000265169539488852\n",
      "Iteration 1466/2000, Loss: 0.00028769660275429487\n",
      "Iteration 1467/2000, Loss: 0.0003354869841132313\n",
      "Iteration 1468/2000, Loss: 0.0002498260873835534\n",
      "Iteration 1469/2000, Loss: 0.0003087325021624565\n",
      "Iteration 1470/2000, Loss: 0.0002074744988931343\n",
      "Iteration 1471/2000, Loss: 0.0004314517427701503\n",
      "Iteration 1472/2000, Loss: 0.0002826071286108345\n",
      "Iteration 1473/2000, Loss: 0.00038770586252212524\n",
      "Iteration 1474/2000, Loss: 0.00023736357979942113\n",
      "Iteration 1475/2000, Loss: 0.00037995586171746254\n",
      "Iteration 1476/2000, Loss: 0.00023866200353950262\n",
      "Iteration 1477/2000, Loss: 0.0002703744394239038\n",
      "Iteration 1478/2000, Loss: 0.0002006105351028964\n",
      "Iteration 1479/2000, Loss: 0.00041663870797492564\n",
      "Iteration 1480/2000, Loss: 0.00031920155743137\n",
      "Iteration 1481/2000, Loss: 0.00018387020099908113\n",
      "Iteration 1482/2000, Loss: 0.00033768848516047\n",
      "Iteration 1483/2000, Loss: 0.00023217032139655203\n",
      "Iteration 1484/2000, Loss: 0.0003236141346860677\n",
      "Iteration 1485/2000, Loss: 0.0002149797510355711\n",
      "Iteration 1486/2000, Loss: 0.00024563062470406294\n",
      "Iteration 1487/2000, Loss: 0.0003398051194380969\n",
      "Iteration 1488/2000, Loss: 0.00035564808058552444\n",
      "Iteration 1489/2000, Loss: 0.00027214366127736866\n",
      "Iteration 1490/2000, Loss: 0.0002561570727266371\n",
      "Iteration 1491/2000, Loss: 0.0002489079488441348\n",
      "Iteration 1492/2000, Loss: 0.000312510208459571\n",
      "Iteration 1493/2000, Loss: 0.00019176841306034476\n",
      "Iteration 1494/2000, Loss: 0.0002188196376664564\n",
      "Iteration 1495/2000, Loss: 0.00029516947688534856\n",
      "Iteration 1496/2000, Loss: 0.0003003835736308247\n",
      "Iteration 1497/2000, Loss: 0.00023223168682307005\n",
      "Iteration 1498/2000, Loss: 0.00014420856314245611\n",
      "Iteration 1499/2000, Loss: 0.00022817983699496835\n",
      "Iteration 1500/2000, Loss: 0.00018314401677343994\n",
      "Iteration 1501/2000, Loss: 0.00036613186239264905\n",
      "Iteration 1502/2000, Loss: 0.0003724074922502041\n",
      "Iteration 1503/2000, Loss: 0.0012231714790686965\n",
      "Iteration 1504/2000, Loss: 0.0002518027904443443\n",
      "Iteration 1505/2000, Loss: 0.0006861106376163661\n",
      "Iteration 1506/2000, Loss: 0.0003540696343407035\n",
      "Iteration 1507/2000, Loss: 0.00023770751431584358\n",
      "Iteration 1508/2000, Loss: 0.0002865991264116019\n",
      "Iteration 1509/2000, Loss: 0.0002624166081659496\n",
      "Iteration 1510/2000, Loss: 0.0003076921566389501\n",
      "Iteration 1511/2000, Loss: 0.00039711000863462687\n",
      "Iteration 1512/2000, Loss: 0.00027140293968841434\n",
      "Iteration 1513/2000, Loss: 0.00032403558725491166\n",
      "Iteration 1514/2000, Loss: 0.00038422291981987655\n",
      "Iteration 1515/2000, Loss: 0.00032328220549970865\n",
      "Iteration 1516/2000, Loss: 0.0005619079456664622\n",
      "Iteration 1517/2000, Loss: 0.00019281494314782321\n",
      "Iteration 1518/2000, Loss: 0.00045225973008200526\n",
      "Iteration 1519/2000, Loss: 0.00046217135968618095\n",
      "Iteration 1520/2000, Loss: 0.0003655031032394618\n",
      "Iteration 1521/2000, Loss: 0.0001343594049103558\n",
      "Iteration 1522/2000, Loss: 0.000278322899248451\n",
      "Iteration 1523/2000, Loss: 0.0001884283556137234\n",
      "Iteration 1524/2000, Loss: 0.00044695156975649297\n",
      "Iteration 1525/2000, Loss: 0.00022498657926917076\n",
      "Iteration 1526/2000, Loss: 0.0005625526537187397\n",
      "Iteration 1527/2000, Loss: 0.00032299652229994535\n",
      "Iteration 1528/2000, Loss: 0.0010208833264186978\n",
      "Iteration 1529/2000, Loss: 0.0002092323120450601\n",
      "Iteration 1530/2000, Loss: 0.00044671486830338836\n",
      "Iteration 1531/2000, Loss: 0.00017970456974580884\n",
      "Iteration 1532/2000, Loss: 0.0004438762553036213\n",
      "Iteration 1533/2000, Loss: 0.00026279364828951657\n",
      "Iteration 1534/2000, Loss: 0.0006062726024538279\n",
      "Iteration 1535/2000, Loss: 0.00023700928431935608\n",
      "Iteration 1536/2000, Loss: 0.0006775764632038772\n",
      "Iteration 1537/2000, Loss: 0.00016942147340159863\n",
      "Iteration 1538/2000, Loss: 0.0003665095428004861\n",
      "Iteration 1539/2000, Loss: 0.0003976256703026593\n",
      "Iteration 1540/2000, Loss: 0.0005345888203009963\n",
      "Iteration 1541/2000, Loss: 0.000270734541118145\n",
      "Iteration 1542/2000, Loss: 0.001079305075109005\n",
      "Iteration 1543/2000, Loss: 0.00015169481048360467\n",
      "Iteration 1544/2000, Loss: 0.0003737719089258462\n",
      "Iteration 1545/2000, Loss: 0.0003056942659895867\n",
      "Iteration 1546/2000, Loss: 0.00044234064989723265\n",
      "Iteration 1547/2000, Loss: 0.00017821634537540376\n",
      "Iteration 1548/2000, Loss: 0.0001867140526883304\n",
      "Iteration 1549/2000, Loss: 0.0001348186342511326\n",
      "Iteration 1550/2000, Loss: 0.00021885993191972375\n",
      "Iteration 1551/2000, Loss: 0.00015679509670007974\n",
      "Iteration 1552/2000, Loss: 0.00015017563418950886\n",
      "Iteration 1553/2000, Loss: 9.940388554241508e-05\n",
      "Iteration 1554/2000, Loss: 0.0001564024860272184\n",
      "Iteration 1555/2000, Loss: 0.00017171091167256236\n",
      "Iteration 1556/2000, Loss: 0.0002125489991158247\n",
      "Iteration 1557/2000, Loss: 0.0003175177553202957\n",
      "Iteration 1558/2000, Loss: 0.00014767871471121907\n",
      "Iteration 1559/2000, Loss: 0.00011044432176277041\n",
      "Iteration 1560/2000, Loss: 9.630747808841988e-05\n",
      "Iteration 1561/2000, Loss: 0.00011535141675267369\n",
      "Iteration 1562/2000, Loss: 0.00014126574387773871\n",
      "Iteration 1563/2000, Loss: 0.0001517564378445968\n",
      "Iteration 1564/2000, Loss: 9.479038999415934e-05\n",
      "Iteration 1565/2000, Loss: 0.0005365266115404665\n",
      "Iteration 1566/2000, Loss: 0.000122227385872975\n",
      "Iteration 1567/2000, Loss: 0.00013376340211834759\n",
      "Iteration 1568/2000, Loss: 0.00015304952103178948\n",
      "Iteration 1569/2000, Loss: 0.00017546871094964445\n",
      "Iteration 1570/2000, Loss: 0.0002761068753898144\n",
      "Iteration 1571/2000, Loss: 0.00021745772392023355\n",
      "Iteration 1572/2000, Loss: 0.00020764186047017574\n",
      "Iteration 1573/2000, Loss: 0.0002656391588971019\n",
      "Iteration 1574/2000, Loss: 0.00013877201126888394\n",
      "Iteration 1575/2000, Loss: 0.00014658141299150884\n",
      "Iteration 1576/2000, Loss: 0.00020106036390643567\n",
      "Iteration 1577/2000, Loss: 0.00017962019774131477\n",
      "Iteration 1578/2000, Loss: 0.0003561453486327082\n",
      "Iteration 1579/2000, Loss: 0.0001557670475449413\n",
      "Iteration 1580/2000, Loss: 0.00024153897538781166\n",
      "Iteration 1581/2000, Loss: 0.0002886064467020333\n",
      "Iteration 1582/2000, Loss: 0.0002202131290687248\n",
      "Iteration 1583/2000, Loss: 0.00015415756206493825\n",
      "Iteration 1584/2000, Loss: 0.00024836271768435836\n",
      "Iteration 1585/2000, Loss: 0.0003712444449774921\n",
      "Iteration 1586/2000, Loss: 0.00026144785806536674\n",
      "Iteration 1587/2000, Loss: 0.0003388461482245475\n",
      "Iteration 1588/2000, Loss: 0.0002924442815128714\n",
      "Iteration 1589/2000, Loss: 0.00015519226144533604\n",
      "Iteration 1590/2000, Loss: 0.0002100782876368612\n",
      "Iteration 1591/2000, Loss: 0.00018102793546859175\n",
      "Iteration 1592/2000, Loss: 0.00019277833052910864\n",
      "Iteration 1593/2000, Loss: 0.00025838546571321785\n",
      "Iteration 1594/2000, Loss: 0.0001957920758286491\n",
      "Iteration 1595/2000, Loss: 0.00020287583174649626\n",
      "Iteration 1596/2000, Loss: 0.00011670750245684758\n",
      "Iteration 1597/2000, Loss: 0.00013619629316963255\n",
      "Iteration 1598/2000, Loss: 0.00016729632625356317\n",
      "Iteration 1599/2000, Loss: 0.00021791596373077482\n",
      "Iteration 1600/2000, Loss: 0.0001233237562701106\n",
      "Iteration 1601/2000, Loss: 0.00020788975234609097\n",
      "Iteration 1602/2000, Loss: 0.00016751124348957092\n",
      "Iteration 1603/2000, Loss: 0.0011687149526551366\n",
      "Iteration 1604/2000, Loss: 0.00022554726456291974\n",
      "Iteration 1605/2000, Loss: 0.00028572342125698924\n",
      "Iteration 1606/2000, Loss: 0.00029226450715214014\n",
      "Iteration 1607/2000, Loss: 0.00011942782293772325\n",
      "Iteration 1608/2000, Loss: 0.00015141685435082763\n",
      "Iteration 1609/2000, Loss: 0.00019258745305705816\n",
      "Iteration 1610/2000, Loss: 9.525895438855514e-05\n",
      "Iteration 1611/2000, Loss: 0.0001158459999714978\n",
      "Iteration 1612/2000, Loss: 0.00048295044689439237\n",
      "Iteration 1613/2000, Loss: 0.0002095784730045125\n",
      "Iteration 1614/2000, Loss: 0.00014123981236480176\n",
      "Iteration 1615/2000, Loss: 0.00013691348431166261\n",
      "Iteration 1616/2000, Loss: 0.00015126228390727192\n",
      "Iteration 1617/2000, Loss: 0.00020180466526653618\n",
      "Iteration 1618/2000, Loss: 0.0002982307050842792\n",
      "Iteration 1619/2000, Loss: 0.00025441774050705135\n",
      "Iteration 1620/2000, Loss: 0.00012441699800547212\n",
      "Iteration 1621/2000, Loss: 0.00035228076740168035\n",
      "Iteration 1622/2000, Loss: 0.00018955819541588426\n",
      "Iteration 1623/2000, Loss: 0.00034998939372599125\n",
      "Iteration 1624/2000, Loss: 0.0001446287496946752\n",
      "Iteration 1625/2000, Loss: 0.0003204865788575262\n",
      "Iteration 1626/2000, Loss: 0.0002280725457239896\n",
      "Iteration 1627/2000, Loss: 0.00020540380501188338\n",
      "Iteration 1628/2000, Loss: 0.00024216769088525325\n",
      "Iteration 1629/2000, Loss: 0.00014012611063662916\n",
      "Iteration 1630/2000, Loss: 0.0002543668379075825\n",
      "Iteration 1631/2000, Loss: 0.00014841121446806937\n",
      "Iteration 1632/2000, Loss: 0.00018664739036466926\n",
      "Iteration 1633/2000, Loss: 0.00047101720701903105\n",
      "Iteration 1634/2000, Loss: 0.00043388328049331903\n",
      "Iteration 1635/2000, Loss: 0.00032197104883380234\n",
      "Iteration 1636/2000, Loss: 0.00042509485501796007\n",
      "Iteration 1637/2000, Loss: 0.0001730846124701202\n",
      "Iteration 1638/2000, Loss: 0.00014990086492616683\n",
      "Iteration 1639/2000, Loss: 0.000140353586175479\n",
      "Iteration 1640/2000, Loss: 0.0001276274269912392\n",
      "Iteration 1641/2000, Loss: 0.00011464855924714357\n",
      "Iteration 1642/2000, Loss: 9.90402841125615e-05\n",
      "Iteration 1643/2000, Loss: 0.00012733177572954446\n",
      "Iteration 1644/2000, Loss: 0.0002288560353918001\n",
      "Iteration 1645/2000, Loss: 0.00018571909458842129\n",
      "Iteration 1646/2000, Loss: 0.00022036566224414855\n",
      "Iteration 1647/2000, Loss: 0.0002017048536799848\n",
      "Iteration 1648/2000, Loss: 0.0002861281391233206\n",
      "Iteration 1649/2000, Loss: 0.00019535755563993007\n",
      "Iteration 1650/2000, Loss: 0.00018413276120554656\n",
      "Iteration 1651/2000, Loss: 0.0002554927777964622\n",
      "Iteration 1652/2000, Loss: 0.00018962356261909008\n",
      "Iteration 1653/2000, Loss: 0.00020332130952738225\n",
      "Iteration 1654/2000, Loss: 0.00012520342716015875\n",
      "Iteration 1655/2000, Loss: 0.00021271186415106058\n",
      "Iteration 1656/2000, Loss: 0.0003184244560543448\n",
      "Iteration 1657/2000, Loss: 0.00015399331459775567\n",
      "Iteration 1658/2000, Loss: 0.00020091691112611443\n",
      "Iteration 1659/2000, Loss: 0.00017064812709577382\n",
      "Iteration 1660/2000, Loss: 0.00012930312368553132\n",
      "Iteration 1661/2000, Loss: 0.00036506386823020875\n",
      "Iteration 1662/2000, Loss: 0.00010733756789704785\n",
      "Iteration 1663/2000, Loss: 0.00021988993103150278\n",
      "Iteration 1664/2000, Loss: 0.0001127047507907264\n",
      "Iteration 1665/2000, Loss: 0.0006215008324943483\n",
      "Iteration 1666/2000, Loss: 0.0001817045413190499\n",
      "Iteration 1667/2000, Loss: 0.00019503652583807707\n",
      "Iteration 1668/2000, Loss: 0.00011628759966697544\n",
      "Iteration 1669/2000, Loss: 0.00011536111560417339\n",
      "Iteration 1670/2000, Loss: 0.00015494519902858883\n",
      "Iteration 1671/2000, Loss: 0.0001337995781796053\n",
      "Iteration 1672/2000, Loss: 0.000288580049527809\n",
      "Iteration 1673/2000, Loss: 0.00024266657419502735\n",
      "Iteration 1674/2000, Loss: 0.00023210234940052032\n",
      "Iteration 1675/2000, Loss: 0.0001600531250005588\n",
      "Iteration 1676/2000, Loss: 0.00021222035866230726\n",
      "Iteration 1677/2000, Loss: 0.0001457173639209941\n",
      "Iteration 1678/2000, Loss: 0.0004822323680855334\n",
      "Iteration 1679/2000, Loss: 0.00014455401105806231\n",
      "Iteration 1680/2000, Loss: 0.00013010839757043868\n",
      "Iteration 1681/2000, Loss: 0.0001516580960014835\n",
      "Iteration 1682/2000, Loss: 0.00039206616929732263\n",
      "Iteration 1683/2000, Loss: 0.00020061113173142076\n",
      "Iteration 1684/2000, Loss: 0.0003569783584680408\n",
      "Iteration 1685/2000, Loss: 0.0003062178730033338\n",
      "Iteration 1686/2000, Loss: 0.00022493911092169583\n",
      "Iteration 1687/2000, Loss: 0.0002132638037437573\n",
      "Iteration 1688/2000, Loss: 0.00011380476644262671\n",
      "Iteration 1689/2000, Loss: 0.00017820917128119618\n",
      "Iteration 1690/2000, Loss: 0.0002245562500320375\n",
      "Iteration 1691/2000, Loss: 0.0001603173732291907\n",
      "Iteration 1692/2000, Loss: 0.0001271732326131314\n",
      "Iteration 1693/2000, Loss: 0.00021237802866380662\n",
      "Iteration 1694/2000, Loss: 0.0001484486710978672\n",
      "Iteration 1695/2000, Loss: 0.00016044486255850643\n",
      "Iteration 1696/2000, Loss: 0.00019470686675049365\n",
      "Iteration 1697/2000, Loss: 0.0001883142103906721\n",
      "Iteration 1698/2000, Loss: 0.0001867922837845981\n",
      "Iteration 1699/2000, Loss: 0.0004716869443655014\n",
      "Iteration 1700/2000, Loss: 0.0001669654157012701\n",
      "Iteration 1701/2000, Loss: 0.0003958925371989608\n",
      "Iteration 1702/2000, Loss: 0.0002908423775807023\n",
      "Iteration 1703/2000, Loss: 0.0002154675021301955\n",
      "Iteration 1704/2000, Loss: 0.00023719015007372946\n",
      "Iteration 1705/2000, Loss: 0.0002341160288779065\n",
      "Iteration 1706/2000, Loss: 0.0003271120076533407\n",
      "Iteration 1707/2000, Loss: 0.00014091363118495792\n",
      "Iteration 1708/2000, Loss: 0.0003988100215792656\n",
      "Iteration 1709/2000, Loss: 0.0002648797526489943\n",
      "Iteration 1710/2000, Loss: 0.00021545229537878186\n",
      "Iteration 1711/2000, Loss: 0.00015990465180948377\n",
      "Iteration 1712/2000, Loss: 0.00012475662515498698\n",
      "Iteration 1713/2000, Loss: 0.00029151662602089345\n",
      "Iteration 1714/2000, Loss: 0.00017253225087188184\n",
      "Iteration 1715/2000, Loss: 0.0002617802529130131\n",
      "Iteration 1716/2000, Loss: 0.00017478347581345588\n",
      "Iteration 1717/2000, Loss: 0.00017674903210718185\n",
      "Iteration 1718/2000, Loss: 0.00025465950602665544\n",
      "Iteration 1719/2000, Loss: 0.00025294863735325634\n",
      "Iteration 1720/2000, Loss: 0.0001136091013904661\n",
      "Iteration 1721/2000, Loss: 0.0001720705913612619\n",
      "Iteration 1722/2000, Loss: 0.00023353597498498857\n",
      "Iteration 1723/2000, Loss: 0.00022556495969183743\n",
      "Iteration 1724/2000, Loss: 0.00026268328656442463\n",
      "Iteration 1725/2000, Loss: 0.00019321225408930331\n",
      "Iteration 1726/2000, Loss: 0.00031236145878210664\n",
      "Iteration 1727/2000, Loss: 0.00020553336071316153\n",
      "Iteration 1728/2000, Loss: 0.00043180063948966563\n",
      "Iteration 1729/2000, Loss: 0.0003027028578799218\n",
      "Iteration 1730/2000, Loss: 0.00017139411647804081\n",
      "Iteration 1731/2000, Loss: 0.00022990045545157045\n",
      "Iteration 1732/2000, Loss: 0.000145632671774365\n",
      "Iteration 1733/2000, Loss: 0.0002912955533247441\n",
      "Iteration 1734/2000, Loss: 0.0001227350439876318\n",
      "Iteration 1735/2000, Loss: 0.00033362756948918104\n",
      "Iteration 1736/2000, Loss: 0.0003373423242010176\n",
      "Iteration 1737/2000, Loss: 0.0001776013959897682\n",
      "Iteration 1738/2000, Loss: 0.00025785071193240583\n",
      "Iteration 1739/2000, Loss: 0.00020456228230614215\n",
      "Iteration 1740/2000, Loss: 0.00020608396152965724\n",
      "Iteration 1741/2000, Loss: 0.00022141601948533207\n",
      "Iteration 1742/2000, Loss: 0.00020511102047748864\n",
      "Iteration 1743/2000, Loss: 0.0002735752204898745\n",
      "Iteration 1744/2000, Loss: 0.00022313633235171437\n",
      "Iteration 1745/2000, Loss: 0.000349502544850111\n",
      "Iteration 1746/2000, Loss: 0.0002662499318830669\n",
      "Iteration 1747/2000, Loss: 0.00023932316980790347\n",
      "Iteration 1748/2000, Loss: 0.0001999019441427663\n",
      "Iteration 1749/2000, Loss: 0.00019188650185242295\n",
      "Iteration 1750/2000, Loss: 0.0001895211753435433\n",
      "Iteration 1751/2000, Loss: 0.0002833961043506861\n",
      "Iteration 1752/2000, Loss: 0.00014558332622982562\n",
      "Iteration 1753/2000, Loss: 0.0002996706170961261\n",
      "Iteration 1754/2000, Loss: 0.00017679011216387153\n",
      "Iteration 1755/2000, Loss: 0.0003068832738790661\n",
      "Iteration 1756/2000, Loss: 0.00022212874318938702\n",
      "Iteration 1757/2000, Loss: 0.00035919752554036677\n",
      "Iteration 1758/2000, Loss: 0.00014417944476008415\n",
      "Iteration 1759/2000, Loss: 0.0003458684659563005\n",
      "Iteration 1760/2000, Loss: 0.0001985567359952256\n",
      "Iteration 1761/2000, Loss: 0.0003835028037428856\n",
      "Iteration 1762/2000, Loss: 0.00022530145361088216\n",
      "Iteration 1763/2000, Loss: 0.00034035686985589564\n",
      "Iteration 1764/2000, Loss: 0.00023297843290492892\n",
      "Iteration 1765/2000, Loss: 0.0002817149506881833\n",
      "Iteration 1766/2000, Loss: 0.0003439003194216639\n",
      "Iteration 1767/2000, Loss: 0.00022498979524243623\n",
      "Iteration 1768/2000, Loss: 0.00037693133344873786\n",
      "Iteration 1769/2000, Loss: 0.00035784696228802204\n",
      "Iteration 1770/2000, Loss: 0.00021492295491043478\n",
      "Iteration 1771/2000, Loss: 0.00019686379528138787\n",
      "Iteration 1772/2000, Loss: 0.00017167111218441278\n",
      "Iteration 1773/2000, Loss: 0.00021249045676086098\n",
      "Iteration 1774/2000, Loss: 0.00017272913828492165\n",
      "Iteration 1775/2000, Loss: 0.00028813857352361083\n",
      "Iteration 1776/2000, Loss: 0.00029723328771069646\n",
      "Iteration 1777/2000, Loss: 0.0002550232456997037\n",
      "Iteration 1778/2000, Loss: 0.0003777938836719841\n",
      "Iteration 1779/2000, Loss: 0.00022142379020806402\n",
      "Iteration 1780/2000, Loss: 0.0002870448224712163\n",
      "Iteration 1781/2000, Loss: 0.00017832436424214393\n",
      "Iteration 1782/2000, Loss: 0.00043217887287028134\n",
      "Iteration 1783/2000, Loss: 0.00031689758179709315\n",
      "Iteration 1784/2000, Loss: 0.0009899426950141788\n",
      "Iteration 1785/2000, Loss: 0.00018398695101495832\n",
      "Iteration 1786/2000, Loss: 0.000348435016348958\n",
      "Iteration 1787/2000, Loss: 0.0004296228289604187\n",
      "Iteration 1788/2000, Loss: 0.00015333318151533604\n",
      "Iteration 1789/2000, Loss: 0.0002265117218485102\n",
      "Iteration 1790/2000, Loss: 0.0001590001629665494\n",
      "Iteration 1791/2000, Loss: 0.00018989857926499099\n",
      "Iteration 1792/2000, Loss: 0.00031242123804986477\n",
      "Iteration 1793/2000, Loss: 0.0004194846551399678\n",
      "Iteration 1794/2000, Loss: 0.0003539497556630522\n",
      "Iteration 1795/2000, Loss: 0.0004018206673208624\n",
      "Iteration 1796/2000, Loss: 0.0006360959960147738\n",
      "Iteration 1797/2000, Loss: 0.000230378529522568\n",
      "Iteration 1798/2000, Loss: 0.00018867319158744067\n",
      "Iteration 1799/2000, Loss: 0.00025988672859966755\n",
      "Iteration 1800/2000, Loss: 0.00036391461617313325\n",
      "Iteration 1801/2000, Loss: 0.00016713718650862575\n",
      "Iteration 1802/2000, Loss: 0.00019478371541481465\n",
      "Iteration 1803/2000, Loss: 0.0001907720579765737\n",
      "Iteration 1804/2000, Loss: 0.0014037404907867312\n",
      "Iteration 1805/2000, Loss: 0.0001945012336364016\n",
      "Iteration 1806/2000, Loss: 0.00019458822498563677\n",
      "Iteration 1807/2000, Loss: 0.00035148675669915974\n",
      "Iteration 1808/2000, Loss: 0.00013637705706059933\n",
      "Iteration 1809/2000, Loss: 0.0005662049516104162\n",
      "Iteration 1810/2000, Loss: 0.00012988281378056854\n",
      "Iteration 1811/2000, Loss: 0.00023392686853185296\n",
      "Iteration 1812/2000, Loss: 0.0002639759914018214\n",
      "Iteration 1813/2000, Loss: 0.0001331389939878136\n",
      "Iteration 1814/2000, Loss: 0.00024991322425194085\n",
      "Iteration 1815/2000, Loss: 0.00017175784159917384\n",
      "Iteration 1816/2000, Loss: 0.00016506620158907026\n",
      "Iteration 1817/2000, Loss: 0.0001623419811949134\n",
      "Iteration 1818/2000, Loss: 0.00031553645385429263\n",
      "Iteration 1819/2000, Loss: 0.00020520186808425933\n",
      "Iteration 1820/2000, Loss: 0.0002143271267414093\n",
      "Iteration 1821/2000, Loss: 0.0002398770157014951\n",
      "Iteration 1822/2000, Loss: 0.000158339535119012\n",
      "Iteration 1823/2000, Loss: 0.0002715218288358301\n",
      "Iteration 1824/2000, Loss: 0.0002705915831029415\n",
      "Iteration 1825/2000, Loss: 0.00017594281234778464\n",
      "Iteration 1826/2000, Loss: 0.00038006945396773517\n",
      "Iteration 1827/2000, Loss: 0.00017148035112768412\n",
      "Iteration 1828/2000, Loss: 0.00016936351312324405\n",
      "Iteration 1829/2000, Loss: 0.00019625993445515633\n",
      "Iteration 1830/2000, Loss: 0.00018772004113998264\n",
      "Iteration 1831/2000, Loss: 0.0005282038473524153\n",
      "Iteration 1832/2000, Loss: 0.0001454470620956272\n",
      "Iteration 1833/2000, Loss: 0.00028465184732340276\n",
      "Iteration 1834/2000, Loss: 0.00011670251114992425\n",
      "Iteration 1835/2000, Loss: 0.0002903291315305978\n",
      "Iteration 1836/2000, Loss: 0.00020928359299432486\n",
      "Iteration 1837/2000, Loss: 0.00021274122991599143\n",
      "Iteration 1838/2000, Loss: 0.00021918666607234627\n",
      "Iteration 1839/2000, Loss: 0.00018269842257723212\n",
      "Iteration 1840/2000, Loss: 8.972655632533133e-05\n",
      "Iteration 1841/2000, Loss: 0.00018313899636268616\n",
      "Iteration 1842/2000, Loss: 0.000237689062487334\n",
      "Iteration 1843/2000, Loss: 0.0003005650651175529\n",
      "Iteration 1844/2000, Loss: 0.0001770401286194101\n",
      "Iteration 1845/2000, Loss: 0.00012404052540659904\n",
      "Iteration 1846/2000, Loss: 0.00018281479424331337\n",
      "Iteration 1847/2000, Loss: 0.00016152590978890657\n",
      "Iteration 1848/2000, Loss: 0.0003523537889122963\n",
      "Iteration 1849/2000, Loss: 9.195962047670037e-05\n",
      "Iteration 1850/2000, Loss: 0.00013731158105656505\n",
      "Iteration 1851/2000, Loss: 0.0001622342533664778\n",
      "Iteration 1852/2000, Loss: 0.00017044550622813404\n",
      "Iteration 1853/2000, Loss: 0.00022843446640763432\n",
      "Iteration 1854/2000, Loss: 0.00036505857133306563\n",
      "Iteration 1855/2000, Loss: 0.0001357208820991218\n",
      "Iteration 1856/2000, Loss: 0.0001791299437172711\n",
      "Iteration 1857/2000, Loss: 0.000225483177928254\n",
      "Iteration 1858/2000, Loss: 0.00010837187437573448\n",
      "Iteration 1859/2000, Loss: 0.00030994435655884445\n",
      "Iteration 1860/2000, Loss: 0.0002403957914793864\n",
      "Iteration 1861/2000, Loss: 0.0004016621969640255\n",
      "Iteration 1862/2000, Loss: 0.00038216367829591036\n",
      "Iteration 1863/2000, Loss: 0.00019030044495593756\n",
      "Iteration 1864/2000, Loss: 0.0003304861602373421\n",
      "Iteration 1865/2000, Loss: 0.00018093184917233884\n",
      "Iteration 1866/2000, Loss: 0.00018406641902402043\n",
      "Iteration 1867/2000, Loss: 0.00012275039625819772\n",
      "Iteration 1868/2000, Loss: 0.00032713700784370303\n",
      "Iteration 1869/2000, Loss: 0.0003281485114712268\n",
      "Iteration 1870/2000, Loss: 0.00015583305503241718\n",
      "Iteration 1871/2000, Loss: 0.00023105416039470583\n",
      "Iteration 1872/2000, Loss: 0.0002757904876489192\n",
      "Iteration 1873/2000, Loss: 0.00026897029601968825\n",
      "Iteration 1874/2000, Loss: 0.00018924695905297995\n",
      "Iteration 1875/2000, Loss: 0.00021784499404020607\n",
      "Iteration 1876/2000, Loss: 0.00029202376026660204\n",
      "Iteration 1877/2000, Loss: 0.0007679474074393511\n",
      "Iteration 1878/2000, Loss: 0.0001801077596610412\n",
      "Iteration 1879/2000, Loss: 0.00012438208796083927\n",
      "Iteration 1880/2000, Loss: 0.0002274461294291541\n",
      "Iteration 1881/2000, Loss: 0.0002089977206196636\n",
      "Iteration 1882/2000, Loss: 0.0001968042051885277\n",
      "Iteration 1883/2000, Loss: 0.0003551409754436463\n",
      "Iteration 1884/2000, Loss: 0.00017691953689791262\n",
      "Iteration 1885/2000, Loss: 0.00017048481095116585\n",
      "Iteration 1886/2000, Loss: 0.000238155058468692\n",
      "Iteration 1887/2000, Loss: 0.00013102554657962173\n",
      "Iteration 1888/2000, Loss: 0.0001432169519830495\n",
      "Iteration 1889/2000, Loss: 0.00016981069347821176\n",
      "Iteration 1890/2000, Loss: 0.00029952049953863025\n",
      "Iteration 1891/2000, Loss: 0.0002165804908145219\n",
      "Iteration 1892/2000, Loss: 0.00014014594489708543\n",
      "Iteration 1893/2000, Loss: 0.00018670254212338477\n",
      "Iteration 1894/2000, Loss: 0.00017349708650726825\n",
      "Iteration 1895/2000, Loss: 0.0001279922726098448\n",
      "Iteration 1896/2000, Loss: 0.00036115743569098413\n",
      "Iteration 1897/2000, Loss: 0.00014108838513493538\n",
      "Iteration 1898/2000, Loss: 0.00018428501789458096\n",
      "Iteration 1899/2000, Loss: 0.0005442864494398236\n",
      "Iteration 1900/2000, Loss: 0.00015879828424658626\n",
      "Iteration 1901/2000, Loss: 0.00012824950681533664\n",
      "Iteration 1902/2000, Loss: 0.00016632277402095497\n",
      "Iteration 1903/2000, Loss: 0.00015599536709487438\n",
      "Iteration 1904/2000, Loss: 0.00013115484034642577\n",
      "Iteration 1905/2000, Loss: 0.00013887275417800993\n",
      "Iteration 1906/2000, Loss: 0.0001133956466219388\n",
      "Iteration 1907/2000, Loss: 0.00014485511928796768\n",
      "Iteration 1908/2000, Loss: 0.00015928175707813352\n",
      "Iteration 1909/2000, Loss: 0.00016133826284203678\n",
      "Iteration 1910/2000, Loss: 0.00022574332251679152\n",
      "Iteration 1911/2000, Loss: 0.0003543189959600568\n",
      "Iteration 1912/2000, Loss: 0.00016760575817897916\n",
      "Iteration 1913/2000, Loss: 0.0001664480078034103\n",
      "Iteration 1914/2000, Loss: 0.00012380916450638324\n",
      "Iteration 1915/2000, Loss: 0.00021979202574584633\n",
      "Iteration 1916/2000, Loss: 0.00022197913494892418\n",
      "Iteration 1917/2000, Loss: 0.00010514173482079059\n",
      "Iteration 1918/2000, Loss: 0.0001572312758071348\n",
      "Iteration 1919/2000, Loss: 8.95562770892866e-05\n",
      "Iteration 1920/2000, Loss: 0.00029390325653366745\n",
      "Iteration 1921/2000, Loss: 0.00026911552413366735\n",
      "Iteration 1922/2000, Loss: 0.00010209868923993781\n",
      "Iteration 1923/2000, Loss: 0.00011987304606009275\n",
      "Iteration 1924/2000, Loss: 0.00017784189549274743\n",
      "Iteration 1925/2000, Loss: 0.00021002940775360912\n",
      "Iteration 1926/2000, Loss: 0.00011777866166085005\n",
      "Iteration 1927/2000, Loss: 0.0002928852627519518\n",
      "Iteration 1928/2000, Loss: 0.00020554190268740058\n",
      "Iteration 1929/2000, Loss: 0.0001642868301132694\n",
      "Iteration 1930/2000, Loss: 0.0001332906394964084\n",
      "Iteration 1931/2000, Loss: 0.00017160671995952725\n",
      "Iteration 1932/2000, Loss: 0.00013652209599968046\n",
      "Iteration 1933/2000, Loss: 0.0001906838151626289\n",
      "Iteration 1934/2000, Loss: 0.00018368476594332606\n",
      "Iteration 1935/2000, Loss: 0.00013804242189507931\n",
      "Iteration 1936/2000, Loss: 0.0001206350716529414\n",
      "Iteration 1937/2000, Loss: 0.0002743579389061779\n",
      "Iteration 1938/2000, Loss: 0.00015026633627712727\n",
      "Iteration 1939/2000, Loss: 0.0001691798388492316\n",
      "Iteration 1940/2000, Loss: 0.00012620074267033488\n",
      "Iteration 1941/2000, Loss: 0.0001282411685679108\n",
      "Iteration 1942/2000, Loss: 8.828362479107454e-05\n",
      "Iteration 1943/2000, Loss: 0.00017844882677309215\n",
      "Iteration 1944/2000, Loss: 0.00011246531357755885\n",
      "Iteration 1945/2000, Loss: 0.00014971506607253104\n",
      "Iteration 1946/2000, Loss: 8.52887169457972e-05\n",
      "Iteration 1947/2000, Loss: 0.00019360941951163113\n",
      "Iteration 1948/2000, Loss: 9.180065535474569e-05\n",
      "Iteration 1949/2000, Loss: 0.00017791187565308064\n",
      "Iteration 1950/2000, Loss: 0.00011563190491870046\n",
      "Iteration 1951/2000, Loss: 0.00013742201554123312\n",
      "Iteration 1952/2000, Loss: 0.00020824999955948442\n",
      "Iteration 1953/2000, Loss: 0.0001842763158492744\n",
      "Iteration 1954/2000, Loss: 0.0001491201255703345\n",
      "Iteration 1955/2000, Loss: 0.00021546139032579958\n",
      "Iteration 1956/2000, Loss: 0.00020679912995547056\n",
      "Iteration 1957/2000, Loss: 0.0005130055942572653\n",
      "Iteration 1958/2000, Loss: 0.0001252994261449203\n",
      "Iteration 1959/2000, Loss: 0.00018202440696768463\n",
      "Iteration 1960/2000, Loss: 0.00010167183791054413\n",
      "Iteration 1961/2000, Loss: 0.00015010690549388528\n",
      "Iteration 1962/2000, Loss: 0.00010657333041308448\n",
      "Iteration 1963/2000, Loss: 0.00012255943147465587\n",
      "Iteration 1964/2000, Loss: 0.0002339171478524804\n",
      "Iteration 1965/2000, Loss: 0.00010122375533683226\n",
      "Iteration 1966/2000, Loss: 0.0001964188413694501\n",
      "Iteration 1967/2000, Loss: 0.00011463119881227612\n",
      "Iteration 1968/2000, Loss: 0.00012345839058980346\n",
      "Iteration 1969/2000, Loss: 0.0001430830016033724\n",
      "Iteration 1970/2000, Loss: 0.00013257695536594838\n",
      "Iteration 1971/2000, Loss: 0.00012023377348668873\n",
      "Iteration 1972/2000, Loss: 0.00019011100812349468\n",
      "Iteration 1973/2000, Loss: 0.000260455854004249\n",
      "Iteration 1974/2000, Loss: 9.901895828079432e-05\n",
      "Iteration 1975/2000, Loss: 0.00011027300206478685\n",
      "Iteration 1976/2000, Loss: 0.0007470918935723603\n",
      "Iteration 1977/2000, Loss: 0.00014871201710775495\n",
      "Iteration 1978/2000, Loss: 0.00018206895038019866\n",
      "Iteration 1979/2000, Loss: 0.00012767453154083341\n",
      "Iteration 1980/2000, Loss: 0.00018199747137259692\n",
      "Iteration 1981/2000, Loss: 0.0001343703333986923\n",
      "Iteration 1982/2000, Loss: 0.00019001173495780677\n",
      "Iteration 1983/2000, Loss: 0.0001508151472080499\n",
      "Iteration 1984/2000, Loss: 0.00014765505329705775\n",
      "Iteration 1985/2000, Loss: 0.00011079945397796109\n",
      "Iteration 1986/2000, Loss: 0.00025420618476346135\n",
      "Iteration 1987/2000, Loss: 0.0001879354240372777\n",
      "Iteration 1988/2000, Loss: 0.0001654095685807988\n",
      "Iteration 1989/2000, Loss: 0.00019590725423768163\n",
      "Iteration 1990/2000, Loss: 0.0001273183588637039\n",
      "Iteration 1991/2000, Loss: 0.0003154128207825124\n",
      "Iteration 1992/2000, Loss: 0.0001667867909418419\n",
      "Iteration 1993/2000, Loss: 0.0005863704718649387\n",
      "Iteration 1994/2000, Loss: 0.00011591361544560641\n",
      "Iteration 1995/2000, Loss: 0.00014692138938698918\n",
      "Iteration 1996/2000, Loss: 0.00014696780999656767\n",
      "Iteration 1997/2000, Loss: 0.0001609677856322378\n",
      "Iteration 1998/2000, Loss: 0.00023357417376246303\n",
      "Iteration 1999/2000, Loss: 0.00018221196660306305\n",
      "Iteration 2000/2000, Loss: 0.0001452074502594769\n",
      "Model weights saved after training and testing with linewidth 100000.0 Hz.\n",
      "\n",
      "\n",
      "Testing with linewidth: 100000.0 Hz and Distance: 5000.0 km\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Testing MSE - Linewidth: 100000.0, Link Distance: 5000.0, Original: 0.005776990670710802, Neural Network: 0.00427792314440012\n",
      "\n",
      "Training with linewidth: 200000.0 Hz\n",
      "Loaded model weights for continued training.\n",
      "\n",
      "Training with linewidth: 200000.0 Hz and Distance: 1000.0 km\n",
      "Iteration 1/2000, Loss: 0.0002647950313985348\n",
      "Iteration 2/2000, Loss: 0.00016621135000605136\n",
      "Iteration 3/2000, Loss: 0.00014491811452899128\n",
      "Iteration 4/2000, Loss: 0.00012834471999667585\n",
      "Iteration 5/2000, Loss: 0.0001073146122507751\n",
      "Iteration 6/2000, Loss: 0.00012746700667776167\n",
      "Iteration 7/2000, Loss: 0.00027981578023172915\n",
      "Iteration 8/2000, Loss: 9.914249676512554e-05\n",
      "Iteration 9/2000, Loss: 0.00025311417994089425\n",
      "Iteration 10/2000, Loss: 0.00018238679331261665\n",
      "Iteration 11/2000, Loss: 0.00012694888573605567\n",
      "Iteration 12/2000, Loss: 0.00037281893310137093\n",
      "Iteration 13/2000, Loss: 0.00013570730516221374\n",
      "Iteration 14/2000, Loss: 0.0003554481954779476\n",
      "Iteration 15/2000, Loss: 0.00021994042617734522\n",
      "Iteration 16/2000, Loss: 0.0002735569141805172\n",
      "Iteration 17/2000, Loss: 0.00017949905304703861\n",
      "Iteration 18/2000, Loss: 0.00022544250532519072\n",
      "Iteration 19/2000, Loss: 0.0002019631356233731\n",
      "Iteration 20/2000, Loss: 0.0001333076652372256\n",
      "Iteration 21/2000, Loss: 0.00017406635743100196\n",
      "Iteration 22/2000, Loss: 0.00014163127343636006\n",
      "Iteration 23/2000, Loss: 0.00021715331240557134\n",
      "Iteration 24/2000, Loss: 0.00013266026508063078\n",
      "Iteration 25/2000, Loss: 0.00011670947424136102\n",
      "Iteration 26/2000, Loss: 0.00012015066022286192\n",
      "Iteration 27/2000, Loss: 0.00011094153160229325\n",
      "Iteration 28/2000, Loss: 0.0001574029738549143\n",
      "Iteration 29/2000, Loss: 0.00015997746959328651\n",
      "Iteration 30/2000, Loss: 0.00019991920271422714\n",
      "Iteration 31/2000, Loss: 0.00015451373474206775\n",
      "Iteration 32/2000, Loss: 0.0001709042553557083\n",
      "Iteration 33/2000, Loss: 0.00013343652244657278\n",
      "Iteration 34/2000, Loss: 0.00015045705367811024\n",
      "Iteration 35/2000, Loss: 0.00013324535393621773\n",
      "Iteration 36/2000, Loss: 0.00016860070172697306\n",
      "Iteration 37/2000, Loss: 0.00025794669636525214\n",
      "Iteration 38/2000, Loss: 0.0002281331835547462\n",
      "Iteration 39/2000, Loss: 0.0003010830841958523\n",
      "Iteration 40/2000, Loss: 0.00019443225755821913\n",
      "Iteration 41/2000, Loss: 0.00041539774974808097\n",
      "Iteration 42/2000, Loss: 0.0001405060465913266\n",
      "Iteration 43/2000, Loss: 0.00025799262220971286\n",
      "Iteration 44/2000, Loss: 0.00018437631661072373\n",
      "Iteration 45/2000, Loss: 0.00015863365842960775\n",
      "Iteration 46/2000, Loss: 0.00018576881848275661\n",
      "Iteration 47/2000, Loss: 0.0001763722684700042\n",
      "Iteration 48/2000, Loss: 0.0001200672413688153\n",
      "Iteration 49/2000, Loss: 0.0003184234374202788\n",
      "Iteration 50/2000, Loss: 0.0002055837685475126\n",
      "Iteration 51/2000, Loss: 0.0002502645365893841\n",
      "Iteration 52/2000, Loss: 0.0003537915472406894\n",
      "Iteration 53/2000, Loss: 0.00016927333490457386\n",
      "Iteration 54/2000, Loss: 0.00027970332303084433\n",
      "Iteration 55/2000, Loss: 0.0001348068763036281\n",
      "Iteration 56/2000, Loss: 0.00021140571334399283\n",
      "Iteration 57/2000, Loss: 0.00011053295020246878\n",
      "Iteration 58/2000, Loss: 0.0002779783389996737\n",
      "Iteration 59/2000, Loss: 0.00016965904796961695\n",
      "Iteration 60/2000, Loss: 0.00021732674213126302\n",
      "Iteration 61/2000, Loss: 0.00016685816808603704\n",
      "Iteration 62/2000, Loss: 0.00033418048406019807\n",
      "Iteration 63/2000, Loss: 0.0002475045039318502\n",
      "Iteration 64/2000, Loss: 0.00028434142586775124\n",
      "Iteration 65/2000, Loss: 0.0001681175926933065\n",
      "Iteration 66/2000, Loss: 0.00013887901150155813\n",
      "Iteration 67/2000, Loss: 0.0002613073156680912\n",
      "Iteration 68/2000, Loss: 0.00028193980688229203\n",
      "Iteration 69/2000, Loss: 0.00020104744180571288\n",
      "Iteration 70/2000, Loss: 0.000286236492684111\n",
      "Iteration 71/2000, Loss: 0.0002074143849313259\n",
      "Iteration 72/2000, Loss: 0.00021432781068142503\n",
      "Iteration 73/2000, Loss: 0.00019184575648978353\n",
      "Iteration 74/2000, Loss: 0.00028027372900396585\n",
      "Iteration 75/2000, Loss: 0.00023442890960723162\n",
      "Iteration 76/2000, Loss: 0.00024560975725762546\n",
      "Iteration 77/2000, Loss: 0.00018206080130767077\n",
      "Iteration 78/2000, Loss: 0.00031867288635112345\n",
      "Iteration 79/2000, Loss: 0.00020146017777733505\n",
      "Iteration 80/2000, Loss: 0.00027473559021018445\n",
      "Iteration 81/2000, Loss: 0.00024203000066336244\n",
      "Iteration 82/2000, Loss: 0.0001096017804229632\n",
      "Iteration 83/2000, Loss: 0.0003650190483313054\n",
      "Iteration 84/2000, Loss: 0.000183673037099652\n",
      "Iteration 85/2000, Loss: 0.00039924727752804756\n",
      "Iteration 86/2000, Loss: 0.0002842178801074624\n",
      "Iteration 87/2000, Loss: 0.00016491174756083637\n",
      "Iteration 88/2000, Loss: 0.00044218197581358254\n",
      "Iteration 89/2000, Loss: 0.0004928921116515994\n",
      "Iteration 90/2000, Loss: 0.00020874824258498847\n",
      "Iteration 91/2000, Loss: 0.00013366714119911194\n",
      "Iteration 92/2000, Loss: 0.00022376673587132245\n",
      "Iteration 93/2000, Loss: 0.000250720651820302\n",
      "Iteration 94/2000, Loss: 0.00014844552788417786\n",
      "Iteration 95/2000, Loss: 0.0001929934078361839\n",
      "Iteration 96/2000, Loss: 0.0002822021197061986\n",
      "Iteration 97/2000, Loss: 0.00021617267339024693\n",
      "Iteration 98/2000, Loss: 0.00026774968137033284\n",
      "Iteration 99/2000, Loss: 0.0002703111676964909\n",
      "Iteration 100/2000, Loss: 0.0003638553898781538\n",
      "Iteration 101/2000, Loss: 0.00015237908519338816\n",
      "Iteration 102/2000, Loss: 0.0002161495212931186\n",
      "Iteration 103/2000, Loss: 0.00017993419896811247\n",
      "Iteration 104/2000, Loss: 0.0002909589384216815\n",
      "Iteration 105/2000, Loss: 0.0003630342544056475\n",
      "Iteration 106/2000, Loss: 0.00024713636958040297\n",
      "Iteration 107/2000, Loss: 0.0001473722659284249\n",
      "Iteration 108/2000, Loss: 0.0002173081593355164\n",
      "Iteration 109/2000, Loss: 0.0001914573076646775\n",
      "Iteration 110/2000, Loss: 0.00035107415169477463\n",
      "Iteration 111/2000, Loss: 0.00022958764748182148\n",
      "Iteration 112/2000, Loss: 0.00020848696294706315\n",
      "Iteration 113/2000, Loss: 0.00017168253543786705\n",
      "Iteration 114/2000, Loss: 0.00023462658282369375\n",
      "Iteration 115/2000, Loss: 0.00014598743291571736\n",
      "Iteration 116/2000, Loss: 0.0003780054103117436\n",
      "Iteration 117/2000, Loss: 0.00015517282008659095\n",
      "Iteration 118/2000, Loss: 0.0002393442118773237\n",
      "Iteration 119/2000, Loss: 0.0001524035760667175\n",
      "Iteration 120/2000, Loss: 0.0001718513376545161\n",
      "Iteration 121/2000, Loss: 0.00013164970732759684\n",
      "Iteration 122/2000, Loss: 0.00016709927876945585\n",
      "Iteration 123/2000, Loss: 0.00019146307022310793\n",
      "Iteration 124/2000, Loss: 0.00020319574105087668\n",
      "Iteration 125/2000, Loss: 0.0001424214569851756\n",
      "Iteration 126/2000, Loss: 0.00026921305106952786\n",
      "Iteration 127/2000, Loss: 0.0001445580564904958\n",
      "Iteration 128/2000, Loss: 0.00021227820252534002\n",
      "Iteration 129/2000, Loss: 0.0001976867497432977\n",
      "Iteration 130/2000, Loss: 0.0001238321274286136\n",
      "Iteration 131/2000, Loss: 0.00017375871539115906\n",
      "Iteration 132/2000, Loss: 0.0001830700784921646\n",
      "Iteration 133/2000, Loss: 0.00016245139704551548\n",
      "Iteration 134/2000, Loss: 0.00018832585192285478\n",
      "Iteration 135/2000, Loss: 0.0001622344716452062\n",
      "Iteration 136/2000, Loss: 0.0001489545393269509\n",
      "Iteration 137/2000, Loss: 0.00013425487850327045\n",
      "Iteration 138/2000, Loss: 0.0001410857221344486\n",
      "Iteration 139/2000, Loss: 0.00018481502775102854\n",
      "Iteration 140/2000, Loss: 0.00012212707952130586\n",
      "Iteration 141/2000, Loss: 0.0003163654764648527\n",
      "Iteration 142/2000, Loss: 0.0001483344385633245\n",
      "Iteration 143/2000, Loss: 0.0005907953018322587\n",
      "Iteration 144/2000, Loss: 0.00013592738832812756\n",
      "Iteration 145/2000, Loss: 0.00022888235980644822\n",
      "Iteration 146/2000, Loss: 0.00019375808187760413\n",
      "Iteration 147/2000, Loss: 0.00020560252596624196\n",
      "Iteration 148/2000, Loss: 0.00020515448704827577\n",
      "Iteration 149/2000, Loss: 0.0001641652634134516\n",
      "Iteration 150/2000, Loss: 0.00021978483709972352\n",
      "Iteration 151/2000, Loss: 0.00016896789020393044\n",
      "Iteration 152/2000, Loss: 0.00019566138507798314\n",
      "Iteration 153/2000, Loss: 0.00015098336734808981\n",
      "Iteration 154/2000, Loss: 0.00015873543452471495\n",
      "Iteration 155/2000, Loss: 0.00016064305964391679\n",
      "Iteration 156/2000, Loss: 0.00030692151631228626\n",
      "Iteration 157/2000, Loss: 0.00017069579917006195\n",
      "Iteration 158/2000, Loss: 0.00026367942336946726\n",
      "Iteration 159/2000, Loss: 0.0003370923222973943\n",
      "Iteration 160/2000, Loss: 0.00017231500532943755\n",
      "Iteration 161/2000, Loss: 0.00037290219916030765\n",
      "Iteration 162/2000, Loss: 0.00018624762014951557\n",
      "Iteration 163/2000, Loss: 0.0003663412353489548\n",
      "Iteration 164/2000, Loss: 0.0003480588784441352\n",
      "Iteration 165/2000, Loss: 0.0003226878179702908\n",
      "Iteration 166/2000, Loss: 0.0002258641761727631\n",
      "Iteration 167/2000, Loss: 0.00031837340793572366\n",
      "Iteration 168/2000, Loss: 0.00018342334078624845\n",
      "Iteration 169/2000, Loss: 0.000281953951343894\n",
      "Iteration 170/2000, Loss: 0.00027741945814341307\n",
      "Iteration 171/2000, Loss: 0.00025454541901126504\n",
      "Iteration 172/2000, Loss: 0.00011409423314034939\n",
      "Iteration 173/2000, Loss: 0.00023281578614842147\n",
      "Iteration 174/2000, Loss: 0.0002130587527062744\n",
      "Iteration 175/2000, Loss: 0.0001948310964507982\n",
      "Iteration 176/2000, Loss: 0.00018463609740138054\n",
      "Iteration 177/2000, Loss: 0.00017594746896065772\n",
      "Iteration 178/2000, Loss: 0.00018391797493677586\n",
      "Iteration 179/2000, Loss: 0.000335562537657097\n",
      "Iteration 180/2000, Loss: 0.00024104220210574567\n",
      "Iteration 181/2000, Loss: 0.0003194116288796067\n",
      "Iteration 182/2000, Loss: 0.00019223363779019564\n",
      "Iteration 183/2000, Loss: 0.00011798078776337206\n",
      "Iteration 184/2000, Loss: 0.000366228079656139\n",
      "Iteration 185/2000, Loss: 0.00035102228866890073\n",
      "Iteration 186/2000, Loss: 0.00018627491954248399\n",
      "Iteration 187/2000, Loss: 0.00017092721827793866\n",
      "Iteration 188/2000, Loss: 0.0003405639436095953\n",
      "Iteration 189/2000, Loss: 0.00017233157996088266\n",
      "Iteration 190/2000, Loss: 0.00020315925939939916\n",
      "Iteration 191/2000, Loss: 0.00024008844047784805\n",
      "Iteration 192/2000, Loss: 0.00014574246597476304\n",
      "Iteration 193/2000, Loss: 0.00015348276065196842\n",
      "Iteration 194/2000, Loss: 0.00014182164159137756\n",
      "Iteration 195/2000, Loss: 0.00013164084521122277\n",
      "Iteration 196/2000, Loss: 0.000132224740809761\n",
      "Iteration 197/2000, Loss: 0.0002669807290658355\n",
      "Iteration 198/2000, Loss: 0.00018458702834323049\n",
      "Iteration 199/2000, Loss: 0.00011405447730794549\n",
      "Iteration 200/2000, Loss: 0.0001665708259679377\n",
      "Iteration 201/2000, Loss: 0.00010127019777428359\n",
      "Iteration 202/2000, Loss: 0.00017431315791327506\n",
      "Iteration 203/2000, Loss: 0.00020308374951127917\n",
      "Iteration 204/2000, Loss: 0.00013793613470625132\n",
      "Iteration 205/2000, Loss: 0.0002656553697306663\n",
      "Iteration 206/2000, Loss: 0.00013202332775108516\n",
      "Iteration 207/2000, Loss: 0.0001480036007706076\n",
      "Iteration 208/2000, Loss: 0.00019027072994504124\n",
      "Iteration 209/2000, Loss: 0.00011081747652497143\n",
      "Iteration 210/2000, Loss: 0.00020597304683178663\n",
      "Iteration 211/2000, Loss: 0.00011768853437388316\n",
      "Iteration 212/2000, Loss: 0.000273332727374509\n",
      "Iteration 213/2000, Loss: 0.00019723872537724674\n",
      "Iteration 214/2000, Loss: 0.0001396593579556793\n",
      "Iteration 215/2000, Loss: 0.00010487284453120083\n",
      "Iteration 216/2000, Loss: 0.00015696849732194096\n",
      "Iteration 217/2000, Loss: 0.00019361947488505393\n",
      "Iteration 218/2000, Loss: 0.0001934077445184812\n",
      "Iteration 219/2000, Loss: 0.00020529807079583406\n",
      "Iteration 220/2000, Loss: 0.00022119948698673397\n",
      "Iteration 221/2000, Loss: 0.00028060507611371577\n",
      "Iteration 222/2000, Loss: 0.00018731909221969545\n",
      "Iteration 223/2000, Loss: 0.0002852025208994746\n",
      "Iteration 224/2000, Loss: 0.00026155399973504245\n",
      "Iteration 225/2000, Loss: 0.00019222385890316218\n",
      "Iteration 226/2000, Loss: 0.0001971819729078561\n",
      "Iteration 227/2000, Loss: 0.00044181672274135053\n",
      "Iteration 228/2000, Loss: 0.00012253544991835952\n",
      "Iteration 229/2000, Loss: 0.0003001583681907505\n",
      "Iteration 230/2000, Loss: 0.00013690530613530427\n",
      "Iteration 231/2000, Loss: 0.0003294429916422814\n",
      "Iteration 232/2000, Loss: 0.0002917609817814082\n",
      "Iteration 233/2000, Loss: 0.00023373449221253395\n",
      "Iteration 234/2000, Loss: 0.0004187656450085342\n",
      "Iteration 235/2000, Loss: 0.00015639184857718647\n",
      "Iteration 236/2000, Loss: 0.00025017099687829614\n",
      "Iteration 237/2000, Loss: 0.00010137047502212226\n",
      "Iteration 238/2000, Loss: 0.0006092698313295841\n",
      "Iteration 239/2000, Loss: 0.0008735028095543385\n",
      "Iteration 240/2000, Loss: 0.00020599956042133272\n",
      "Iteration 241/2000, Loss: 0.00023698694712948054\n",
      "Iteration 242/2000, Loss: 0.0002417437790427357\n",
      "Iteration 243/2000, Loss: 0.0001914220629259944\n",
      "Iteration 244/2000, Loss: 0.00016935351595748216\n",
      "Iteration 245/2000, Loss: 0.00016193435294553638\n",
      "Iteration 246/2000, Loss: 0.00020068946469109505\n",
      "Iteration 247/2000, Loss: 0.00018728060240391642\n",
      "Iteration 248/2000, Loss: 0.0001690913486527279\n",
      "Iteration 249/2000, Loss: 0.00013921626668889076\n",
      "Iteration 250/2000, Loss: 0.0002602704626042396\n",
      "Iteration 251/2000, Loss: 0.00022687713499180973\n",
      "Iteration 252/2000, Loss: 0.00011875969357788563\n",
      "Iteration 253/2000, Loss: 0.00028501247288659215\n",
      "Iteration 254/2000, Loss: 0.00015877177065704018\n",
      "Iteration 255/2000, Loss: 0.0003724646521732211\n",
      "Iteration 256/2000, Loss: 0.0003371982602402568\n",
      "Iteration 257/2000, Loss: 0.00012197041360195726\n",
      "Iteration 258/2000, Loss: 0.0005033517372794449\n",
      "Iteration 259/2000, Loss: 0.00015220927889458835\n",
      "Iteration 260/2000, Loss: 0.00016260585107374936\n",
      "Iteration 261/2000, Loss: 0.0002727595274336636\n",
      "Iteration 262/2000, Loss: 0.00027458067052066326\n",
      "Iteration 263/2000, Loss: 0.00015278809587471187\n",
      "Iteration 264/2000, Loss: 0.00018914244719780982\n",
      "Iteration 265/2000, Loss: 0.0003568977117538452\n",
      "Iteration 266/2000, Loss: 0.00022506444656755775\n",
      "Iteration 267/2000, Loss: 0.00013235138612799346\n",
      "Iteration 268/2000, Loss: 0.0003646852564997971\n",
      "Iteration 269/2000, Loss: 0.00014645364717580378\n",
      "Iteration 270/2000, Loss: 0.0002648102235980332\n",
      "Iteration 271/2000, Loss: 0.0003483044565655291\n",
      "Iteration 272/2000, Loss: 0.00033047236502170563\n",
      "Iteration 273/2000, Loss: 0.00017662935715634376\n",
      "Iteration 274/2000, Loss: 0.00022958772024139762\n",
      "Iteration 275/2000, Loss: 0.00017703676712699234\n",
      "Iteration 276/2000, Loss: 0.00026143889408558607\n",
      "Iteration 277/2000, Loss: 0.00044636012171395123\n",
      "Iteration 278/2000, Loss: 0.00022859756427351385\n",
      "Iteration 279/2000, Loss: 0.00043689156882464886\n",
      "Iteration 280/2000, Loss: 0.00022259926481638104\n",
      "Iteration 281/2000, Loss: 0.00041126221185550094\n",
      "Iteration 282/2000, Loss: 0.0005144436145201325\n",
      "Iteration 283/2000, Loss: 0.00016095521277748048\n",
      "Iteration 284/2000, Loss: 0.0004582042747642845\n",
      "Iteration 285/2000, Loss: 0.00038456488982774317\n",
      "Iteration 286/2000, Loss: 0.00017145404126495123\n",
      "Iteration 287/2000, Loss: 0.00025543654919601977\n",
      "Iteration 288/2000, Loss: 0.00018316645582672209\n",
      "Iteration 289/2000, Loss: 0.00029306902433745563\n",
      "Iteration 290/2000, Loss: 0.00027498684357851744\n",
      "Iteration 291/2000, Loss: 0.0002466913138050586\n",
      "Iteration 292/2000, Loss: 0.0002059156831819564\n",
      "Iteration 293/2000, Loss: 0.0002840300730895251\n",
      "Iteration 294/2000, Loss: 0.00020305869111325592\n",
      "Iteration 295/2000, Loss: 0.0002746596292126924\n",
      "Iteration 296/2000, Loss: 0.00030893587972968817\n",
      "Iteration 297/2000, Loss: 0.00018043439195025712\n",
      "Iteration 298/2000, Loss: 0.0002547385811340064\n",
      "Iteration 299/2000, Loss: 0.00018556743452791125\n",
      "Iteration 300/2000, Loss: 0.00015111170068848878\n",
      "Iteration 301/2000, Loss: 0.00034927212982438505\n",
      "Iteration 302/2000, Loss: 0.00015086514758877456\n",
      "Iteration 303/2000, Loss: 0.0002781332877930254\n",
      "Iteration 304/2000, Loss: 0.0001419791515218094\n",
      "Iteration 305/2000, Loss: 0.000211208505788818\n",
      "Iteration 306/2000, Loss: 0.0001647892058826983\n",
      "Iteration 307/2000, Loss: 0.0002782330848276615\n",
      "Iteration 308/2000, Loss: 0.000177057518158108\n",
      "Iteration 309/2000, Loss: 0.00015793922648299485\n",
      "Iteration 310/2000, Loss: 0.00011455416824901477\n",
      "Iteration 311/2000, Loss: 0.00046480182209052145\n",
      "Iteration 312/2000, Loss: 0.0003788597241509706\n",
      "Iteration 313/2000, Loss: 0.0006072910036891699\n",
      "Iteration 314/2000, Loss: 0.00021853356156498194\n",
      "Iteration 315/2000, Loss: 0.00018807365268003196\n",
      "Iteration 316/2000, Loss: 0.00021789751190226525\n",
      "Iteration 317/2000, Loss: 0.00019127193081658334\n",
      "Iteration 318/2000, Loss: 0.000289637508103624\n",
      "Iteration 319/2000, Loss: 0.00025832047685980797\n",
      "Iteration 320/2000, Loss: 0.00024335690250154585\n",
      "Iteration 321/2000, Loss: 0.00018541260214988142\n",
      "Iteration 322/2000, Loss: 0.00015633252041880041\n",
      "Iteration 323/2000, Loss: 0.00028880671015940607\n",
      "Iteration 324/2000, Loss: 0.00020303040219005197\n",
      "Iteration 325/2000, Loss: 0.00023378734476864338\n",
      "Iteration 326/2000, Loss: 0.0003119246393907815\n",
      "Iteration 327/2000, Loss: 0.00022866277140565217\n",
      "Iteration 328/2000, Loss: 0.00012334695202298462\n",
      "Iteration 329/2000, Loss: 0.0002701652410905808\n",
      "Iteration 330/2000, Loss: 0.00017360922356601804\n",
      "Iteration 331/2000, Loss: 0.00019972596783190966\n",
      "Iteration 332/2000, Loss: 0.00030677361064590514\n",
      "Iteration 333/2000, Loss: 0.00016749858332332224\n",
      "Iteration 334/2000, Loss: 0.00028103881049901247\n",
      "Iteration 335/2000, Loss: 0.00022882821212988347\n",
      "Iteration 336/2000, Loss: 0.00040413346141576767\n",
      "Iteration 337/2000, Loss: 0.00023210590006783605\n",
      "Iteration 338/2000, Loss: 0.0002572572266217321\n",
      "Iteration 339/2000, Loss: 0.00041040382348001003\n",
      "Iteration 340/2000, Loss: 0.00032087854924611747\n",
      "Iteration 341/2000, Loss: 0.00033017375972121954\n",
      "Iteration 342/2000, Loss: 0.00026483996771275997\n",
      "Iteration 343/2000, Loss: 0.00014883391850162297\n",
      "Iteration 344/2000, Loss: 0.0005725278169848025\n",
      "Iteration 345/2000, Loss: 0.00031602467061020434\n",
      "Iteration 346/2000, Loss: 0.0003162802313454449\n",
      "Iteration 347/2000, Loss: 0.0003214075113646686\n",
      "Iteration 348/2000, Loss: 0.0003712401376105845\n",
      "Iteration 349/2000, Loss: 0.0004713274829555303\n",
      "Iteration 350/2000, Loss: 0.00013961957301944494\n",
      "Iteration 351/2000, Loss: 0.0004704841994680464\n",
      "Iteration 352/2000, Loss: 0.00029794033616781235\n",
      "Iteration 353/2000, Loss: 0.000365493877325207\n",
      "Iteration 354/2000, Loss: 0.00035051655140705407\n",
      "Iteration 355/2000, Loss: 0.00038567170850001276\n",
      "Iteration 356/2000, Loss: 0.00031436709105037153\n",
      "Iteration 357/2000, Loss: 0.0006211175350472331\n",
      "Iteration 358/2000, Loss: 0.00021884412853978574\n",
      "Iteration 359/2000, Loss: 0.0002967792097479105\n",
      "Iteration 360/2000, Loss: 0.00020164177112746984\n",
      "Iteration 361/2000, Loss: 0.0002724285004660487\n",
      "Iteration 362/2000, Loss: 0.0002903798595070839\n",
      "Iteration 363/2000, Loss: 0.00020604814926628023\n",
      "Iteration 364/2000, Loss: 0.0006052543176338077\n",
      "Iteration 365/2000, Loss: 0.00021223592921160161\n",
      "Iteration 366/2000, Loss: 0.0003565677034202963\n",
      "Iteration 367/2000, Loss: 0.00011408063437556848\n",
      "Iteration 368/2000, Loss: 0.00037175542092882097\n",
      "Iteration 369/2000, Loss: 0.00031336394022218883\n",
      "Iteration 370/2000, Loss: 0.000282040040474385\n",
      "Iteration 371/2000, Loss: 0.00036963322781957686\n",
      "Iteration 372/2000, Loss: 0.00020481085812207311\n",
      "Iteration 373/2000, Loss: 0.0005449419259093702\n",
      "Iteration 374/2000, Loss: 0.00017679145094007254\n",
      "Iteration 375/2000, Loss: 0.00029043160611763597\n",
      "Iteration 376/2000, Loss: 0.0001641890121391043\n",
      "Iteration 377/2000, Loss: 0.0001844301586970687\n",
      "Iteration 378/2000, Loss: 0.00023991617490537465\n",
      "Iteration 379/2000, Loss: 0.00035294759436510503\n",
      "Iteration 380/2000, Loss: 0.0002336511097382754\n",
      "Iteration 381/2000, Loss: 0.0002758684568107128\n",
      "Iteration 382/2000, Loss: 0.0006230055005289614\n",
      "Iteration 383/2000, Loss: 0.0006013453239575028\n",
      "Iteration 384/2000, Loss: 0.00020601949654519558\n",
      "Iteration 385/2000, Loss: 0.0003772939380723983\n",
      "Iteration 386/2000, Loss: 0.0003116592997685075\n",
      "Iteration 387/2000, Loss: 0.00031109966221265495\n",
      "Iteration 388/2000, Loss: 0.00020081883121747524\n",
      "Iteration 389/2000, Loss: 0.00040062254993245006\n",
      "Iteration 390/2000, Loss: 0.00046654301695525646\n",
      "Iteration 391/2000, Loss: 0.00023182401491794735\n",
      "Iteration 392/2000, Loss: 0.0003791305352933705\n",
      "Iteration 393/2000, Loss: 0.00017321232007816434\n",
      "Iteration 394/2000, Loss: 0.0002701913472265005\n",
      "Iteration 395/2000, Loss: 0.00017800038040149957\n",
      "Iteration 396/2000, Loss: 0.00017940995167009532\n",
      "Iteration 397/2000, Loss: 0.00014362930960487574\n",
      "Iteration 398/2000, Loss: 0.00019741167488973588\n",
      "Iteration 399/2000, Loss: 0.00021579398890025914\n",
      "Iteration 400/2000, Loss: 0.0002346769324503839\n",
      "Iteration 401/2000, Loss: 0.00014545054000336677\n",
      "Iteration 402/2000, Loss: 0.0002140433352906257\n",
      "Iteration 403/2000, Loss: 0.00020427998970262706\n",
      "Iteration 404/2000, Loss: 0.00029682653257623315\n",
      "Iteration 405/2000, Loss: 0.00019986341067124158\n",
      "Iteration 406/2000, Loss: 0.00014412018936127424\n",
      "Iteration 407/2000, Loss: 0.00036196329165250063\n",
      "Iteration 408/2000, Loss: 0.00013543754175771028\n",
      "Iteration 409/2000, Loss: 0.0002989442436955869\n",
      "Iteration 410/2000, Loss: 0.0002198073925683275\n",
      "Iteration 411/2000, Loss: 0.00030362626421265304\n",
      "Iteration 412/2000, Loss: 0.0001650604826863855\n",
      "Iteration 413/2000, Loss: 0.00019253921345807612\n",
      "Iteration 414/2000, Loss: 0.00020375006715767086\n",
      "Iteration 415/2000, Loss: 0.0002283559151692316\n",
      "Iteration 416/2000, Loss: 0.00019711803179234266\n",
      "Iteration 417/2000, Loss: 0.0002488276513759047\n",
      "Iteration 418/2000, Loss: 0.0002725636586546898\n",
      "Iteration 419/2000, Loss: 0.00014158882549963892\n",
      "Iteration 420/2000, Loss: 0.0001656214299146086\n",
      "Iteration 421/2000, Loss: 0.00017384359671268612\n",
      "Iteration 422/2000, Loss: 0.00022212746262084693\n",
      "Iteration 423/2000, Loss: 0.00017615924298297614\n",
      "Iteration 424/2000, Loss: 0.00020875253539998084\n",
      "Iteration 425/2000, Loss: 0.00015301848179660738\n",
      "Iteration 426/2000, Loss: 0.0001975055638467893\n",
      "Iteration 427/2000, Loss: 0.0004061975341755897\n",
      "Iteration 428/2000, Loss: 0.0003181115025654435\n",
      "Iteration 429/2000, Loss: 0.00019806573982350528\n",
      "Iteration 430/2000, Loss: 0.0001522954407846555\n",
      "Iteration 431/2000, Loss: 0.0002859668165910989\n",
      "Iteration 432/2000, Loss: 0.0002701185585465282\n",
      "Iteration 433/2000, Loss: 0.00027632174897007644\n",
      "Iteration 434/2000, Loss: 0.0003409716591704637\n",
      "Iteration 435/2000, Loss: 0.00015841005370020866\n",
      "Iteration 436/2000, Loss: 0.00025701028062030673\n",
      "Iteration 437/2000, Loss: 0.00027444795705378056\n",
      "Iteration 438/2000, Loss: 0.0003622242948040366\n",
      "Iteration 439/2000, Loss: 0.0002818579087033868\n",
      "Iteration 440/2000, Loss: 0.0002577906707301736\n",
      "Iteration 441/2000, Loss: 0.00017052708426490426\n",
      "Iteration 442/2000, Loss: 0.0006638170452788472\n",
      "Iteration 443/2000, Loss: 0.0004910405841656029\n",
      "Iteration 444/2000, Loss: 0.0001283011370105669\n",
      "Iteration 445/2000, Loss: 0.00017670019587967545\n",
      "Iteration 446/2000, Loss: 0.00025119719794020057\n",
      "Iteration 447/2000, Loss: 0.00047368707600980997\n",
      "Iteration 448/2000, Loss: 0.00022732498473487794\n",
      "Iteration 449/2000, Loss: 0.00022110302234068513\n",
      "Iteration 450/2000, Loss: 0.00017173153173644096\n",
      "Iteration 451/2000, Loss: 0.0003125026705674827\n",
      "Iteration 452/2000, Loss: 0.00029595670639537275\n",
      "Iteration 453/2000, Loss: 0.0002280314511153847\n",
      "Iteration 454/2000, Loss: 0.00029789310065098107\n",
      "Iteration 455/2000, Loss: 0.00026521316613070667\n",
      "Iteration 456/2000, Loss: 0.0003004955069627613\n",
      "Iteration 457/2000, Loss: 0.00030892339418642223\n",
      "Iteration 458/2000, Loss: 0.00020300147298257798\n",
      "Iteration 459/2000, Loss: 0.00036301196087151766\n",
      "Iteration 460/2000, Loss: 0.0001734191318973899\n",
      "Iteration 461/2000, Loss: 0.0004546673153527081\n",
      "Iteration 462/2000, Loss: 0.00019515983876772225\n",
      "Iteration 463/2000, Loss: 0.00021486739569809288\n",
      "Iteration 464/2000, Loss: 0.00022760471620131284\n",
      "Iteration 465/2000, Loss: 0.00020975663210265338\n",
      "Iteration 466/2000, Loss: 0.0001621199189685285\n",
      "Iteration 467/2000, Loss: 0.00024804097483865917\n",
      "Iteration 468/2000, Loss: 0.000184889548108913\n",
      "Iteration 469/2000, Loss: 0.00017306504014413804\n",
      "Iteration 470/2000, Loss: 0.00015902587620075792\n",
      "Iteration 471/2000, Loss: 0.00020143129222560674\n",
      "Iteration 472/2000, Loss: 0.0002256810839753598\n",
      "Iteration 473/2000, Loss: 0.00032439955975860357\n",
      "Iteration 474/2000, Loss: 0.0002656358992680907\n",
      "Iteration 475/2000, Loss: 0.0001307242055190727\n",
      "Iteration 476/2000, Loss: 0.00015398576215375215\n",
      "Iteration 477/2000, Loss: 0.00015072678797878325\n",
      "Iteration 478/2000, Loss: 0.0001933799940161407\n",
      "Iteration 479/2000, Loss: 0.00018711794109549373\n",
      "Iteration 480/2000, Loss: 0.00012177568714832887\n",
      "Iteration 481/2000, Loss: 0.00021426432067528367\n",
      "Iteration 482/2000, Loss: 0.00018538378935772926\n",
      "Iteration 483/2000, Loss: 0.00021935072436463088\n",
      "Iteration 484/2000, Loss: 0.0002551716170273721\n",
      "Iteration 485/2000, Loss: 0.00020310272520873696\n",
      "Iteration 486/2000, Loss: 0.00019765117031056434\n",
      "Iteration 487/2000, Loss: 0.00026418562629260123\n",
      "Iteration 488/2000, Loss: 0.00014408829156309366\n",
      "Iteration 489/2000, Loss: 0.0002848351141437888\n",
      "Iteration 490/2000, Loss: 0.00018994361744262278\n",
      "Iteration 491/2000, Loss: 0.00024487607879564166\n",
      "Iteration 492/2000, Loss: 0.00026384746888652444\n",
      "Iteration 493/2000, Loss: 0.00012889837671536952\n",
      "Iteration 494/2000, Loss: 0.00010865041986107826\n",
      "Iteration 495/2000, Loss: 0.00023861035879235715\n",
      "Iteration 496/2000, Loss: 0.00012437539407983422\n",
      "Iteration 497/2000, Loss: 0.00011808555427705869\n",
      "Iteration 498/2000, Loss: 0.00012243182573001832\n",
      "Iteration 499/2000, Loss: 0.000126340237329714\n",
      "Iteration 500/2000, Loss: 0.0002831582969520241\n",
      "Iteration 501/2000, Loss: 0.0001337421708740294\n",
      "Iteration 502/2000, Loss: 0.00017438466602470726\n",
      "Iteration 503/2000, Loss: 0.0001808107044780627\n",
      "Iteration 504/2000, Loss: 0.00011157889093738049\n",
      "Iteration 505/2000, Loss: 0.00022285828890744597\n",
      "Iteration 506/2000, Loss: 0.0001497704506618902\n",
      "Iteration 507/2000, Loss: 0.00015214865561574697\n",
      "Iteration 508/2000, Loss: 0.00012523532495833933\n",
      "Iteration 509/2000, Loss: 0.00018343876581639051\n",
      "Iteration 510/2000, Loss: 0.0002464777498971671\n",
      "Iteration 511/2000, Loss: 0.00018834286311175674\n",
      "Iteration 512/2000, Loss: 0.0002858775551430881\n",
      "Iteration 513/2000, Loss: 0.00031266247970052063\n",
      "Iteration 514/2000, Loss: 0.00015663716476410627\n",
      "Iteration 515/2000, Loss: 0.000107543972262647\n",
      "Iteration 516/2000, Loss: 0.0001534063630970195\n",
      "Iteration 517/2000, Loss: 0.00020882296666968614\n",
      "Iteration 518/2000, Loss: 0.00020680275338236243\n",
      "Iteration 519/2000, Loss: 0.0001581970282131806\n",
      "Iteration 520/2000, Loss: 0.0001804494095267728\n",
      "Iteration 521/2000, Loss: 0.00018390754121355712\n",
      "Iteration 522/2000, Loss: 0.00018684779934119433\n",
      "Iteration 523/2000, Loss: 0.00023843972303438932\n",
      "Iteration 524/2000, Loss: 0.000230924561037682\n",
      "Iteration 525/2000, Loss: 0.00015457799599971622\n",
      "Iteration 526/2000, Loss: 0.00016897066961973906\n",
      "Iteration 527/2000, Loss: 0.00015274134057108313\n",
      "Iteration 528/2000, Loss: 0.00012638824409805238\n",
      "Iteration 529/2000, Loss: 0.0001310129591729492\n",
      "Iteration 530/2000, Loss: 0.0001571234461152926\n",
      "Iteration 531/2000, Loss: 0.0004146591236349195\n",
      "Iteration 532/2000, Loss: 0.0001409332180628553\n",
      "Iteration 533/2000, Loss: 0.0002113923546858132\n",
      "Iteration 534/2000, Loss: 0.00034646986750885844\n",
      "Iteration 535/2000, Loss: 0.0003836549003608525\n",
      "Iteration 536/2000, Loss: 0.00031801563454791903\n",
      "Iteration 537/2000, Loss: 0.00023576128296554089\n",
      "Iteration 538/2000, Loss: 0.00024319562362506986\n",
      "Iteration 539/2000, Loss: 0.0001427261158823967\n",
      "Iteration 540/2000, Loss: 0.00024380040122196078\n",
      "Iteration 541/2000, Loss: 0.00017158575064968318\n",
      "Iteration 542/2000, Loss: 0.00010464303340995684\n",
      "Iteration 543/2000, Loss: 0.00017195723194163293\n",
      "Iteration 544/2000, Loss: 0.0001817022857721895\n",
      "Iteration 545/2000, Loss: 0.00016030856932047755\n",
      "Iteration 546/2000, Loss: 0.00013815595593769103\n",
      "Iteration 547/2000, Loss: 0.00017040518287103623\n",
      "Iteration 548/2000, Loss: 0.00016874755965545774\n",
      "Iteration 549/2000, Loss: 0.00025994243333116174\n",
      "Iteration 550/2000, Loss: 0.0001650041522225365\n",
      "Iteration 551/2000, Loss: 0.0002394883194938302\n",
      "Iteration 552/2000, Loss: 0.00018547687795944512\n",
      "Iteration 553/2000, Loss: 0.0006226766854524612\n",
      "Iteration 554/2000, Loss: 0.00016506886458955705\n",
      "Iteration 555/2000, Loss: 0.00022109599376562983\n",
      "Iteration 556/2000, Loss: 0.00015005106979515404\n",
      "Iteration 557/2000, Loss: 0.0001497189950896427\n",
      "Iteration 558/2000, Loss: 0.0002169992949347943\n",
      "Iteration 559/2000, Loss: 0.00013561839296016842\n",
      "Iteration 560/2000, Loss: 0.0002080756239593029\n",
      "Iteration 561/2000, Loss: 0.00021665776148438454\n",
      "Iteration 562/2000, Loss: 0.0002245028008474037\n",
      "Iteration 563/2000, Loss: 0.00023955749929882586\n",
      "Iteration 564/2000, Loss: 0.00026319545577280223\n",
      "Iteration 565/2000, Loss: 0.0001723842287901789\n",
      "Iteration 566/2000, Loss: 0.00038955139461904764\n",
      "Iteration 567/2000, Loss: 0.00027016716194339097\n",
      "Iteration 568/2000, Loss: 0.00019566564878914505\n",
      "Iteration 569/2000, Loss: 0.00023170308850239962\n",
      "Iteration 570/2000, Loss: 0.000173912791069597\n",
      "Iteration 571/2000, Loss: 0.00024932995438575745\n",
      "Iteration 572/2000, Loss: 0.00010718932026065886\n",
      "Iteration 573/2000, Loss: 0.0004985254490748048\n",
      "Iteration 574/2000, Loss: 0.0001552235335111618\n",
      "Iteration 575/2000, Loss: 0.0002378045319346711\n",
      "Iteration 576/2000, Loss: 0.00022015334980096668\n",
      "Iteration 577/2000, Loss: 0.00013313742238096893\n",
      "Iteration 578/2000, Loss: 0.00032263362663798034\n",
      "Iteration 579/2000, Loss: 0.00016134946781676263\n",
      "Iteration 580/2000, Loss: 0.0002233017876278609\n",
      "Iteration 581/2000, Loss: 0.0003314472269266844\n",
      "Iteration 582/2000, Loss: 0.0002252682752441615\n",
      "Iteration 583/2000, Loss: 0.00016758557467255741\n",
      "Iteration 584/2000, Loss: 0.00019597425125539303\n",
      "Iteration 585/2000, Loss: 0.0001578387018525973\n",
      "Iteration 586/2000, Loss: 0.0002531324571464211\n",
      "Iteration 587/2000, Loss: 0.00017832945741247386\n",
      "Iteration 588/2000, Loss: 0.00016154364857356995\n",
      "Iteration 589/2000, Loss: 0.00032571807969361544\n",
      "Iteration 590/2000, Loss: 0.00047584300045855343\n",
      "Iteration 591/2000, Loss: 0.00021999076125212014\n",
      "Iteration 592/2000, Loss: 0.0003921779862139374\n",
      "Iteration 593/2000, Loss: 0.00026776682352647185\n",
      "Iteration 594/2000, Loss: 0.00035481524537317455\n",
      "Iteration 595/2000, Loss: 0.00012440409045666456\n",
      "Iteration 596/2000, Loss: 0.00030774527112953365\n",
      "Iteration 597/2000, Loss: 0.0003466007183305919\n",
      "Iteration 598/2000, Loss: 0.00015652143338229507\n",
      "Iteration 599/2000, Loss: 0.00035646985634230077\n",
      "Iteration 600/2000, Loss: 0.00018876937974710017\n",
      "Iteration 601/2000, Loss: 0.00030509132193401456\n",
      "Iteration 602/2000, Loss: 0.000129813895910047\n",
      "Iteration 603/2000, Loss: 0.0003297460498288274\n",
      "Iteration 604/2000, Loss: 0.00019136141054332256\n",
      "Iteration 605/2000, Loss: 0.0001907550758915022\n",
      "Iteration 606/2000, Loss: 0.00014250159438233823\n",
      "Iteration 607/2000, Loss: 0.0004851818084716797\n",
      "Iteration 608/2000, Loss: 0.0001643583964323625\n",
      "Iteration 609/2000, Loss: 0.00017706109792925417\n",
      "Iteration 610/2000, Loss: 0.00016133554163388908\n",
      "Iteration 611/2000, Loss: 0.00016742863226681948\n",
      "Iteration 612/2000, Loss: 0.00012518141011241823\n",
      "Iteration 613/2000, Loss: 0.0002620140148792416\n",
      "Iteration 614/2000, Loss: 0.0005514672957360744\n",
      "Iteration 615/2000, Loss: 0.00013785657938569784\n",
      "Iteration 616/2000, Loss: 0.0009335497161373496\n",
      "Iteration 617/2000, Loss: 0.00025210384046658874\n",
      "Iteration 618/2000, Loss: 0.000456985057098791\n",
      "Iteration 619/2000, Loss: 0.0002741404459811747\n",
      "Iteration 620/2000, Loss: 0.00022084185911808163\n",
      "Iteration 621/2000, Loss: 0.0003212957817595452\n",
      "Iteration 622/2000, Loss: 0.00016914804291445762\n",
      "Iteration 623/2000, Loss: 0.00044454206363298\n",
      "Iteration 624/2000, Loss: 0.0002111502835759893\n",
      "Iteration 625/2000, Loss: 0.00039509148336946964\n",
      "Iteration 626/2000, Loss: 0.0005349916173145175\n",
      "Iteration 627/2000, Loss: 0.0003830204368568957\n",
      "Iteration 628/2000, Loss: 0.0003020790754817426\n",
      "Iteration 629/2000, Loss: 0.000397943367715925\n",
      "Iteration 630/2000, Loss: 0.00021895184181630611\n",
      "Iteration 631/2000, Loss: 0.00033096331753768027\n",
      "Iteration 632/2000, Loss: 0.0002478369860909879\n",
      "Iteration 633/2000, Loss: 0.000638841069303453\n",
      "Iteration 634/2000, Loss: 0.0003664470568764955\n",
      "Iteration 635/2000, Loss: 0.0002732232678681612\n",
      "Iteration 636/2000, Loss: 0.0004433349531609565\n",
      "Iteration 637/2000, Loss: 0.0004566566494759172\n",
      "Iteration 638/2000, Loss: 0.0003693675098475069\n",
      "Iteration 639/2000, Loss: 0.00038233326631598175\n",
      "Iteration 640/2000, Loss: 0.00032699733856134117\n",
      "Iteration 641/2000, Loss: 0.00033517094561830163\n",
      "Iteration 642/2000, Loss: 0.0008342201472260058\n",
      "Iteration 643/2000, Loss: 0.00022394275583792478\n",
      "Iteration 644/2000, Loss: 0.0004832290578633547\n",
      "Iteration 645/2000, Loss: 0.0002996200928464532\n",
      "Iteration 646/2000, Loss: 0.00020308060629758984\n",
      "Iteration 647/2000, Loss: 0.0003642011433839798\n",
      "Iteration 648/2000, Loss: 0.00012992149277124554\n",
      "Iteration 649/2000, Loss: 0.00030339136719703674\n",
      "Iteration 650/2000, Loss: 0.00024162163026630878\n",
      "Iteration 651/2000, Loss: 0.00021003332221880555\n",
      "Iteration 652/2000, Loss: 0.00041594248614273965\n",
      "Iteration 653/2000, Loss: 0.00011707364319590852\n",
      "Iteration 654/2000, Loss: 0.00023529429745394737\n",
      "Iteration 655/2000, Loss: 0.00013028504326939583\n",
      "Iteration 656/2000, Loss: 0.00024646741803735495\n",
      "Iteration 657/2000, Loss: 0.00015023293963167816\n",
      "Iteration 658/2000, Loss: 0.00019327829068060964\n",
      "Iteration 659/2000, Loss: 0.0004963970859535038\n",
      "Iteration 660/2000, Loss: 0.0001918416382977739\n",
      "Iteration 661/2000, Loss: 0.00019090529531240463\n",
      "Iteration 662/2000, Loss: 0.00017556469538249075\n",
      "Iteration 663/2000, Loss: 0.0002446036087349057\n",
      "Iteration 664/2000, Loss: 0.00024331054009962827\n",
      "Iteration 665/2000, Loss: 0.0002622503670863807\n",
      "Iteration 666/2000, Loss: 0.0001767641952028498\n",
      "Iteration 667/2000, Loss: 0.0003850993816740811\n",
      "Iteration 668/2000, Loss: 0.0002628322981763631\n",
      "Iteration 669/2000, Loss: 0.0002396446216152981\n",
      "Iteration 670/2000, Loss: 0.00012966527719981968\n",
      "Iteration 671/2000, Loss: 0.00020193264936096966\n",
      "Iteration 672/2000, Loss: 0.0003714933118317276\n",
      "Iteration 673/2000, Loss: 0.00025359963183291256\n",
      "Iteration 674/2000, Loss: 0.00027756724739447236\n",
      "Iteration 675/2000, Loss: 0.0002719442709349096\n",
      "Iteration 676/2000, Loss: 0.0002834003826137632\n",
      "Iteration 677/2000, Loss: 0.00033184641506522894\n",
      "Iteration 678/2000, Loss: 0.00020238058641552925\n",
      "Iteration 679/2000, Loss: 0.0002926472225226462\n",
      "Iteration 680/2000, Loss: 0.00022326436010189354\n",
      "Iteration 681/2000, Loss: 0.0003433444653637707\n",
      "Iteration 682/2000, Loss: 0.00016296684043481946\n",
      "Iteration 683/2000, Loss: 0.0003498623555060476\n",
      "Iteration 684/2000, Loss: 0.0003786032903008163\n",
      "Iteration 685/2000, Loss: 0.00018537872529122978\n",
      "Iteration 686/2000, Loss: 0.00024049323110375553\n",
      "Iteration 687/2000, Loss: 0.00011688773520290852\n",
      "Iteration 688/2000, Loss: 0.00023219553986564279\n",
      "Iteration 689/2000, Loss: 0.00023639778373762965\n",
      "Iteration 690/2000, Loss: 0.00029813675791956484\n",
      "Iteration 691/2000, Loss: 0.00020492472685873508\n",
      "Iteration 692/2000, Loss: 0.0005665186326950788\n",
      "Iteration 693/2000, Loss: 0.00012923691247124225\n",
      "Iteration 694/2000, Loss: 0.00018190844275522977\n",
      "Iteration 695/2000, Loss: 0.00022505312517751008\n",
      "Iteration 696/2000, Loss: 0.00018285310943610966\n",
      "Iteration 697/2000, Loss: 0.0003042863099835813\n",
      "Iteration 698/2000, Loss: 0.00024241891514975578\n",
      "Iteration 699/2000, Loss: 0.0002567534102126956\n",
      "Iteration 700/2000, Loss: 0.00014449970331043005\n",
      "Iteration 701/2000, Loss: 0.00020707845396827906\n",
      "Iteration 702/2000, Loss: 0.00030925546889193356\n",
      "Iteration 703/2000, Loss: 0.00016256207891274244\n",
      "Iteration 704/2000, Loss: 0.00014355796156451106\n",
      "Iteration 705/2000, Loss: 0.0001655353989917785\n",
      "Iteration 706/2000, Loss: 0.00019574176985770464\n",
      "Iteration 707/2000, Loss: 0.0003564780345186591\n",
      "Iteration 708/2000, Loss: 0.00031405771733261645\n",
      "Iteration 709/2000, Loss: 0.00012822094140574336\n",
      "Iteration 710/2000, Loss: 0.00021234650921542197\n",
      "Iteration 711/2000, Loss: 0.00014863544492982328\n",
      "Iteration 712/2000, Loss: 0.0003155433514621109\n",
      "Iteration 713/2000, Loss: 0.00016139939543791115\n",
      "Iteration 714/2000, Loss: 0.0002938481920864433\n",
      "Iteration 715/2000, Loss: 0.00016506543033756316\n",
      "Iteration 716/2000, Loss: 0.0002123813610523939\n",
      "Iteration 717/2000, Loss: 0.00019278647960163653\n",
      "Iteration 718/2000, Loss: 0.00027349338051863015\n",
      "Iteration 719/2000, Loss: 0.0003729260934051126\n",
      "Iteration 720/2000, Loss: 0.00036105571780353785\n",
      "Iteration 721/2000, Loss: 0.00042128897621296346\n",
      "Iteration 722/2000, Loss: 0.00017796804604586214\n",
      "Iteration 723/2000, Loss: 0.0004395125724840909\n",
      "Iteration 724/2000, Loss: 0.0002917317906394601\n",
      "Iteration 725/2000, Loss: 0.00030153311672620475\n",
      "Iteration 726/2000, Loss: 0.00021378383098635823\n",
      "Iteration 727/2000, Loss: 0.0002675657160580158\n",
      "Iteration 728/2000, Loss: 0.00032792254933156073\n",
      "Iteration 729/2000, Loss: 0.0001616717199794948\n",
      "Iteration 730/2000, Loss: 0.00040562625508755445\n",
      "Iteration 731/2000, Loss: 0.00010508761624805629\n",
      "Iteration 732/2000, Loss: 0.0003427733026910573\n",
      "Iteration 733/2000, Loss: 0.0002536110987421125\n",
      "Iteration 734/2000, Loss: 0.00036175583954900503\n",
      "Iteration 735/2000, Loss: 0.00023617940314579755\n",
      "Iteration 736/2000, Loss: 0.00014688240480609238\n",
      "Iteration 737/2000, Loss: 0.0002484402502886951\n",
      "Iteration 738/2000, Loss: 0.00025841520982794464\n",
      "Iteration 739/2000, Loss: 0.0003459678846411407\n",
      "Iteration 740/2000, Loss: 0.00015237052866723388\n",
      "Iteration 741/2000, Loss: 0.00022841519967187196\n",
      "Iteration 742/2000, Loss: 0.00011340141645632684\n",
      "Iteration 743/2000, Loss: 0.0002742284850683063\n",
      "Iteration 744/2000, Loss: 0.0003473212127573788\n",
      "Iteration 745/2000, Loss: 0.00016583807882852852\n",
      "Iteration 746/2000, Loss: 0.0002126259496435523\n",
      "Iteration 747/2000, Loss: 0.00021453251247294247\n",
      "Iteration 748/2000, Loss: 0.00016649888129904866\n",
      "Iteration 749/2000, Loss: 0.00016597224748693407\n",
      "Iteration 750/2000, Loss: 0.00013482158828992397\n",
      "Iteration 751/2000, Loss: 0.00018309231381863356\n",
      "Iteration 752/2000, Loss: 0.0002269731485284865\n",
      "Iteration 753/2000, Loss: 0.00023213271924760193\n",
      "Iteration 754/2000, Loss: 0.00013267758185975254\n",
      "Iteration 755/2000, Loss: 0.00023562168644275516\n",
      "Iteration 756/2000, Loss: 0.00011535646626725793\n",
      "Iteration 757/2000, Loss: 0.0001831546105677262\n",
      "Iteration 758/2000, Loss: 0.00028786048642359674\n",
      "Iteration 759/2000, Loss: 0.00022870740212965757\n",
      "Iteration 760/2000, Loss: 0.0002238254528492689\n",
      "Iteration 761/2000, Loss: 0.0003546532243490219\n",
      "Iteration 762/2000, Loss: 0.00022108225675765425\n",
      "Iteration 763/2000, Loss: 0.0002265029470436275\n",
      "Iteration 764/2000, Loss: 0.00031815317925065756\n",
      "Iteration 765/2000, Loss: 0.00047410459956154227\n",
      "Iteration 766/2000, Loss: 0.0004317647544667125\n",
      "Iteration 767/2000, Loss: 0.00019530433928593993\n",
      "Iteration 768/2000, Loss: 0.00028015355928801\n",
      "Iteration 769/2000, Loss: 0.0002164410543628037\n",
      "Iteration 770/2000, Loss: 0.00019409904780332\n",
      "Iteration 771/2000, Loss: 0.00015632556460332125\n",
      "Iteration 772/2000, Loss: 0.00019792273815255612\n",
      "Iteration 773/2000, Loss: 0.00016380177112296224\n",
      "Iteration 774/2000, Loss: 0.0002100289857480675\n",
      "Iteration 775/2000, Loss: 0.00015763446572236717\n",
      "Iteration 776/2000, Loss: 0.00021166371880099177\n",
      "Iteration 777/2000, Loss: 0.00033180619357153773\n",
      "Iteration 778/2000, Loss: 9.646419493947178e-05\n",
      "Iteration 779/2000, Loss: 0.0001700636639725417\n",
      "Iteration 780/2000, Loss: 0.00018977088620886207\n",
      "Iteration 781/2000, Loss: 0.00017233887047041208\n",
      "Iteration 782/2000, Loss: 0.00028188375290483236\n",
      "Iteration 783/2000, Loss: 0.0003663661773316562\n",
      "Iteration 784/2000, Loss: 0.0002835637715179473\n",
      "Iteration 785/2000, Loss: 0.00019698093819897622\n",
      "Iteration 786/2000, Loss: 0.0001481750950915739\n",
      "Iteration 787/2000, Loss: 0.00010332429519621655\n",
      "Iteration 788/2000, Loss: 0.00024859493714757264\n",
      "Iteration 789/2000, Loss: 0.00016513888840563595\n",
      "Iteration 790/2000, Loss: 0.0001947692799149081\n",
      "Iteration 791/2000, Loss: 0.0002452853659633547\n",
      "Iteration 792/2000, Loss: 0.00021826762531418353\n",
      "Iteration 793/2000, Loss: 0.00011836896010208875\n",
      "Iteration 794/2000, Loss: 0.00012030620564473793\n",
      "Iteration 795/2000, Loss: 0.0002452245098538697\n",
      "Iteration 796/2000, Loss: 0.00015545428323093802\n",
      "Iteration 797/2000, Loss: 0.00016118033090606332\n",
      "Iteration 798/2000, Loss: 0.00010630588803905994\n",
      "Iteration 799/2000, Loss: 0.00035628562909550965\n",
      "Iteration 800/2000, Loss: 0.00018091476522386074\n",
      "Iteration 801/2000, Loss: 0.000218287663301453\n",
      "Iteration 802/2000, Loss: 0.00016125703405123204\n",
      "Iteration 803/2000, Loss: 0.0001248682092409581\n",
      "Iteration 804/2000, Loss: 0.00014672485121991485\n",
      "Iteration 805/2000, Loss: 0.00024873961228877306\n",
      "Iteration 806/2000, Loss: 0.00020221847807988524\n",
      "Iteration 807/2000, Loss: 0.00019581429660320282\n",
      "Iteration 808/2000, Loss: 0.0001338038855465129\n",
      "Iteration 809/2000, Loss: 0.00022826032363809645\n",
      "Iteration 810/2000, Loss: 0.0002688882523216307\n",
      "Iteration 811/2000, Loss: 0.00022625501151196659\n",
      "Iteration 812/2000, Loss: 0.0001956174528459087\n",
      "Iteration 813/2000, Loss: 0.000220034911762923\n",
      "Iteration 814/2000, Loss: 0.00022717607498634607\n",
      "Iteration 815/2000, Loss: 0.00025994962197728455\n",
      "Iteration 816/2000, Loss: 0.000201955801458098\n",
      "Iteration 817/2000, Loss: 0.00013029217370785773\n",
      "Iteration 818/2000, Loss: 0.0003007428895216435\n",
      "Iteration 819/2000, Loss: 0.00023066520225256681\n",
      "Iteration 820/2000, Loss: 0.0002611966338008642\n",
      "Iteration 821/2000, Loss: 0.00024454062804579735\n",
      "Iteration 822/2000, Loss: 0.00021418080723378807\n",
      "Iteration 823/2000, Loss: 0.0001637972891330719\n",
      "Iteration 824/2000, Loss: 0.00012310741294641048\n",
      "Iteration 825/2000, Loss: 0.00024286337429657578\n",
      "Iteration 826/2000, Loss: 0.00015080923913046718\n",
      "Iteration 827/2000, Loss: 0.00017306343943346292\n",
      "Iteration 828/2000, Loss: 0.00012667005648836493\n",
      "Iteration 829/2000, Loss: 0.00032854260643944144\n",
      "Iteration 830/2000, Loss: 0.00023891976161394268\n",
      "Iteration 831/2000, Loss: 0.00042172035318799317\n",
      "Iteration 832/2000, Loss: 0.00027918993146158755\n",
      "Iteration 833/2000, Loss: 0.00026276952121406794\n",
      "Iteration 834/2000, Loss: 0.00010308925266144797\n",
      "Iteration 835/2000, Loss: 0.00021227376419119537\n",
      "Iteration 836/2000, Loss: 0.00018181609630119056\n",
      "Iteration 837/2000, Loss: 0.00014280159666668624\n",
      "Iteration 838/2000, Loss: 0.00027783680707216263\n",
      "Iteration 839/2000, Loss: 0.0002249305252917111\n",
      "Iteration 840/2000, Loss: 0.00010078534978674725\n",
      "Iteration 841/2000, Loss: 0.0001458493061363697\n",
      "Iteration 842/2000, Loss: 0.00014617311535403132\n",
      "Iteration 843/2000, Loss: 0.00011241409811191261\n",
      "Iteration 844/2000, Loss: 0.0001204781437991187\n",
      "Iteration 845/2000, Loss: 0.00029267434729263186\n",
      "Iteration 846/2000, Loss: 0.0005160824512131512\n",
      "Iteration 847/2000, Loss: 0.0001504968386143446\n",
      "Iteration 848/2000, Loss: 0.00010911041317740455\n",
      "Iteration 849/2000, Loss: 0.0001802532497094944\n",
      "Iteration 850/2000, Loss: 0.00013785090413875878\n",
      "Iteration 851/2000, Loss: 0.00017885056149680167\n",
      "Iteration 852/2000, Loss: 0.00014531331544276327\n",
      "Iteration 853/2000, Loss: 0.00018144026398658752\n",
      "Iteration 854/2000, Loss: 0.00031026630313135684\n",
      "Iteration 855/2000, Loss: 0.00015473975508939475\n",
      "Iteration 856/2000, Loss: 0.0002557054103817791\n",
      "Iteration 857/2000, Loss: 0.0001551228779135272\n",
      "Iteration 858/2000, Loss: 0.0002157172275474295\n",
      "Iteration 859/2000, Loss: 0.00020321692863944918\n",
      "Iteration 860/2000, Loss: 0.00013563234824687243\n",
      "Iteration 861/2000, Loss: 0.0001362441835226491\n",
      "Iteration 862/2000, Loss: 0.00012304949632380158\n",
      "Iteration 863/2000, Loss: 0.00014739818288944662\n",
      "Iteration 864/2000, Loss: 0.00011413519678171724\n",
      "Iteration 865/2000, Loss: 0.00023859018983785063\n",
      "Iteration 866/2000, Loss: 0.000239624161622487\n",
      "Iteration 867/2000, Loss: 0.00012952357064932585\n",
      "Iteration 868/2000, Loss: 0.00027933213277719915\n",
      "Iteration 869/2000, Loss: 0.00015043838357087225\n",
      "Iteration 870/2000, Loss: 0.0002461649419274181\n",
      "Iteration 871/2000, Loss: 0.00016491793212480843\n",
      "Iteration 872/2000, Loss: 0.00019283239089418203\n",
      "Iteration 873/2000, Loss: 0.00019331088697072119\n",
      "Iteration 874/2000, Loss: 0.00019828960648737848\n",
      "Iteration 875/2000, Loss: 0.00018280117365065962\n",
      "Iteration 876/2000, Loss: 0.00016678206156939268\n",
      "Iteration 877/2000, Loss: 0.00020087392476852983\n",
      "Iteration 878/2000, Loss: 0.0001736219710437581\n",
      "Iteration 879/2000, Loss: 0.00022267857275437564\n",
      "Iteration 880/2000, Loss: 0.00042274713632650673\n",
      "Iteration 881/2000, Loss: 0.00016545258404221386\n",
      "Iteration 882/2000, Loss: 0.00024567078799009323\n",
      "Iteration 883/2000, Loss: 0.00022738261031918228\n",
      "Iteration 884/2000, Loss: 0.000372806447558105\n",
      "Iteration 885/2000, Loss: 0.00030523145687766373\n",
      "Iteration 886/2000, Loss: 0.0005537599790841341\n",
      "Iteration 887/2000, Loss: 0.0001505991240264848\n",
      "Iteration 888/2000, Loss: 0.00020938119268976152\n",
      "Iteration 889/2000, Loss: 0.00018614002328831702\n",
      "Iteration 890/2000, Loss: 0.00011117894609924406\n",
      "Iteration 891/2000, Loss: 0.0002172405511373654\n",
      "Iteration 892/2000, Loss: 0.00012379331747069955\n",
      "Iteration 893/2000, Loss: 0.000271468743449077\n",
      "Iteration 894/2000, Loss: 0.0001750357187120244\n",
      "Iteration 895/2000, Loss: 0.00010410280083306134\n",
      "Iteration 896/2000, Loss: 0.00018815805378835648\n",
      "Iteration 897/2000, Loss: 0.00019006742513738573\n",
      "Iteration 898/2000, Loss: 0.0003307588631287217\n",
      "Iteration 899/2000, Loss: 0.0001886219542939216\n",
      "Iteration 900/2000, Loss: 0.0003243235987611115\n",
      "Iteration 901/2000, Loss: 0.0002704356738831848\n",
      "Iteration 902/2000, Loss: 0.00047969745355658233\n",
      "Iteration 903/2000, Loss: 0.0002481967385392636\n",
      "Iteration 904/2000, Loss: 0.00015795794024597853\n",
      "Iteration 905/2000, Loss: 0.00027644142392091453\n",
      "Iteration 906/2000, Loss: 0.00017501875117886811\n",
      "Iteration 907/2000, Loss: 0.0016009521204978228\n",
      "Iteration 908/2000, Loss: 0.00017553538782522082\n",
      "Iteration 909/2000, Loss: 0.00030657544266432524\n",
      "Iteration 910/2000, Loss: 0.00024546595523133874\n",
      "Iteration 911/2000, Loss: 0.00018057208217214793\n",
      "Iteration 912/2000, Loss: 0.0002817847125697881\n",
      "Iteration 913/2000, Loss: 0.00024055285030044615\n",
      "Iteration 914/2000, Loss: 0.00029177500982768834\n",
      "Iteration 915/2000, Loss: 0.0002984102175105363\n",
      "Iteration 916/2000, Loss: 0.00020172543008811772\n",
      "Iteration 917/2000, Loss: 0.0002756411558948457\n",
      "Iteration 918/2000, Loss: 0.00025177138741128147\n",
      "Iteration 919/2000, Loss: 0.00013332744129002094\n",
      "Iteration 920/2000, Loss: 0.00015501692541874945\n",
      "Iteration 921/2000, Loss: 0.00021977165306452662\n",
      "Iteration 922/2000, Loss: 0.0001877352624433115\n",
      "Iteration 923/2000, Loss: 0.00012873427476733923\n",
      "Iteration 924/2000, Loss: 0.0001342870236840099\n",
      "Iteration 925/2000, Loss: 0.00020874447363894433\n",
      "Iteration 926/2000, Loss: 0.00033606289071030915\n",
      "Iteration 927/2000, Loss: 0.00016369066725019366\n",
      "Iteration 928/2000, Loss: 0.00032939857919700444\n",
      "Iteration 929/2000, Loss: 0.00018444335728418082\n",
      "Iteration 930/2000, Loss: 0.00018133728008251637\n",
      "Iteration 931/2000, Loss: 0.0001512242597527802\n",
      "Iteration 932/2000, Loss: 0.00017699146701488644\n",
      "Iteration 933/2000, Loss: 0.00022291968343779445\n",
      "Iteration 934/2000, Loss: 0.00015019821876194328\n",
      "Iteration 935/2000, Loss: 0.00024310388835147023\n",
      "Iteration 936/2000, Loss: 0.00016271333151962608\n",
      "Iteration 937/2000, Loss: 0.00017312441195826977\n",
      "Iteration 938/2000, Loss: 0.00015618299948982894\n",
      "Iteration 939/2000, Loss: 0.00018943082250189036\n",
      "Iteration 940/2000, Loss: 0.00014256808208301663\n",
      "Iteration 941/2000, Loss: 0.000113924645120278\n",
      "Iteration 942/2000, Loss: 0.00024183177447412163\n",
      "Iteration 943/2000, Loss: 0.00021939714497420937\n",
      "Iteration 944/2000, Loss: 0.00019519725174177438\n",
      "Iteration 945/2000, Loss: 0.00019940781930927187\n",
      "Iteration 946/2000, Loss: 0.0002061595005216077\n",
      "Iteration 947/2000, Loss: 0.0001975828781723976\n",
      "Iteration 948/2000, Loss: 0.0002011820033658296\n",
      "Iteration 949/2000, Loss: 0.00019337277626618743\n",
      "Iteration 950/2000, Loss: 0.00023621950822416693\n",
      "Iteration 951/2000, Loss: 0.00044654120574705303\n",
      "Iteration 952/2000, Loss: 8.913473720895126e-05\n",
      "Iteration 953/2000, Loss: 0.00023063285334501415\n",
      "Iteration 954/2000, Loss: 0.00014686906069982797\n",
      "Iteration 955/2000, Loss: 0.00024014613882172853\n",
      "Iteration 956/2000, Loss: 0.0001717398117762059\n",
      "Iteration 957/2000, Loss: 0.0002625737979542464\n",
      "Iteration 958/2000, Loss: 0.0001023329168674536\n",
      "Iteration 959/2000, Loss: 0.0002071812195936218\n",
      "Iteration 960/2000, Loss: 0.00015979762247297913\n",
      "Iteration 961/2000, Loss: 0.0002418774674879387\n",
      "Iteration 962/2000, Loss: 0.00016566296108067036\n",
      "Iteration 963/2000, Loss: 0.0003076987632084638\n",
      "Iteration 964/2000, Loss: 0.0003105207288172096\n",
      "Iteration 965/2000, Loss: 0.0002057053497992456\n",
      "Iteration 966/2000, Loss: 0.000183370168088004\n",
      "Iteration 967/2000, Loss: 9.11600436666049e-05\n",
      "Iteration 968/2000, Loss: 0.00017429843137506396\n",
      "Iteration 969/2000, Loss: 0.0004014464793726802\n",
      "Iteration 970/2000, Loss: 0.0001562741381349042\n",
      "Iteration 971/2000, Loss: 0.00011917943629669026\n",
      "Iteration 972/2000, Loss: 0.00014624622417613864\n",
      "Iteration 973/2000, Loss: 0.00010088170529343188\n",
      "Iteration 974/2000, Loss: 0.00011939377873204648\n",
      "Iteration 975/2000, Loss: 0.00014994519005995244\n",
      "Iteration 976/2000, Loss: 0.00021806165750604123\n",
      "Iteration 977/2000, Loss: 0.0002840602246578783\n",
      "Iteration 978/2000, Loss: 0.00020755465084221214\n",
      "Iteration 979/2000, Loss: 0.0002754188608378172\n",
      "Iteration 980/2000, Loss: 0.00021827960154041648\n",
      "Iteration 981/2000, Loss: 0.00016244921425823122\n",
      "Iteration 982/2000, Loss: 0.00016505579696968198\n",
      "Iteration 983/2000, Loss: 0.00016308324120473117\n",
      "Iteration 984/2000, Loss: 0.00019349872309248894\n",
      "Iteration 985/2000, Loss: 0.00020115259394515306\n",
      "Iteration 986/2000, Loss: 0.00029035157058387995\n",
      "Iteration 987/2000, Loss: 0.0002751463034655899\n",
      "Iteration 988/2000, Loss: 0.000304442219203338\n",
      "Iteration 989/2000, Loss: 0.00016310591308865696\n",
      "Iteration 990/2000, Loss: 0.000294391589704901\n",
      "Iteration 991/2000, Loss: 0.00043672238825820386\n",
      "Iteration 992/2000, Loss: 0.000162532611284405\n",
      "Iteration 993/2000, Loss: 0.00040156961767934263\n",
      "Iteration 994/2000, Loss: 0.00021805641881655902\n",
      "Iteration 995/2000, Loss: 0.00020934143685735762\n",
      "Iteration 996/2000, Loss: 0.00022932386491447687\n",
      "Iteration 997/2000, Loss: 0.00020712940022349358\n",
      "Iteration 998/2000, Loss: 0.00022622168762609363\n",
      "Iteration 999/2000, Loss: 0.00015925531624816358\n",
      "Iteration 1000/2000, Loss: 0.0003081423637922853\n",
      "Iteration 1001/2000, Loss: 0.00027032470097765326\n",
      "Iteration 1002/2000, Loss: 0.00022037916642148048\n",
      "Iteration 1003/2000, Loss: 0.00021625518274959177\n",
      "Iteration 1004/2000, Loss: 0.00019311998039484024\n",
      "Iteration 1005/2000, Loss: 0.000258559943176806\n",
      "Iteration 1006/2000, Loss: 0.0003458899154793471\n",
      "Iteration 1007/2000, Loss: 0.00034035005955956876\n",
      "Iteration 1008/2000, Loss: 0.00021976976131554693\n",
      "Iteration 1009/2000, Loss: 0.0004472812288440764\n",
      "Iteration 1010/2000, Loss: 0.00029783989884890616\n",
      "Iteration 1011/2000, Loss: 0.0002566299808677286\n",
      "Iteration 1012/2000, Loss: 0.00016437680460512638\n",
      "Iteration 1013/2000, Loss: 0.0003124667564406991\n",
      "Iteration 1014/2000, Loss: 0.00023420681827701628\n",
      "Iteration 1015/2000, Loss: 0.0001888822444016114\n",
      "Iteration 1016/2000, Loss: 0.00018192888819612563\n",
      "Iteration 1017/2000, Loss: 0.00017772532009985298\n",
      "Iteration 1018/2000, Loss: 0.0002396561612840742\n",
      "Iteration 1019/2000, Loss: 0.00012359606625977904\n",
      "Iteration 1020/2000, Loss: 0.00027073753881268203\n",
      "Iteration 1021/2000, Loss: 0.0002927735331468284\n",
      "Iteration 1022/2000, Loss: 0.00025728021864779294\n",
      "Iteration 1023/2000, Loss: 0.00037004484329372644\n",
      "Iteration 1024/2000, Loss: 0.00037425771006383\n",
      "Iteration 1025/2000, Loss: 0.0003077839792240411\n",
      "Iteration 1026/2000, Loss: 0.0002083030267385766\n",
      "Iteration 1027/2000, Loss: 0.00016018700262065977\n",
      "Iteration 1028/2000, Loss: 0.00028798868879675865\n",
      "Iteration 1029/2000, Loss: 0.0002806949196383357\n",
      "Iteration 1030/2000, Loss: 0.00019382298341952264\n",
      "Iteration 1031/2000, Loss: 0.00027877773391082883\n",
      "Iteration 1032/2000, Loss: 0.0001690469216555357\n",
      "Iteration 1033/2000, Loss: 0.00038631775532849133\n",
      "Iteration 1034/2000, Loss: 0.00021270575234666467\n",
      "Iteration 1035/2000, Loss: 0.0001675934618106112\n",
      "Iteration 1036/2000, Loss: 0.00020820011559408158\n",
      "Iteration 1037/2000, Loss: 0.000252579222433269\n",
      "Iteration 1038/2000, Loss: 0.0005296109011396766\n",
      "Iteration 1039/2000, Loss: 0.00020381092326715589\n",
      "Iteration 1040/2000, Loss: 0.00018323483527638018\n",
      "Iteration 1041/2000, Loss: 0.00025021337205544114\n",
      "Iteration 1042/2000, Loss: 0.00019742162839975208\n",
      "Iteration 1043/2000, Loss: 0.0001489367277827114\n",
      "Iteration 1044/2000, Loss: 0.00015956674178596586\n",
      "Iteration 1045/2000, Loss: 0.00032219348940998316\n",
      "Iteration 1046/2000, Loss: 0.00017139535339083523\n",
      "Iteration 1047/2000, Loss: 0.00024034471425693482\n",
      "Iteration 1048/2000, Loss: 0.00020478240912780166\n",
      "Iteration 1049/2000, Loss: 0.00019227755547035486\n",
      "Iteration 1050/2000, Loss: 0.0002656993456184864\n",
      "Iteration 1051/2000, Loss: 0.0002541347639635205\n",
      "Iteration 1052/2000, Loss: 0.0001939965004567057\n",
      "Iteration 1053/2000, Loss: 0.00021899784042034298\n",
      "Iteration 1054/2000, Loss: 0.00018031020590569824\n",
      "Iteration 1055/2000, Loss: 0.0001987776777241379\n",
      "Iteration 1056/2000, Loss: 0.00033246283419430256\n",
      "Iteration 1057/2000, Loss: 0.00013309869973454624\n",
      "Iteration 1058/2000, Loss: 0.00018499995348975062\n",
      "Iteration 1059/2000, Loss: 0.0005039690295234323\n",
      "Iteration 1060/2000, Loss: 0.00028393068350851536\n",
      "Iteration 1061/2000, Loss: 0.0001292876695515588\n",
      "Iteration 1062/2000, Loss: 0.00020459666848182678\n",
      "Iteration 1063/2000, Loss: 0.00036228777025826275\n",
      "Iteration 1064/2000, Loss: 0.00024894889793358743\n",
      "Iteration 1065/2000, Loss: 0.00026310209068469703\n",
      "Iteration 1066/2000, Loss: 0.00031353876693174243\n",
      "Iteration 1067/2000, Loss: 0.00024445075541734695\n",
      "Iteration 1068/2000, Loss: 0.00044494427856989205\n",
      "Iteration 1069/2000, Loss: 0.000186248347745277\n",
      "Iteration 1070/2000, Loss: 0.0003234242903999984\n",
      "Iteration 1071/2000, Loss: 0.00020968705939594656\n",
      "Iteration 1072/2000, Loss: 0.00034633668838068843\n",
      "Iteration 1073/2000, Loss: 0.0002099328557960689\n",
      "Iteration 1074/2000, Loss: 0.00030653004068881273\n",
      "Iteration 1075/2000, Loss: 0.00034266419243067503\n",
      "Iteration 1076/2000, Loss: 0.00027840567054226995\n",
      "Iteration 1077/2000, Loss: 0.00019571369921322912\n",
      "Iteration 1078/2000, Loss: 0.00011710669787134975\n",
      "Iteration 1079/2000, Loss: 0.00020957589731551707\n",
      "Iteration 1080/2000, Loss: 0.0005587592604570091\n",
      "Iteration 1081/2000, Loss: 0.00020971674530301243\n",
      "Iteration 1082/2000, Loss: 0.00017594080418348312\n",
      "Iteration 1083/2000, Loss: 0.0003123069182038307\n",
      "Iteration 1084/2000, Loss: 0.00025125156389549375\n",
      "Iteration 1085/2000, Loss: 0.00018175787408836186\n",
      "Iteration 1086/2000, Loss: 0.00040465465281158686\n",
      "Iteration 1087/2000, Loss: 0.00024586202926002443\n",
      "Iteration 1088/2000, Loss: 0.00014851804007776082\n",
      "Iteration 1089/2000, Loss: 0.00017526221927255392\n",
      "Iteration 1090/2000, Loss: 0.00026225598412565887\n",
      "Iteration 1091/2000, Loss: 0.00033307276316918433\n",
      "Iteration 1092/2000, Loss: 0.00028088880935683846\n",
      "Iteration 1093/2000, Loss: 0.0003070936072617769\n",
      "Iteration 1094/2000, Loss: 0.00018581276526674628\n",
      "Iteration 1095/2000, Loss: 0.00029825521050952375\n",
      "Iteration 1096/2000, Loss: 0.0001740079460432753\n",
      "Iteration 1097/2000, Loss: 0.0002726243110373616\n",
      "Iteration 1098/2000, Loss: 0.00023803036310710013\n",
      "Iteration 1099/2000, Loss: 0.00018805745639838278\n",
      "Iteration 1100/2000, Loss: 0.00018505188927520066\n",
      "Iteration 1101/2000, Loss: 0.0002442667610011995\n",
      "Iteration 1102/2000, Loss: 0.00018946394266095012\n",
      "Iteration 1103/2000, Loss: 8.520462142769247e-05\n",
      "Iteration 1104/2000, Loss: 0.00017374660819768906\n",
      "Iteration 1105/2000, Loss: 0.00040485343197360635\n",
      "Iteration 1106/2000, Loss: 0.00020118865359108895\n",
      "Iteration 1107/2000, Loss: 0.00021715566981583834\n",
      "Iteration 1108/2000, Loss: 0.00022497000463772565\n",
      "Iteration 1109/2000, Loss: 0.0002656740543898195\n",
      "Iteration 1110/2000, Loss: 0.0002141459408449009\n",
      "Iteration 1111/2000, Loss: 0.00022568966960534453\n",
      "Iteration 1112/2000, Loss: 0.00013584901171270758\n",
      "Iteration 1113/2000, Loss: 0.00012887813500128686\n",
      "Iteration 1114/2000, Loss: 0.000991345732472837\n",
      "Iteration 1115/2000, Loss: 0.00015697019989602268\n",
      "Iteration 1116/2000, Loss: 0.00016271727508865297\n",
      "Iteration 1117/2000, Loss: 0.00016831081302370876\n",
      "Iteration 1118/2000, Loss: 0.00015651331341359764\n",
      "Iteration 1119/2000, Loss: 0.0001904486707644537\n",
      "Iteration 1120/2000, Loss: 0.00016895774751901627\n",
      "Iteration 1121/2000, Loss: 0.0002409859444014728\n",
      "Iteration 1122/2000, Loss: 0.00034892186522483826\n",
      "Iteration 1123/2000, Loss: 0.00021851916972082108\n",
      "Iteration 1124/2000, Loss: 0.00023573053476866335\n",
      "Iteration 1125/2000, Loss: 0.00016408927331212908\n",
      "Iteration 1126/2000, Loss: 0.000226554911932908\n",
      "Iteration 1127/2000, Loss: 0.0006569804390892386\n",
      "Iteration 1128/2000, Loss: 0.00013580934319179505\n",
      "Iteration 1129/2000, Loss: 0.0001565196580486372\n",
      "Iteration 1130/2000, Loss: 0.00022890679247211665\n",
      "Iteration 1131/2000, Loss: 0.00019512447761371732\n",
      "Iteration 1132/2000, Loss: 0.0002178316644858569\n",
      "Iteration 1133/2000, Loss: 0.0001225970045197755\n",
      "Iteration 1134/2000, Loss: 0.00017937898519448936\n",
      "Iteration 1135/2000, Loss: 0.00013247370952740312\n",
      "Iteration 1136/2000, Loss: 0.00020270590903237462\n",
      "Iteration 1137/2000, Loss: 0.00013126044359523803\n",
      "Iteration 1138/2000, Loss: 0.00020330341067165136\n",
      "Iteration 1139/2000, Loss: 0.0002613391261547804\n",
      "Iteration 1140/2000, Loss: 0.0002734332229010761\n",
      "Iteration 1141/2000, Loss: 0.00014510239998344332\n",
      "Iteration 1142/2000, Loss: 0.0002469070313964039\n",
      "Iteration 1143/2000, Loss: 0.00027506801416166127\n",
      "Iteration 1144/2000, Loss: 0.00019222112314309925\n",
      "Iteration 1145/2000, Loss: 0.0002537027176003903\n",
      "Iteration 1146/2000, Loss: 0.00015300442464649677\n",
      "Iteration 1147/2000, Loss: 0.0002532903745304793\n",
      "Iteration 1148/2000, Loss: 0.00017935852520167828\n",
      "Iteration 1149/2000, Loss: 0.0003482895263005048\n",
      "Iteration 1150/2000, Loss: 0.0002647818473633379\n",
      "Iteration 1151/2000, Loss: 0.0001355520071228966\n",
      "Iteration 1152/2000, Loss: 0.0002258239983348176\n",
      "Iteration 1153/2000, Loss: 0.00035392786958254874\n",
      "Iteration 1154/2000, Loss: 0.00025785021716728806\n",
      "Iteration 1155/2000, Loss: 0.00017627241322770715\n",
      "Iteration 1156/2000, Loss: 0.0001542302343295887\n",
      "Iteration 1157/2000, Loss: 0.00027415985823608935\n",
      "Iteration 1158/2000, Loss: 0.00018988132069353014\n",
      "Iteration 1159/2000, Loss: 0.0002332831354578957\n",
      "Iteration 1160/2000, Loss: 0.001244787243194878\n",
      "Iteration 1161/2000, Loss: 0.0003924945485778153\n",
      "Iteration 1162/2000, Loss: 0.000299106992315501\n",
      "Iteration 1163/2000, Loss: 0.00022582626843359321\n",
      "Iteration 1164/2000, Loss: 0.00011453250772319734\n",
      "Iteration 1165/2000, Loss: 0.0002511656784918159\n",
      "Iteration 1166/2000, Loss: 0.00026416496257297695\n",
      "Iteration 1167/2000, Loss: 0.00020469138689804822\n",
      "Iteration 1168/2000, Loss: 0.00018080128938890994\n",
      "Iteration 1169/2000, Loss: 0.00011746471136575565\n",
      "Iteration 1170/2000, Loss: 0.0006712929462082684\n",
      "Iteration 1171/2000, Loss: 0.0001749249640852213\n",
      "Iteration 1172/2000, Loss: 0.00018323901167605072\n",
      "Iteration 1173/2000, Loss: 0.0001693360391072929\n",
      "Iteration 1174/2000, Loss: 0.0002332469157408923\n",
      "Iteration 1175/2000, Loss: 0.0001503854728071019\n",
      "Iteration 1176/2000, Loss: 0.0002051377232419327\n",
      "Iteration 1177/2000, Loss: 0.0001408627285854891\n",
      "Iteration 1178/2000, Loss: 0.00017076342192012817\n",
      "Iteration 1179/2000, Loss: 0.0001376107393298298\n",
      "Iteration 1180/2000, Loss: 0.0003000823489855975\n",
      "Iteration 1181/2000, Loss: 0.00023764281650073826\n",
      "Iteration 1182/2000, Loss: 0.0003575773152988404\n",
      "Iteration 1183/2000, Loss: 0.0002244432980660349\n",
      "Iteration 1184/2000, Loss: 0.00011878070654347539\n",
      "Iteration 1185/2000, Loss: 0.0001323993201367557\n",
      "Iteration 1186/2000, Loss: 0.00016913487343117595\n",
      "Iteration 1187/2000, Loss: 0.00011153955711051822\n",
      "Iteration 1188/2000, Loss: 0.000468371290480718\n",
      "Iteration 1189/2000, Loss: 0.00011348905536578968\n",
      "Iteration 1190/2000, Loss: 0.00017225995543412864\n",
      "Iteration 1191/2000, Loss: 0.00010676896636141464\n",
      "Iteration 1192/2000, Loss: 0.00014940912660676986\n",
      "Iteration 1193/2000, Loss: 0.00015726873243693262\n",
      "Iteration 1194/2000, Loss: 0.00019199997768737376\n",
      "Iteration 1195/2000, Loss: 0.00011058183736167848\n",
      "Iteration 1196/2000, Loss: 0.00029803719371557236\n",
      "Iteration 1197/2000, Loss: 0.0003065363271161914\n",
      "Iteration 1198/2000, Loss: 0.00022649380844086409\n",
      "Iteration 1199/2000, Loss: 0.00019698202959261835\n",
      "Iteration 1200/2000, Loss: 0.0001410287368344143\n",
      "Iteration 1201/2000, Loss: 0.0003789042530115694\n",
      "Iteration 1202/2000, Loss: 0.0009190247510559857\n",
      "Iteration 1203/2000, Loss: 0.00027862004935741425\n",
      "Iteration 1204/2000, Loss: 0.00025548410485498607\n",
      "Iteration 1205/2000, Loss: 0.00012494194379542023\n",
      "Iteration 1206/2000, Loss: 0.00019593977776821703\n",
      "Iteration 1207/2000, Loss: 0.00014037002983968705\n",
      "Iteration 1208/2000, Loss: 0.00012504326878115535\n",
      "Iteration 1209/2000, Loss: 0.0001193229909404181\n",
      "Iteration 1210/2000, Loss: 0.00015302796964533627\n",
      "Iteration 1211/2000, Loss: 0.00027500890428200364\n",
      "Iteration 1212/2000, Loss: 0.00019908917602151632\n",
      "Iteration 1213/2000, Loss: 0.00040958879981189966\n",
      "Iteration 1214/2000, Loss: 0.0002912739000748843\n",
      "Iteration 1215/2000, Loss: 0.00014979536354076117\n",
      "Iteration 1216/2000, Loss: 0.00021297250350471586\n",
      "Iteration 1217/2000, Loss: 0.00015587850066367537\n",
      "Iteration 1218/2000, Loss: 0.00021532390383072197\n",
      "Iteration 1219/2000, Loss: 0.00032851763535290956\n",
      "Iteration 1220/2000, Loss: 0.0002338185004191473\n",
      "Iteration 1221/2000, Loss: 0.0003107009979430586\n",
      "Iteration 1222/2000, Loss: 0.00023509262246079743\n",
      "Iteration 1223/2000, Loss: 0.0002751604188233614\n",
      "Iteration 1224/2000, Loss: 0.0002863218542188406\n",
      "Iteration 1225/2000, Loss: 0.00014503157581202686\n",
      "Iteration 1226/2000, Loss: 0.0004118187353014946\n",
      "Iteration 1227/2000, Loss: 0.00020396859326865524\n",
      "Iteration 1228/2000, Loss: 0.00023265996424015611\n",
      "Iteration 1229/2000, Loss: 0.0003295703209005296\n",
      "Iteration 1230/2000, Loss: 0.00019092517322860658\n",
      "Iteration 1231/2000, Loss: 0.0003044103505089879\n",
      "Iteration 1232/2000, Loss: 0.0001527282438473776\n",
      "Iteration 1233/2000, Loss: 0.0002500387199688703\n",
      "Iteration 1234/2000, Loss: 0.0003016898117493838\n",
      "Iteration 1235/2000, Loss: 0.0001840207987697795\n",
      "Iteration 1236/2000, Loss: 0.0002226509532192722\n",
      "Iteration 1237/2000, Loss: 0.0001772230607457459\n",
      "Iteration 1238/2000, Loss: 0.00025983620434999466\n",
      "Iteration 1239/2000, Loss: 0.00019752797379624099\n",
      "Iteration 1240/2000, Loss: 0.0002127014595316723\n",
      "Iteration 1241/2000, Loss: 0.00020213727839291096\n",
      "Iteration 1242/2000, Loss: 0.00017188908532261848\n",
      "Iteration 1243/2000, Loss: 0.00032797540188767016\n",
      "Iteration 1244/2000, Loss: 0.0003293520421721041\n",
      "Iteration 1245/2000, Loss: 0.00029468146385625005\n",
      "Iteration 1246/2000, Loss: 0.0002164356701541692\n",
      "Iteration 1247/2000, Loss: 0.00031627388671040535\n",
      "Iteration 1248/2000, Loss: 0.0003451489610597491\n",
      "Iteration 1249/2000, Loss: 0.00016448991664219648\n",
      "Iteration 1250/2000, Loss: 0.0007250671042129397\n",
      "Iteration 1251/2000, Loss: 0.00017879980441648513\n",
      "Iteration 1252/2000, Loss: 0.00022493333381135017\n",
      "Iteration 1253/2000, Loss: 0.0001848957472248003\n",
      "Iteration 1254/2000, Loss: 0.0002287152747157961\n",
      "Iteration 1255/2000, Loss: 0.0002077711105812341\n",
      "Iteration 1256/2000, Loss: 0.00019903491192962974\n",
      "Iteration 1257/2000, Loss: 0.00026226736372336745\n",
      "Iteration 1258/2000, Loss: 0.0001722561864880845\n",
      "Iteration 1259/2000, Loss: 0.0001988475996768102\n",
      "Iteration 1260/2000, Loss: 0.00020564894657582045\n",
      "Iteration 1261/2000, Loss: 0.00020676459826063365\n",
      "Iteration 1262/2000, Loss: 0.00011101503332611173\n",
      "Iteration 1263/2000, Loss: 0.00021699223725590855\n",
      "Iteration 1264/2000, Loss: 0.00018305433331988752\n",
      "Iteration 1265/2000, Loss: 0.0004569336015265435\n",
      "Iteration 1266/2000, Loss: 0.00046553832362405956\n",
      "Iteration 1267/2000, Loss: 0.00021033677330706269\n",
      "Iteration 1268/2000, Loss: 0.00028439637389965355\n",
      "Iteration 1269/2000, Loss: 0.0002415891212876886\n",
      "Iteration 1270/2000, Loss: 0.0002608365612104535\n",
      "Iteration 1271/2000, Loss: 0.00021158232993911952\n",
      "Iteration 1272/2000, Loss: 0.00028698224923573434\n",
      "Iteration 1273/2000, Loss: 0.0001672142097959295\n",
      "Iteration 1274/2000, Loss: 0.0003657472843769938\n",
      "Iteration 1275/2000, Loss: 0.00021348247537389398\n",
      "Iteration 1276/2000, Loss: 0.0002873882185667753\n",
      "Iteration 1277/2000, Loss: 0.00022502873616758734\n",
      "Iteration 1278/2000, Loss: 0.00017289041716139764\n",
      "Iteration 1279/2000, Loss: 0.00025959263439290226\n",
      "Iteration 1280/2000, Loss: 0.00018946122145280242\n",
      "Iteration 1281/2000, Loss: 0.00038703932659700513\n",
      "Iteration 1282/2000, Loss: 0.00017941535043064505\n",
      "Iteration 1283/2000, Loss: 0.0006249721627682447\n",
      "Iteration 1284/2000, Loss: 0.00017301487969234586\n",
      "Iteration 1285/2000, Loss: 0.0004986092681065202\n",
      "Iteration 1286/2000, Loss: 0.0005210714298300445\n",
      "Iteration 1287/2000, Loss: 0.0002213576080976054\n",
      "Iteration 1288/2000, Loss: 0.00043812915100716054\n",
      "Iteration 1289/2000, Loss: 0.0002604062610771507\n",
      "Iteration 1290/2000, Loss: 0.0003577711177058518\n",
      "Iteration 1291/2000, Loss: 0.0003224092361051589\n",
      "Iteration 1292/2000, Loss: 0.00022298299882095307\n",
      "Iteration 1293/2000, Loss: 0.0003852240042760968\n",
      "Iteration 1294/2000, Loss: 0.0003042745520360768\n",
      "Iteration 1295/2000, Loss: 0.00022318806441035122\n",
      "Iteration 1296/2000, Loss: 0.0001922472147271037\n",
      "Iteration 1297/2000, Loss: 0.000172988380654715\n",
      "Iteration 1298/2000, Loss: 0.00023160691489465535\n",
      "Iteration 1299/2000, Loss: 0.0002003616391448304\n",
      "Iteration 1300/2000, Loss: 0.00010549254511715844\n",
      "Iteration 1301/2000, Loss: 0.00010097107588080689\n",
      "Iteration 1302/2000, Loss: 0.00038699692231602967\n",
      "Iteration 1303/2000, Loss: 0.00040234479820355773\n",
      "Iteration 1304/2000, Loss: 0.00028008761000819504\n",
      "Iteration 1305/2000, Loss: 0.0002275359584018588\n",
      "Iteration 1306/2000, Loss: 0.0002791470615193248\n",
      "Iteration 1307/2000, Loss: 0.00019357337441761047\n",
      "Iteration 1308/2000, Loss: 0.00024662178475409746\n",
      "Iteration 1309/2000, Loss: 0.00016781444719526917\n",
      "Iteration 1310/2000, Loss: 0.00025125598767772317\n",
      "Iteration 1311/2000, Loss: 0.00019338290439918637\n",
      "Iteration 1312/2000, Loss: 0.0003689770819619298\n",
      "Iteration 1313/2000, Loss: 0.00033199816243723035\n",
      "Iteration 1314/2000, Loss: 0.0002445299760438502\n",
      "Iteration 1315/2000, Loss: 0.0005964972660876811\n",
      "Iteration 1316/2000, Loss: 0.00019291868375148624\n",
      "Iteration 1317/2000, Loss: 0.0005097190733067691\n",
      "Iteration 1318/2000, Loss: 0.00041512513416819274\n",
      "Iteration 1319/2000, Loss: 0.00022495031589642167\n",
      "Iteration 1320/2000, Loss: 0.0005944337463006377\n",
      "Iteration 1321/2000, Loss: 0.00027506041806191206\n",
      "Iteration 1322/2000, Loss: 0.00042147102067247033\n",
      "Iteration 1323/2000, Loss: 0.00032246860791929066\n",
      "Iteration 1324/2000, Loss: 0.0002787609410006553\n",
      "Iteration 1325/2000, Loss: 0.00030281260842457414\n",
      "Iteration 1326/2000, Loss: 0.00026134547078981996\n",
      "Iteration 1327/2000, Loss: 0.0006009067292325199\n",
      "Iteration 1328/2000, Loss: 0.000221680587856099\n",
      "Iteration 1329/2000, Loss: 0.00030936699477024376\n",
      "Iteration 1330/2000, Loss: 0.00016562103701289743\n",
      "Iteration 1331/2000, Loss: 0.00038830647827126086\n",
      "Iteration 1332/2000, Loss: 0.00019616766076069325\n",
      "Iteration 1333/2000, Loss: 0.00024188584939111024\n",
      "Iteration 1334/2000, Loss: 0.0001917829067679122\n",
      "Iteration 1335/2000, Loss: 0.00027830261387862265\n",
      "Iteration 1336/2000, Loss: 0.0002113246009685099\n",
      "Iteration 1337/2000, Loss: 0.0003337981179356575\n",
      "Iteration 1338/2000, Loss: 0.00029106333386152983\n",
      "Iteration 1339/2000, Loss: 0.00031370011856779456\n",
      "Iteration 1340/2000, Loss: 0.0003421454457566142\n",
      "Iteration 1341/2000, Loss: 0.00018994153651874512\n",
      "Iteration 1342/2000, Loss: 0.00023509387392550707\n",
      "Iteration 1343/2000, Loss: 0.0005855561466887593\n",
      "Iteration 1344/2000, Loss: 0.00019640993559733033\n",
      "Iteration 1345/2000, Loss: 0.00017565056623425335\n",
      "Iteration 1346/2000, Loss: 0.00017657518037594855\n",
      "Iteration 1347/2000, Loss: 0.0002196918212575838\n",
      "Iteration 1348/2000, Loss: 0.0002965998719446361\n",
      "Iteration 1349/2000, Loss: 9.618660260457546e-05\n",
      "Iteration 1350/2000, Loss: 0.00017940464022103697\n",
      "Iteration 1351/2000, Loss: 0.00013018686149735004\n",
      "Iteration 1352/2000, Loss: 0.0001980404631467536\n",
      "Iteration 1353/2000, Loss: 0.00012091267126379535\n",
      "Iteration 1354/2000, Loss: 0.00014437428035307676\n",
      "Iteration 1355/2000, Loss: 0.00025662159896455705\n",
      "Iteration 1356/2000, Loss: 0.00016425266221631318\n",
      "Iteration 1357/2000, Loss: 0.00024616223527118564\n",
      "Iteration 1358/2000, Loss: 0.00027833771309815347\n",
      "Iteration 1359/2000, Loss: 0.00016524559760000557\n",
      "Iteration 1360/2000, Loss: 0.00015150477702263743\n",
      "Iteration 1361/2000, Loss: 0.00036093845847062767\n",
      "Iteration 1362/2000, Loss: 0.00024243695952463895\n",
      "Iteration 1363/2000, Loss: 0.00012013800005661324\n",
      "Iteration 1364/2000, Loss: 0.00023295335995499045\n",
      "Iteration 1365/2000, Loss: 0.0001320423325523734\n",
      "Iteration 1366/2000, Loss: 9.245880937669426e-05\n",
      "Iteration 1367/2000, Loss: 0.00012960151070728898\n",
      "Iteration 1368/2000, Loss: 0.00021864728478249162\n",
      "Iteration 1369/2000, Loss: 0.00012168286048108712\n",
      "Iteration 1370/2000, Loss: 0.00013490165292751044\n",
      "Iteration 1371/2000, Loss: 0.00014602227020077407\n",
      "Iteration 1372/2000, Loss: 0.00023419670469593257\n",
      "Iteration 1373/2000, Loss: 0.00022492372954729944\n",
      "Iteration 1374/2000, Loss: 0.0002127769694197923\n",
      "Iteration 1375/2000, Loss: 0.00022238495876081288\n",
      "Iteration 1376/2000, Loss: 0.00020755910372827202\n",
      "Iteration 1377/2000, Loss: 0.0001894381712190807\n",
      "Iteration 1378/2000, Loss: 0.0002355777978664264\n",
      "Iteration 1379/2000, Loss: 0.00017528983880765736\n",
      "Iteration 1380/2000, Loss: 0.00018811962218023837\n",
      "Iteration 1381/2000, Loss: 0.0001306197082158178\n",
      "Iteration 1382/2000, Loss: 0.00023439846700057387\n",
      "Iteration 1383/2000, Loss: 0.00026596811949275434\n",
      "Iteration 1384/2000, Loss: 0.0002612974785733968\n",
      "Iteration 1385/2000, Loss: 0.00017531870980747044\n",
      "Iteration 1386/2000, Loss: 0.00028035021387040615\n",
      "Iteration 1387/2000, Loss: 0.00021527920034714043\n",
      "Iteration 1388/2000, Loss: 0.00016254265210591257\n",
      "Iteration 1389/2000, Loss: 0.00043231205199845135\n",
      "Iteration 1390/2000, Loss: 0.0004047341935802251\n",
      "Iteration 1391/2000, Loss: 0.00026451866142451763\n",
      "Iteration 1392/2000, Loss: 0.0003782185085583478\n",
      "Iteration 1393/2000, Loss: 0.00017063150880858302\n",
      "Iteration 1394/2000, Loss: 0.0004231336060911417\n",
      "Iteration 1395/2000, Loss: 0.00017789796402212232\n",
      "Iteration 1396/2000, Loss: 0.0002722553617786616\n",
      "Iteration 1397/2000, Loss: 0.00013619396486319602\n",
      "Iteration 1398/2000, Loss: 0.0002052432973869145\n",
      "Iteration 1399/2000, Loss: 0.0001554325281176716\n",
      "Iteration 1400/2000, Loss: 0.00015425513265654445\n",
      "Iteration 1401/2000, Loss: 0.00024456108803860843\n",
      "Iteration 1402/2000, Loss: 0.0002557084371801466\n",
      "Iteration 1403/2000, Loss: 0.0002737130271270871\n",
      "Iteration 1404/2000, Loss: 0.0002090866764774546\n",
      "Iteration 1405/2000, Loss: 0.00023093614436220378\n",
      "Iteration 1406/2000, Loss: 0.00024518626742064953\n",
      "Iteration 1407/2000, Loss: 0.0002946403401438147\n",
      "Iteration 1408/2000, Loss: 0.0002798132481984794\n",
      "Iteration 1409/2000, Loss: 0.0001619633985683322\n",
      "Iteration 1410/2000, Loss: 0.0002330215647816658\n",
      "Iteration 1411/2000, Loss: 0.00033432678901590407\n",
      "Iteration 1412/2000, Loss: 0.0003182001819368452\n",
      "Iteration 1413/2000, Loss: 0.00017009394650813192\n",
      "Iteration 1414/2000, Loss: 0.0001981278182938695\n",
      "Iteration 1415/2000, Loss: 0.0002881437831092626\n",
      "Iteration 1416/2000, Loss: 0.00029688424547202885\n",
      "Iteration 1417/2000, Loss: 0.00030290347058326006\n",
      "Iteration 1418/2000, Loss: 0.00011160667054355145\n",
      "Iteration 1419/2000, Loss: 0.00021773278422188014\n",
      "Iteration 1420/2000, Loss: 0.00014835712499916553\n",
      "Iteration 1421/2000, Loss: 0.00036197443841956556\n",
      "Iteration 1422/2000, Loss: 0.0002500191330909729\n",
      "Iteration 1423/2000, Loss: 0.0008972139330580831\n",
      "Iteration 1424/2000, Loss: 0.0001871400891104713\n",
      "Iteration 1425/2000, Loss: 0.00018817010277416557\n",
      "Iteration 1426/2000, Loss: 0.000276635168120265\n",
      "Iteration 1427/2000, Loss: 0.0002674036950338632\n",
      "Iteration 1428/2000, Loss: 0.0002132546214852482\n",
      "Iteration 1429/2000, Loss: 0.00015270235599018633\n",
      "Iteration 1430/2000, Loss: 0.00011205252667423338\n",
      "Iteration 1431/2000, Loss: 0.00034966078237630427\n",
      "Iteration 1432/2000, Loss: 0.00015960213204380125\n",
      "Iteration 1433/2000, Loss: 0.00022264511790126562\n",
      "Iteration 1434/2000, Loss: 0.00021415780065581203\n",
      "Iteration 1435/2000, Loss: 0.0002216692955698818\n",
      "Iteration 1436/2000, Loss: 0.0003240323276259005\n",
      "Iteration 1437/2000, Loss: 0.00022553351300302893\n",
      "Iteration 1438/2000, Loss: 0.00024179105821531266\n",
      "Iteration 1439/2000, Loss: 0.00019727942708414048\n",
      "Iteration 1440/2000, Loss: 0.0001813638664316386\n",
      "Iteration 1441/2000, Loss: 0.00017678085714578629\n",
      "Iteration 1442/2000, Loss: 0.00021172265405766666\n",
      "Iteration 1443/2000, Loss: 0.0001163899214589037\n",
      "Iteration 1444/2000, Loss: 0.0003195672179572284\n",
      "Iteration 1445/2000, Loss: 0.00019867530500050634\n",
      "Iteration 1446/2000, Loss: 0.00017224106704816222\n",
      "Iteration 1447/2000, Loss: 0.0001321824820479378\n",
      "Iteration 1448/2000, Loss: 0.00010493558511370793\n",
      "Iteration 1449/2000, Loss: 0.00013137322093825787\n",
      "Iteration 1450/2000, Loss: 0.00047239160630851984\n",
      "Iteration 1451/2000, Loss: 0.0001757996215019375\n",
      "Iteration 1452/2000, Loss: 9.320813114754856e-05\n",
      "Iteration 1453/2000, Loss: 0.00012018518464174122\n",
      "Iteration 1454/2000, Loss: 0.00028693946660496294\n",
      "Iteration 1455/2000, Loss: 0.0002772936422843486\n",
      "Iteration 1456/2000, Loss: 0.00015186650853138417\n",
      "Iteration 1457/2000, Loss: 0.00015781696129124612\n",
      "Iteration 1458/2000, Loss: 0.00018324791744817048\n",
      "Iteration 1459/2000, Loss: 0.00025841384194791317\n",
      "Iteration 1460/2000, Loss: 0.0002410731976851821\n",
      "Iteration 1461/2000, Loss: 0.00018404325237497687\n",
      "Iteration 1462/2000, Loss: 0.0001920509384945035\n",
      "Iteration 1463/2000, Loss: 0.0003209987189620733\n",
      "Iteration 1464/2000, Loss: 0.0001624256547074765\n",
      "Iteration 1465/2000, Loss: 0.0001494889729656279\n",
      "Iteration 1466/2000, Loss: 0.0002514223160687834\n",
      "Iteration 1467/2000, Loss: 0.00015456153778359294\n",
      "Iteration 1468/2000, Loss: 0.00012427153706084937\n",
      "Iteration 1469/2000, Loss: 0.0001352730323560536\n",
      "Iteration 1470/2000, Loss: 0.00016041030175983906\n",
      "Iteration 1471/2000, Loss: 0.00013774856051895767\n",
      "Iteration 1472/2000, Loss: 0.00012995180441066623\n",
      "Iteration 1473/2000, Loss: 0.00026023504324257374\n",
      "Iteration 1474/2000, Loss: 0.00015059103316161782\n",
      "Iteration 1475/2000, Loss: 9.220535139320418e-05\n",
      "Iteration 1476/2000, Loss: 0.00012862461153417826\n",
      "Iteration 1477/2000, Loss: 0.0001168894741567783\n",
      "Iteration 1478/2000, Loss: 0.00013118790229782462\n",
      "Iteration 1479/2000, Loss: 0.00023641690495423973\n",
      "Iteration 1480/2000, Loss: 0.00016797345597296953\n",
      "Iteration 1481/2000, Loss: 0.0002188028593081981\n",
      "Iteration 1482/2000, Loss: 0.00023285805946215987\n",
      "Iteration 1483/2000, Loss: 0.00073788728332147\n",
      "Iteration 1484/2000, Loss: 0.00017916620709002018\n",
      "Iteration 1485/2000, Loss: 0.0003562051570042968\n",
      "Iteration 1486/2000, Loss: 0.00011411814921302721\n",
      "Iteration 1487/2000, Loss: 0.00022060539049562067\n",
      "Iteration 1488/2000, Loss: 0.0001459778140997514\n",
      "Iteration 1489/2000, Loss: 0.00023691155365668237\n",
      "Iteration 1490/2000, Loss: 0.0002499222755432129\n",
      "Iteration 1491/2000, Loss: 0.00019331867224536836\n",
      "Iteration 1492/2000, Loss: 0.0001909317506942898\n",
      "Iteration 1493/2000, Loss: 0.00026325887301936746\n",
      "Iteration 1494/2000, Loss: 0.00022757821716368198\n",
      "Iteration 1495/2000, Loss: 0.00022678390087094158\n",
      "Iteration 1496/2000, Loss: 0.00018542793986853212\n",
      "Iteration 1497/2000, Loss: 0.00042644538916647434\n",
      "Iteration 1498/2000, Loss: 0.0004611231852322817\n",
      "Iteration 1499/2000, Loss: 0.00013909827976021916\n",
      "Iteration 1500/2000, Loss: 0.0004140453820582479\n",
      "Iteration 1501/2000, Loss: 0.0001921313814818859\n",
      "Iteration 1502/2000, Loss: 0.0003658614296000451\n",
      "Iteration 1503/2000, Loss: 0.00031479637254960835\n",
      "Iteration 1504/2000, Loss: 0.000569667259696871\n",
      "Iteration 1505/2000, Loss: 0.00027450028574094176\n",
      "Iteration 1506/2000, Loss: 0.00048664765199646354\n",
      "Iteration 1507/2000, Loss: 0.00020392842998262495\n",
      "Iteration 1508/2000, Loss: 0.0005500670522451401\n",
      "Iteration 1509/2000, Loss: 0.000262841465882957\n",
      "Iteration 1510/2000, Loss: 0.0003608021361287683\n",
      "Iteration 1511/2000, Loss: 0.0003762939595617354\n",
      "Iteration 1512/2000, Loss: 0.00030342646641656756\n",
      "Iteration 1513/2000, Loss: 0.0006102433544583619\n",
      "Iteration 1514/2000, Loss: 0.00020584840967785567\n",
      "Iteration 1515/2000, Loss: 0.0004125995037611574\n",
      "Iteration 1516/2000, Loss: 0.0002439070667605847\n",
      "Iteration 1517/2000, Loss: 0.0004006649542134255\n",
      "Iteration 1518/2000, Loss: 0.00018943897157441825\n",
      "Iteration 1519/2000, Loss: 0.0006282827234826982\n",
      "Iteration 1520/2000, Loss: 0.00031964448862709105\n",
      "Iteration 1521/2000, Loss: 0.0003927763318642974\n",
      "Iteration 1522/2000, Loss: 0.0007660873234272003\n",
      "Iteration 1523/2000, Loss: 0.00020673550898209214\n",
      "Iteration 1524/2000, Loss: 0.0002995309478137642\n",
      "Iteration 1525/2000, Loss: 0.00010692104115150869\n",
      "Iteration 1526/2000, Loss: 0.00018228394037578255\n",
      "Iteration 1527/2000, Loss: 0.0001326545316260308\n",
      "Iteration 1528/2000, Loss: 0.0004340939049143344\n",
      "Iteration 1529/2000, Loss: 0.0003305708523839712\n",
      "Iteration 1530/2000, Loss: 0.00021557498257607222\n",
      "Iteration 1531/2000, Loss: 0.00036839870153926313\n",
      "Iteration 1532/2000, Loss: 0.0003214430471416563\n",
      "Iteration 1533/2000, Loss: 0.00020323388162069023\n",
      "Iteration 1534/2000, Loss: 0.0002631831157486886\n",
      "Iteration 1535/2000, Loss: 0.00019619252998381853\n",
      "Iteration 1536/2000, Loss: 0.0002540619170758873\n",
      "Iteration 1537/2000, Loss: 0.00023917823273222893\n",
      "Iteration 1538/2000, Loss: 0.00033692250144667923\n",
      "Iteration 1539/2000, Loss: 0.00016222341218963265\n",
      "Iteration 1540/2000, Loss: 0.0001386995572829619\n",
      "Iteration 1541/2000, Loss: 0.0001467342081014067\n",
      "Iteration 1542/2000, Loss: 0.0001809072564356029\n",
      "Iteration 1543/2000, Loss: 0.0001623519347049296\n",
      "Iteration 1544/2000, Loss: 0.0003238704812247306\n",
      "Iteration 1545/2000, Loss: 0.000245740549871698\n",
      "Iteration 1546/2000, Loss: 0.0001332985411863774\n",
      "Iteration 1547/2000, Loss: 0.00031056979787535965\n",
      "Iteration 1548/2000, Loss: 0.0001303492608712986\n",
      "Iteration 1549/2000, Loss: 0.0002601526794023812\n",
      "Iteration 1550/2000, Loss: 0.0002036733931163326\n",
      "Iteration 1551/2000, Loss: 0.00020926656725350767\n",
      "Iteration 1552/2000, Loss: 0.000255357677815482\n",
      "Iteration 1553/2000, Loss: 0.00014387484407052398\n",
      "Iteration 1554/2000, Loss: 0.00018329196609556675\n",
      "Iteration 1555/2000, Loss: 0.00021772531908936799\n",
      "Iteration 1556/2000, Loss: 0.00014117640967015177\n",
      "Iteration 1557/2000, Loss: 0.00015282828826457262\n",
      "Iteration 1558/2000, Loss: 0.0001517652563052252\n",
      "Iteration 1559/2000, Loss: 0.00044293786049820483\n",
      "Iteration 1560/2000, Loss: 0.0001180881736217998\n",
      "Iteration 1561/2000, Loss: 0.0001845851365942508\n",
      "Iteration 1562/2000, Loss: 0.00022453107521869242\n",
      "Iteration 1563/2000, Loss: 0.00019873563724104315\n",
      "Iteration 1564/2000, Loss: 0.0001468930277042091\n",
      "Iteration 1565/2000, Loss: 0.00025866684154607356\n",
      "Iteration 1566/2000, Loss: 0.0002590201620478183\n",
      "Iteration 1567/2000, Loss: 0.0003280786913819611\n",
      "Iteration 1568/2000, Loss: 0.0002290589764015749\n",
      "Iteration 1569/2000, Loss: 0.00030500191496685147\n",
      "Iteration 1570/2000, Loss: 0.00026878350763581693\n",
      "Iteration 1571/2000, Loss: 0.00024627201491966844\n",
      "Iteration 1572/2000, Loss: 0.00021156747243367136\n",
      "Iteration 1573/2000, Loss: 0.00014798497431911528\n",
      "Iteration 1574/2000, Loss: 0.00019421362958382815\n",
      "Iteration 1575/2000, Loss: 0.00016121049702633172\n",
      "Iteration 1576/2000, Loss: 0.00033966032788157463\n",
      "Iteration 1577/2000, Loss: 0.0002338528574910015\n",
      "Iteration 1578/2000, Loss: 0.000282889639493078\n",
      "Iteration 1579/2000, Loss: 0.0002616081037558615\n",
      "Iteration 1580/2000, Loss: 0.00020869360014330596\n",
      "Iteration 1581/2000, Loss: 0.00033681263448670506\n",
      "Iteration 1582/2000, Loss: 0.00013042555656284094\n",
      "Iteration 1583/2000, Loss: 0.00030964476172812283\n",
      "Iteration 1584/2000, Loss: 0.00014540409028995782\n",
      "Iteration 1585/2000, Loss: 0.0003211714792996645\n",
      "Iteration 1586/2000, Loss: 0.0002774205640889704\n",
      "Iteration 1587/2000, Loss: 0.00031046627555042505\n",
      "Iteration 1588/2000, Loss: 0.00016206075088120997\n",
      "Iteration 1589/2000, Loss: 0.000311869487632066\n",
      "Iteration 1590/2000, Loss: 0.00031332901562564075\n",
      "Iteration 1591/2000, Loss: 0.00012357781815808266\n",
      "Iteration 1592/2000, Loss: 0.00013403044431470335\n",
      "Iteration 1593/2000, Loss: 0.00020001945085823536\n",
      "Iteration 1594/2000, Loss: 0.00013099765055812895\n",
      "Iteration 1595/2000, Loss: 0.00027857517125084996\n",
      "Iteration 1596/2000, Loss: 0.00025967834517359734\n",
      "Iteration 1597/2000, Loss: 0.00021018895495217294\n",
      "Iteration 1598/2000, Loss: 0.00020913076878059655\n",
      "Iteration 1599/2000, Loss: 0.00030148777295835316\n",
      "Iteration 1600/2000, Loss: 0.0001696339895715937\n",
      "Iteration 1601/2000, Loss: 0.00015896685363259166\n",
      "Iteration 1602/2000, Loss: 0.00014177634147927165\n",
      "Iteration 1603/2000, Loss: 0.00015207262185867876\n",
      "Iteration 1604/2000, Loss: 0.00015517215069849044\n",
      "Iteration 1605/2000, Loss: 0.0001844039506977424\n",
      "Iteration 1606/2000, Loss: 0.0002076475357171148\n",
      "Iteration 1607/2000, Loss: 0.00017210216901730746\n",
      "Iteration 1608/2000, Loss: 0.00020685243362095207\n",
      "Iteration 1609/2000, Loss: 0.0001488359848735854\n",
      "Iteration 1610/2000, Loss: 0.0002903642307501286\n",
      "Iteration 1611/2000, Loss: 0.0003631153376773\n",
      "Iteration 1612/2000, Loss: 0.00012438152043614537\n",
      "Iteration 1613/2000, Loss: 0.00031157612102106214\n",
      "Iteration 1614/2000, Loss: 0.0001878955663414672\n",
      "Iteration 1615/2000, Loss: 0.00020739511819556355\n",
      "Iteration 1616/2000, Loss: 0.00022853209520690143\n",
      "Iteration 1617/2000, Loss: 0.00012864044401794672\n",
      "Iteration 1618/2000, Loss: 0.00016279000556096435\n",
      "Iteration 1619/2000, Loss: 0.0001506864937255159\n",
      "Iteration 1620/2000, Loss: 0.00023030588636174798\n",
      "Iteration 1621/2000, Loss: 0.0001472615113016218\n",
      "Iteration 1622/2000, Loss: 0.0001604418212082237\n",
      "Iteration 1623/2000, Loss: 0.00016173208132386208\n",
      "Iteration 1624/2000, Loss: 0.0003401442663744092\n",
      "Iteration 1625/2000, Loss: 0.00025919423205778003\n",
      "Iteration 1626/2000, Loss: 9.756627696333453e-05\n",
      "Iteration 1627/2000, Loss: 0.00018621426715981215\n",
      "Iteration 1628/2000, Loss: 0.00017895513155963272\n",
      "Iteration 1629/2000, Loss: 0.00015390841872431338\n",
      "Iteration 1630/2000, Loss: 0.00021938442660029978\n",
      "Iteration 1631/2000, Loss: 0.00016724552551750094\n",
      "Iteration 1632/2000, Loss: 0.0001686645409790799\n",
      "Iteration 1633/2000, Loss: 0.00013118285278324038\n",
      "Iteration 1634/2000, Loss: 0.00017427882994525135\n",
      "Iteration 1635/2000, Loss: 0.0001540851080790162\n",
      "Iteration 1636/2000, Loss: 0.0001203871943289414\n",
      "Iteration 1637/2000, Loss: 0.0003844338934868574\n",
      "Iteration 1638/2000, Loss: 0.00019758014241233468\n",
      "Iteration 1639/2000, Loss: 0.0002687703527044505\n",
      "Iteration 1640/2000, Loss: 0.0001352101389784366\n",
      "Iteration 1641/2000, Loss: 0.0003060692688450217\n",
      "Iteration 1642/2000, Loss: 0.00026300616445951164\n",
      "Iteration 1643/2000, Loss: 0.0001727562485029921\n",
      "Iteration 1644/2000, Loss: 0.000303513603284955\n",
      "Iteration 1645/2000, Loss: 0.00023701776808593422\n",
      "Iteration 1646/2000, Loss: 0.00020599999697878957\n",
      "Iteration 1647/2000, Loss: 0.00016611955652479082\n",
      "Iteration 1648/2000, Loss: 0.00014636018022429198\n",
      "Iteration 1649/2000, Loss: 0.00017546938033774495\n",
      "Iteration 1650/2000, Loss: 0.00012614810839295387\n",
      "Iteration 1651/2000, Loss: 0.0001792488037608564\n",
      "Iteration 1652/2000, Loss: 0.00014789929264225066\n",
      "Iteration 1653/2000, Loss: 0.0003927857324015349\n",
      "Iteration 1654/2000, Loss: 0.00026594268274493515\n",
      "Iteration 1655/2000, Loss: 0.00027407638845033944\n",
      "Iteration 1656/2000, Loss: 0.0004703032609540969\n",
      "Iteration 1657/2000, Loss: 0.0002772064181044698\n",
      "Iteration 1658/2000, Loss: 0.0002482348936609924\n",
      "Iteration 1659/2000, Loss: 0.0006995318690314889\n",
      "Iteration 1660/2000, Loss: 0.00020093898638151586\n",
      "Iteration 1661/2000, Loss: 0.00028499981272034347\n",
      "Iteration 1662/2000, Loss: 0.0006013271049596369\n",
      "Iteration 1663/2000, Loss: 0.00018224822997581214\n",
      "Iteration 1664/2000, Loss: 0.0003061274765059352\n",
      "Iteration 1665/2000, Loss: 0.0002345645916648209\n",
      "Iteration 1666/2000, Loss: 0.00032259951694868505\n",
      "Iteration 1667/2000, Loss: 0.0002432682813378051\n",
      "Iteration 1668/2000, Loss: 0.00024370818573515862\n",
      "Iteration 1669/2000, Loss: 0.0006135341245681047\n",
      "Iteration 1670/2000, Loss: 0.0003205183893442154\n",
      "Iteration 1671/2000, Loss: 0.0004933135351166129\n",
      "Iteration 1672/2000, Loss: 0.0002470970503054559\n",
      "Iteration 1673/2000, Loss: 0.000374478375306353\n",
      "Iteration 1674/2000, Loss: 0.0001985546841751784\n",
      "Iteration 1675/2000, Loss: 0.0002318730839760974\n",
      "Iteration 1676/2000, Loss: 0.00022646320576313883\n",
      "Iteration 1677/2000, Loss: 0.0002860988024622202\n",
      "Iteration 1678/2000, Loss: 0.0010418263264000416\n",
      "Iteration 1679/2000, Loss: 0.00025290207122452557\n",
      "Iteration 1680/2000, Loss: 0.00023314905411098152\n",
      "Iteration 1681/2000, Loss: 0.0002061763225356117\n",
      "Iteration 1682/2000, Loss: 0.0003333266358822584\n",
      "Iteration 1683/2000, Loss: 0.00012968461669515818\n",
      "Iteration 1684/2000, Loss: 0.00029235673719085753\n",
      "Iteration 1685/2000, Loss: 0.00025264848954975605\n",
      "Iteration 1686/2000, Loss: 0.0002579739666543901\n",
      "Iteration 1687/2000, Loss: 0.00014207571803126484\n",
      "Iteration 1688/2000, Loss: 0.0003955153515562415\n",
      "Iteration 1689/2000, Loss: 0.0002354252792429179\n",
      "Iteration 1690/2000, Loss: 0.00014588725753128529\n",
      "Iteration 1691/2000, Loss: 0.0003421979781705886\n",
      "Iteration 1692/2000, Loss: 0.0001915454340633005\n",
      "Iteration 1693/2000, Loss: 0.00018734131299424917\n",
      "Iteration 1694/2000, Loss: 0.0003088794182986021\n",
      "Iteration 1695/2000, Loss: 0.00014785033999942243\n",
      "Iteration 1696/2000, Loss: 0.00016181891260202974\n",
      "Iteration 1697/2000, Loss: 0.00014165799075271934\n",
      "Iteration 1698/2000, Loss: 0.00018618893227539957\n",
      "Iteration 1699/2000, Loss: 0.0002553524391259998\n",
      "Iteration 1700/2000, Loss: 0.0003145023074466735\n",
      "Iteration 1701/2000, Loss: 0.0001505634281784296\n",
      "Iteration 1702/2000, Loss: 0.0002416244533378631\n",
      "Iteration 1703/2000, Loss: 0.0002760072238743305\n",
      "Iteration 1704/2000, Loss: 0.00026703692856244743\n",
      "Iteration 1705/2000, Loss: 0.00021485578326974064\n",
      "Iteration 1706/2000, Loss: 0.00019694723596330732\n",
      "Iteration 1707/2000, Loss: 0.0002692650305107236\n",
      "Iteration 1708/2000, Loss: 0.0004648198955692351\n",
      "Iteration 1709/2000, Loss: 0.0001573840418132022\n",
      "Iteration 1710/2000, Loss: 0.00016830065578687936\n",
      "Iteration 1711/2000, Loss: 0.00023237490677274764\n",
      "Iteration 1712/2000, Loss: 0.00019981565128546208\n",
      "Iteration 1713/2000, Loss: 0.00013988910359330475\n",
      "Iteration 1714/2000, Loss: 0.000304869026876986\n",
      "Iteration 1715/2000, Loss: 0.00019341890583746135\n",
      "Iteration 1716/2000, Loss: 0.0002224049821961671\n",
      "Iteration 1717/2000, Loss: 0.00026277187862433493\n",
      "Iteration 1718/2000, Loss: 0.00015342723054345697\n",
      "Iteration 1719/2000, Loss: 0.00021238939370959997\n",
      "Iteration 1720/2000, Loss: 0.0002251841506222263\n",
      "Iteration 1721/2000, Loss: 0.00020390110148582608\n",
      "Iteration 1722/2000, Loss: 0.00019279304251540452\n",
      "Iteration 1723/2000, Loss: 0.00021998531883582473\n",
      "Iteration 1724/2000, Loss: 0.0003298610099591315\n",
      "Iteration 1725/2000, Loss: 0.00018681953952182084\n",
      "Iteration 1726/2000, Loss: 0.00013762727030552924\n",
      "Iteration 1727/2000, Loss: 0.00012530166713986546\n",
      "Iteration 1728/2000, Loss: 0.00020280778699088842\n",
      "Iteration 1729/2000, Loss: 0.00021858708350919187\n",
      "Iteration 1730/2000, Loss: 0.00012690825678873807\n",
      "Iteration 1731/2000, Loss: 0.00035999048850499094\n",
      "Iteration 1732/2000, Loss: 0.0001448415277991444\n",
      "Iteration 1733/2000, Loss: 0.00014328341057989746\n",
      "Iteration 1734/2000, Loss: 0.00013779991422779858\n",
      "Iteration 1735/2000, Loss: 0.00024660798953846097\n",
      "Iteration 1736/2000, Loss: 0.00014573820226360112\n",
      "Iteration 1737/2000, Loss: 0.0002307547110831365\n",
      "Iteration 1738/2000, Loss: 0.00015273760072886944\n",
      "Iteration 1739/2000, Loss: 0.00017861365631688386\n",
      "Iteration 1740/2000, Loss: 0.00023126736050471663\n",
      "Iteration 1741/2000, Loss: 0.00011629059008555487\n",
      "Iteration 1742/2000, Loss: 0.00019344419706612825\n",
      "Iteration 1743/2000, Loss: 0.00019678114040289074\n",
      "Iteration 1744/2000, Loss: 0.00022184733825270087\n",
      "Iteration 1745/2000, Loss: 0.00017208933422807604\n",
      "Iteration 1746/2000, Loss: 0.0007781803142279387\n",
      "Iteration 1747/2000, Loss: 0.00025368155911564827\n",
      "Iteration 1748/2000, Loss: 0.00019739790877792984\n",
      "Iteration 1749/2000, Loss: 0.00023118707758840173\n",
      "Iteration 1750/2000, Loss: 0.00016099402273539454\n",
      "Iteration 1751/2000, Loss: 0.0002002857654588297\n",
      "Iteration 1752/2000, Loss: 0.00032240463769994676\n",
      "Iteration 1753/2000, Loss: 0.00014447780267801136\n",
      "Iteration 1754/2000, Loss: 0.00026671375962905586\n",
      "Iteration 1755/2000, Loss: 0.00018313918553758413\n",
      "Iteration 1756/2000, Loss: 0.00015658106713090092\n",
      "Iteration 1757/2000, Loss: 0.0002242909395135939\n",
      "Iteration 1758/2000, Loss: 0.00019086006795987487\n",
      "Iteration 1759/2000, Loss: 0.00019821712339762598\n",
      "Iteration 1760/2000, Loss: 0.00019845765200443566\n",
      "Iteration 1761/2000, Loss: 0.00019811216043308377\n",
      "Iteration 1762/2000, Loss: 0.0001994584163185209\n",
      "Iteration 1763/2000, Loss: 0.0003050571831408888\n",
      "Iteration 1764/2000, Loss: 0.0002829243603628129\n",
      "Iteration 1765/2000, Loss: 0.0001571979810250923\n",
      "Iteration 1766/2000, Loss: 0.0001748841896187514\n",
      "Iteration 1767/2000, Loss: 0.000245381350396201\n",
      "Iteration 1768/2000, Loss: 0.0001757308782543987\n",
      "Iteration 1769/2000, Loss: 0.00025830575032159686\n",
      "Iteration 1770/2000, Loss: 0.0002930705959443003\n",
      "Iteration 1771/2000, Loss: 0.00022272557544056326\n",
      "Iteration 1772/2000, Loss: 0.0005113676306791604\n",
      "Iteration 1773/2000, Loss: 0.00019410756067372859\n",
      "Iteration 1774/2000, Loss: 0.00019652651099022478\n",
      "Iteration 1775/2000, Loss: 0.0001431906275684014\n",
      "Iteration 1776/2000, Loss: 0.0002468299644533545\n",
      "Iteration 1777/2000, Loss: 0.00017305997607763857\n",
      "Iteration 1778/2000, Loss: 0.00015668952255509794\n",
      "Iteration 1779/2000, Loss: 0.00033626274671405554\n",
      "Iteration 1780/2000, Loss: 0.00010611040488583967\n",
      "Iteration 1781/2000, Loss: 0.0002363954990869388\n",
      "Iteration 1782/2000, Loss: 0.0003583412035368383\n",
      "Iteration 1783/2000, Loss: 0.00023824659001547843\n",
      "Iteration 1784/2000, Loss: 0.00036753001040779054\n",
      "Iteration 1785/2000, Loss: 0.00015091213572304696\n",
      "Iteration 1786/2000, Loss: 0.00021257025946397334\n",
      "Iteration 1787/2000, Loss: 0.0002176668931497261\n",
      "Iteration 1788/2000, Loss: 0.0002304283407283947\n",
      "Iteration 1789/2000, Loss: 0.000284430047031492\n",
      "Iteration 1790/2000, Loss: 0.00014988322800491005\n",
      "Iteration 1791/2000, Loss: 0.00023911138123366982\n",
      "Iteration 1792/2000, Loss: 0.0001612058113096282\n",
      "Iteration 1793/2000, Loss: 0.0002861680113710463\n",
      "Iteration 1794/2000, Loss: 0.0002868749725166708\n",
      "Iteration 1795/2000, Loss: 0.0002272408310091123\n",
      "Iteration 1796/2000, Loss: 0.00015336919750552624\n",
      "Iteration 1797/2000, Loss: 0.0002657135482877493\n",
      "Iteration 1798/2000, Loss: 0.00021731753076892346\n",
      "Iteration 1799/2000, Loss: 0.0002637639991007745\n",
      "Iteration 1800/2000, Loss: 0.00022507314861286432\n",
      "Iteration 1801/2000, Loss: 0.00030214583966881037\n",
      "Iteration 1802/2000, Loss: 0.00025105668464675546\n",
      "Iteration 1803/2000, Loss: 0.0002535225939936936\n",
      "Iteration 1804/2000, Loss: 0.00029515920323319733\n",
      "Iteration 1805/2000, Loss: 0.00022844295017421246\n",
      "Iteration 1806/2000, Loss: 0.0004281884175725281\n",
      "Iteration 1807/2000, Loss: 0.0003263823455199599\n",
      "Iteration 1808/2000, Loss: 0.00018910664948634803\n",
      "Iteration 1809/2000, Loss: 0.0003200874198228121\n",
      "Iteration 1810/2000, Loss: 0.00026489124866202474\n",
      "Iteration 1811/2000, Loss: 0.0003810037742368877\n",
      "Iteration 1812/2000, Loss: 0.0002582831948529929\n",
      "Iteration 1813/2000, Loss: 0.000487781799165532\n",
      "Iteration 1814/2000, Loss: 0.0001570088934386149\n",
      "Iteration 1815/2000, Loss: 0.0002964017039630562\n",
      "Iteration 1816/2000, Loss: 0.0002626564528327435\n",
      "Iteration 1817/2000, Loss: 0.00020030360610689968\n",
      "Iteration 1818/2000, Loss: 0.00023531558690592647\n",
      "Iteration 1819/2000, Loss: 0.0002632676623761654\n",
      "Iteration 1820/2000, Loss: 0.0002973638183902949\n",
      "Iteration 1821/2000, Loss: 0.00020173110533505678\n",
      "Iteration 1822/2000, Loss: 0.00024526158813387156\n",
      "Iteration 1823/2000, Loss: 0.00023174200032372028\n",
      "Iteration 1824/2000, Loss: 0.00026281358441337943\n",
      "Iteration 1825/2000, Loss: 0.00013267426402308047\n",
      "Iteration 1826/2000, Loss: 0.0001423325011273846\n",
      "Iteration 1827/2000, Loss: 0.00016059045447036624\n",
      "Iteration 1828/2000, Loss: 0.000140495176310651\n",
      "Iteration 1829/2000, Loss: 0.00014152379299048334\n",
      "Iteration 1830/2000, Loss: 0.00016044855874497443\n",
      "Iteration 1831/2000, Loss: 0.00019622214313130826\n",
      "Iteration 1832/2000, Loss: 0.0001517888013040647\n",
      "Iteration 1833/2000, Loss: 0.00012442121806088835\n",
      "Iteration 1834/2000, Loss: 0.00018602075579110533\n",
      "Iteration 1835/2000, Loss: 0.00012898151180706918\n",
      "Iteration 1836/2000, Loss: 0.00012091410462744534\n",
      "Iteration 1837/2000, Loss: 0.00012485444312915206\n",
      "Iteration 1838/2000, Loss: 0.00012663193047046661\n",
      "Iteration 1839/2000, Loss: 0.00013195528299547732\n",
      "Iteration 1840/2000, Loss: 0.00015779527893755585\n",
      "Iteration 1841/2000, Loss: 0.0002708326210267842\n",
      "Iteration 1842/2000, Loss: 0.0001555413327878341\n",
      "Iteration 1843/2000, Loss: 0.00013434368884190917\n",
      "Iteration 1844/2000, Loss: 8.218218863476068e-05\n",
      "Iteration 1845/2000, Loss: 0.0001649173063924536\n",
      "Iteration 1846/2000, Loss: 0.00018033297965303063\n",
      "Iteration 1847/2000, Loss: 0.00012879383575636894\n",
      "Iteration 1848/2000, Loss: 0.00011535239173099399\n",
      "Iteration 1849/2000, Loss: 0.00010952501179417595\n",
      "Iteration 1850/2000, Loss: 0.0001462124491808936\n",
      "Iteration 1851/2000, Loss: 0.00015673106827307492\n",
      "Iteration 1852/2000, Loss: 0.00011795252794399858\n",
      "Iteration 1853/2000, Loss: 0.00013397808652371168\n",
      "Iteration 1854/2000, Loss: 0.00017028854927048087\n",
      "Iteration 1855/2000, Loss: 0.00011693120177369565\n",
      "Iteration 1856/2000, Loss: 0.00018686744442675263\n",
      "Iteration 1857/2000, Loss: 0.0002394402981735766\n",
      "Iteration 1858/2000, Loss: 9.797135862754658e-05\n",
      "Iteration 1859/2000, Loss: 0.00012810732005164027\n",
      "Iteration 1860/2000, Loss: 0.0003313497581984848\n",
      "Iteration 1861/2000, Loss: 0.0003366521850693971\n",
      "Iteration 1862/2000, Loss: 0.00018295606423635036\n",
      "Iteration 1863/2000, Loss: 0.00014701983309350908\n",
      "Iteration 1864/2000, Loss: 0.0002468809543643147\n",
      "Iteration 1865/2000, Loss: 0.0003289596061222255\n",
      "Iteration 1866/2000, Loss: 0.0001450970594305545\n",
      "Iteration 1867/2000, Loss: 0.00018836591334547848\n",
      "Iteration 1868/2000, Loss: 0.00016426977526862174\n",
      "Iteration 1869/2000, Loss: 0.00017394380120094866\n",
      "Iteration 1870/2000, Loss: 0.00012259873619768769\n",
      "Iteration 1871/2000, Loss: 0.00016985843831207603\n",
      "Iteration 1872/2000, Loss: 0.00017880462110042572\n",
      "Iteration 1873/2000, Loss: 0.00015466487093362957\n",
      "Iteration 1874/2000, Loss: 0.00021637741883751005\n",
      "Iteration 1875/2000, Loss: 0.00011744378571165726\n",
      "Iteration 1876/2000, Loss: 0.00033448109752498567\n",
      "Iteration 1877/2000, Loss: 0.0001280995347769931\n",
      "Iteration 1878/2000, Loss: 0.0001902475778479129\n",
      "Iteration 1879/2000, Loss: 0.00015765595890115947\n",
      "Iteration 1880/2000, Loss: 0.0001741718006087467\n",
      "Iteration 1881/2000, Loss: 0.00012003994197584689\n",
      "Iteration 1882/2000, Loss: 0.0001970476150745526\n",
      "Iteration 1883/2000, Loss: 0.00017262906476389617\n",
      "Iteration 1884/2000, Loss: 0.0001172728298115544\n",
      "Iteration 1885/2000, Loss: 0.0001224962034029886\n",
      "Iteration 1886/2000, Loss: 9.021084406413138e-05\n",
      "Iteration 1887/2000, Loss: 0.00016143992252182215\n",
      "Iteration 1888/2000, Loss: 0.00015177989553194493\n",
      "Iteration 1889/2000, Loss: 0.00012743596744257957\n",
      "Iteration 1890/2000, Loss: 0.0001719942520139739\n",
      "Iteration 1891/2000, Loss: 0.00011990781786153093\n",
      "Iteration 1892/2000, Loss: 0.0001461122592445463\n",
      "Iteration 1893/2000, Loss: 0.00011507983435876667\n",
      "Iteration 1894/2000, Loss: 0.00019216274085920304\n",
      "Iteration 1895/2000, Loss: 0.0003291908069513738\n",
      "Iteration 1896/2000, Loss: 0.00023900599626358598\n",
      "Iteration 1897/2000, Loss: 0.0001895313325803727\n",
      "Iteration 1898/2000, Loss: 0.00029977818485349417\n",
      "Iteration 1899/2000, Loss: 0.00015923284809105098\n",
      "Iteration 1900/2000, Loss: 0.0003094445273745805\n",
      "Iteration 1901/2000, Loss: 0.00019350633374415338\n",
      "Iteration 1902/2000, Loss: 0.0003650685539469123\n",
      "Iteration 1903/2000, Loss: 0.00020779974875040352\n",
      "Iteration 1904/2000, Loss: 0.0003530268731992692\n",
      "Iteration 1905/2000, Loss: 0.00039657429442740977\n",
      "Iteration 1906/2000, Loss: 0.00018481764709576964\n",
      "Iteration 1907/2000, Loss: 0.00034748323378153145\n",
      "Iteration 1908/2000, Loss: 0.00014281309267971665\n",
      "Iteration 1909/2000, Loss: 0.00047959008952602744\n",
      "Iteration 1910/2000, Loss: 0.00022578606149181724\n",
      "Iteration 1911/2000, Loss: 0.00025480700423941016\n",
      "Iteration 1912/2000, Loss: 0.0002443628909531981\n",
      "Iteration 1913/2000, Loss: 0.00015973387053236365\n",
      "Iteration 1914/2000, Loss: 0.00026670066290535033\n",
      "Iteration 1915/2000, Loss: 0.0004246051248628646\n",
      "Iteration 1916/2000, Loss: 0.0003838068514596671\n",
      "Iteration 1917/2000, Loss: 0.00020730264077428728\n",
      "Iteration 1918/2000, Loss: 0.00024360395036637783\n",
      "Iteration 1919/2000, Loss: 0.0007999103399924934\n",
      "Iteration 1920/2000, Loss: 0.0003604356315918267\n",
      "Iteration 1921/2000, Loss: 0.00033823057310655713\n",
      "Iteration 1922/2000, Loss: 0.00025827172794379294\n",
      "Iteration 1923/2000, Loss: 0.00027941318694502115\n",
      "Iteration 1924/2000, Loss: 0.00028180531808175147\n",
      "Iteration 1925/2000, Loss: 0.00020283942285459489\n",
      "Iteration 1926/2000, Loss: 0.00019440308096818626\n",
      "Iteration 1927/2000, Loss: 0.0002669304667506367\n",
      "Iteration 1928/2000, Loss: 0.00015431948122568429\n",
      "Iteration 1929/2000, Loss: 0.00015071283269207925\n",
      "Iteration 1930/2000, Loss: 0.00012494433030951768\n",
      "Iteration 1931/2000, Loss: 0.0001863711659098044\n",
      "Iteration 1932/2000, Loss: 0.0003588648105505854\n",
      "Iteration 1933/2000, Loss: 0.00011619746510405093\n",
      "Iteration 1934/2000, Loss: 0.00024014196242205799\n",
      "Iteration 1935/2000, Loss: 0.00012264036922715604\n",
      "Iteration 1936/2000, Loss: 0.00014547188766300678\n",
      "Iteration 1937/2000, Loss: 0.00020189381029922515\n",
      "Iteration 1938/2000, Loss: 0.0001623894349904731\n",
      "Iteration 1939/2000, Loss: 0.00020509834575932473\n",
      "Iteration 1940/2000, Loss: 0.00021230535639915615\n",
      "Iteration 1941/2000, Loss: 0.00018440732674207538\n",
      "Iteration 1942/2000, Loss: 0.00013230614422354847\n",
      "Iteration 1943/2000, Loss: 9.760838293004781e-05\n",
      "Iteration 1944/2000, Loss: 0.00016889032849576324\n",
      "Iteration 1945/2000, Loss: 0.00014516248484142125\n",
      "Iteration 1946/2000, Loss: 0.0001667376927798614\n",
      "Iteration 1947/2000, Loss: 0.00018672971054911613\n",
      "Iteration 1948/2000, Loss: 0.0002812582824844867\n",
      "Iteration 1949/2000, Loss: 0.0003761190746445209\n",
      "Iteration 1950/2000, Loss: 0.00020559363474603742\n",
      "Iteration 1951/2000, Loss: 0.00020909977320116013\n",
      "Iteration 1952/2000, Loss: 0.0001744931796565652\n",
      "Iteration 1953/2000, Loss: 0.00016765489999670535\n",
      "Iteration 1954/2000, Loss: 0.0001512586313765496\n",
      "Iteration 1955/2000, Loss: 0.00019636258366517723\n",
      "Iteration 1956/2000, Loss: 0.00017103276331909\n",
      "Iteration 1957/2000, Loss: 0.00015062070451676846\n",
      "Iteration 1958/2000, Loss: 0.00017047362052835524\n",
      "Iteration 1959/2000, Loss: 0.00031596969347447157\n",
      "Iteration 1960/2000, Loss: 0.00010318341082893312\n",
      "Iteration 1961/2000, Loss: 0.00023926999710965902\n",
      "Iteration 1962/2000, Loss: 0.00015974021516740322\n",
      "Iteration 1963/2000, Loss: 0.00021458842093124986\n",
      "Iteration 1964/2000, Loss: 0.00015333537885453552\n",
      "Iteration 1965/2000, Loss: 0.00014671837561763823\n",
      "Iteration 1966/2000, Loss: 0.00016939280612859875\n",
      "Iteration 1967/2000, Loss: 0.0001444641820853576\n",
      "Iteration 1968/2000, Loss: 0.00013799156295135617\n",
      "Iteration 1969/2000, Loss: 0.00015094199625309557\n",
      "Iteration 1970/2000, Loss: 0.00018466476467438042\n",
      "Iteration 1971/2000, Loss: 0.00013998703798279166\n",
      "Iteration 1972/2000, Loss: 0.00033569286460988224\n",
      "Iteration 1973/2000, Loss: 0.0001488770794821903\n",
      "Iteration 1974/2000, Loss: 0.00021082580497022718\n",
      "Iteration 1975/2000, Loss: 0.00013495409802999347\n",
      "Iteration 1976/2000, Loss: 0.00012944787158630788\n",
      "Iteration 1977/2000, Loss: 0.00022631423780694604\n",
      "Iteration 1978/2000, Loss: 0.0001598096132511273\n",
      "Iteration 1979/2000, Loss: 0.000255368126090616\n",
      "Iteration 1980/2000, Loss: 0.00020460803352762014\n",
      "Iteration 1981/2000, Loss: 0.00017567658505868167\n",
      "Iteration 1982/2000, Loss: 0.00013368351210374385\n",
      "Iteration 1983/2000, Loss: 0.00022350135259330273\n",
      "Iteration 1984/2000, Loss: 0.00011932478082599118\n",
      "Iteration 1985/2000, Loss: 0.0001753552205627784\n",
      "Iteration 1986/2000, Loss: 0.0001205320586450398\n",
      "Iteration 1987/2000, Loss: 0.00020496774232015014\n",
      "Iteration 1988/2000, Loss: 0.00015075350529514253\n",
      "Iteration 1989/2000, Loss: 0.00016407258226536214\n",
      "Iteration 1990/2000, Loss: 9.434141975361854e-05\n",
      "Iteration 1991/2000, Loss: 0.00018347802688367665\n",
      "Iteration 1992/2000, Loss: 0.0002696010051295161\n",
      "Iteration 1993/2000, Loss: 9.55418508965522e-05\n",
      "Iteration 1994/2000, Loss: 0.00010800774180097505\n",
      "Iteration 1995/2000, Loss: 0.00026514672208577394\n",
      "Iteration 1996/2000, Loss: 0.00014916200598236173\n",
      "Iteration 1997/2000, Loss: 0.00023147249885369092\n",
      "Iteration 1998/2000, Loss: 0.00012718500511255115\n",
      "Iteration 1999/2000, Loss: 0.000161190633662045\n",
      "Iteration 2000/2000, Loss: 0.00016040002810768783\n",
      "Model weights saved after training and testing with linewidth 200000.0 Hz.\n",
      "\n",
      "\n",
      "Testing with linewidth: 200000.0 Hz and Distance: 1000.0 km\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Testing MSE - Linewidth: 200000.0, Link Distance: 1000.0, Original: 0.005653996951878071, Neural Network: 0.0011740245390683413\n",
      "\n",
      "Training with linewidth: 200000.0 Hz and Distance: 2000.0 km\n",
      "Iteration 1/2000, Loss: 0.00015667101251892745\n",
      "Iteration 2/2000, Loss: 0.00033014555810950696\n",
      "Iteration 3/2000, Loss: 0.00020676104759331793\n",
      "Iteration 4/2000, Loss: 0.00019149987201672047\n",
      "Iteration 5/2000, Loss: 0.00022402353351935744\n",
      "Iteration 6/2000, Loss: 0.00019262880960013717\n",
      "Iteration 7/2000, Loss: 0.00019997888011857867\n",
      "Iteration 8/2000, Loss: 0.00014373415615409613\n",
      "Iteration 9/2000, Loss: 0.00021305966947693378\n",
      "Iteration 10/2000, Loss: 0.00015997480659279972\n",
      "Iteration 11/2000, Loss: 0.0002420214150333777\n",
      "Iteration 12/2000, Loss: 0.000429073377745226\n",
      "Iteration 13/2000, Loss: 0.00019885861547663808\n",
      "Iteration 14/2000, Loss: 0.0002987784973811358\n",
      "Iteration 15/2000, Loss: 0.0002737765898928046\n",
      "Iteration 16/2000, Loss: 0.00017064143321476877\n",
      "Iteration 17/2000, Loss: 0.0005283354548737407\n",
      "Iteration 18/2000, Loss: 0.0002904050634242594\n",
      "Iteration 19/2000, Loss: 0.0003244302642997354\n",
      "Iteration 20/2000, Loss: 0.00032157660461962223\n",
      "Iteration 21/2000, Loss: 0.00017876195488497615\n",
      "Iteration 22/2000, Loss: 0.000340556085575372\n",
      "Iteration 23/2000, Loss: 0.000272102392045781\n",
      "Iteration 24/2000, Loss: 0.00021083220781292766\n",
      "Iteration 25/2000, Loss: 0.00018834978982340544\n",
      "Iteration 26/2000, Loss: 8.759037154959515e-05\n",
      "Iteration 27/2000, Loss: 0.000235324478126131\n",
      "Iteration 28/2000, Loss: 0.0001798543380573392\n",
      "Iteration 29/2000, Loss: 0.0003856817784253508\n",
      "Iteration 30/2000, Loss: 0.0002545046736486256\n",
      "Iteration 31/2000, Loss: 0.00021227842080406845\n",
      "Iteration 32/2000, Loss: 0.00024367895093746483\n",
      "Iteration 33/2000, Loss: 0.00016880877956282347\n",
      "Iteration 34/2000, Loss: 0.00021679377823602408\n",
      "Iteration 35/2000, Loss: 0.0001410958357155323\n",
      "Iteration 36/2000, Loss: 0.00031503624632023275\n",
      "Iteration 37/2000, Loss: 0.00023807663819752634\n",
      "Iteration 38/2000, Loss: 0.00017841244698502123\n",
      "Iteration 39/2000, Loss: 0.0003020742442458868\n",
      "Iteration 40/2000, Loss: 0.0001844283688114956\n",
      "Iteration 41/2000, Loss: 0.00029386021196842194\n",
      "Iteration 42/2000, Loss: 0.00029309254023246467\n",
      "Iteration 43/2000, Loss: 0.0002587436174508184\n",
      "Iteration 44/2000, Loss: 0.0005303545040078461\n",
      "Iteration 45/2000, Loss: 0.00021910551004111767\n",
      "Iteration 46/2000, Loss: 0.00033551763044670224\n",
      "Iteration 47/2000, Loss: 0.00010751342779258266\n",
      "Iteration 48/2000, Loss: 0.0006719433004036546\n",
      "Iteration 49/2000, Loss: 0.00043270274181850255\n",
      "Iteration 50/2000, Loss: 0.00028130246209912\n",
      "Iteration 51/2000, Loss: 0.00047185600851662457\n",
      "Iteration 52/2000, Loss: 0.00016212265472859144\n",
      "Iteration 53/2000, Loss: 0.00032023643143475056\n",
      "Iteration 54/2000, Loss: 0.0001158259401563555\n",
      "Iteration 55/2000, Loss: 0.0005051036714576185\n",
      "Iteration 56/2000, Loss: 0.00031103467335924506\n",
      "Iteration 57/2000, Loss: 0.00031969856354407966\n",
      "Iteration 58/2000, Loss: 0.00044158505625091493\n",
      "Iteration 59/2000, Loss: 0.00018981966422870755\n",
      "Iteration 60/2000, Loss: 0.00044428653200156987\n",
      "Iteration 61/2000, Loss: 0.0002645463973749429\n",
      "Iteration 62/2000, Loss: 0.0003329977043904364\n",
      "Iteration 63/2000, Loss: 0.0005966226453892887\n",
      "Iteration 64/2000, Loss: 0.00013797533756587654\n",
      "Iteration 65/2000, Loss: 0.0003875419788528234\n",
      "Iteration 66/2000, Loss: 0.00010535173350945115\n",
      "Iteration 67/2000, Loss: 0.0004613508644979447\n",
      "Iteration 68/2000, Loss: 0.0005647614016197622\n",
      "Iteration 69/2000, Loss: 0.0003325498546473682\n",
      "Iteration 70/2000, Loss: 0.0003500532766338438\n",
      "Iteration 71/2000, Loss: 0.00012189836706966162\n",
      "Iteration 72/2000, Loss: 0.0002260642359033227\n",
      "Iteration 73/2000, Loss: 9.56555813900195e-05\n",
      "Iteration 74/2000, Loss: 0.0004418330790940672\n",
      "Iteration 75/2000, Loss: 0.00016689309268258512\n",
      "Iteration 76/2000, Loss: 0.00028738914988934994\n",
      "Iteration 77/2000, Loss: 0.000269043433945626\n",
      "Iteration 78/2000, Loss: 0.0002241375477751717\n",
      "Iteration 79/2000, Loss: 0.00012786223669536412\n",
      "Iteration 80/2000, Loss: 0.0003937804722227156\n",
      "Iteration 81/2000, Loss: 0.00046363152796402574\n",
      "Iteration 82/2000, Loss: 0.00014211911184247583\n",
      "Iteration 83/2000, Loss: 0.00022415199782699347\n",
      "Iteration 84/2000, Loss: 0.0001730547664919868\n",
      "Iteration 85/2000, Loss: 0.0006526875076815486\n",
      "Iteration 86/2000, Loss: 0.00019592675380408764\n",
      "Iteration 87/2000, Loss: 0.00013449485413730145\n",
      "Iteration 88/2000, Loss: 0.00022297936084214598\n",
      "Iteration 89/2000, Loss: 0.00011731669656001031\n",
      "Iteration 90/2000, Loss: 0.00018839380936697125\n",
      "Iteration 91/2000, Loss: 0.00012150596739957109\n",
      "Iteration 92/2000, Loss: 0.0002570097567513585\n",
      "Iteration 93/2000, Loss: 0.00031638285145163536\n",
      "Iteration 94/2000, Loss: 0.000421118747908622\n",
      "Iteration 95/2000, Loss: 0.0002777750778477639\n",
      "Iteration 96/2000, Loss: 0.0002344712702324614\n",
      "Iteration 97/2000, Loss: 0.0003294192429166287\n",
      "Iteration 98/2000, Loss: 0.0002335524040972814\n",
      "Iteration 99/2000, Loss: 0.000454256049124524\n",
      "Iteration 100/2000, Loss: 0.0003959174791816622\n",
      "Iteration 101/2000, Loss: 0.0004411052796058357\n",
      "Iteration 102/2000, Loss: 0.000256120169069618\n",
      "Iteration 103/2000, Loss: 0.00021605973597615957\n",
      "Iteration 104/2000, Loss: 0.000230626726988703\n",
      "Iteration 105/2000, Loss: 0.00013100437354296446\n",
      "Iteration 106/2000, Loss: 0.0005565956817008555\n",
      "Iteration 107/2000, Loss: 0.00026556968805380166\n",
      "Iteration 108/2000, Loss: 0.00011842660751426592\n",
      "Iteration 109/2000, Loss: 0.00016428589879069477\n",
      "Iteration 110/2000, Loss: 0.00028048831154592335\n",
      "Iteration 111/2000, Loss: 0.00015575860743410885\n",
      "Iteration 112/2000, Loss: 0.00016103255620691925\n",
      "Iteration 113/2000, Loss: 0.000396035989979282\n",
      "Iteration 114/2000, Loss: 0.00019724728190340102\n",
      "Iteration 115/2000, Loss: 0.00021918039419688284\n",
      "Iteration 116/2000, Loss: 0.00014987864415161312\n",
      "Iteration 117/2000, Loss: 0.00026818757760338485\n",
      "Iteration 118/2000, Loss: 0.00017743204080034047\n",
      "Iteration 119/2000, Loss: 0.0004243561706971377\n",
      "Iteration 120/2000, Loss: 0.00025096439640037715\n",
      "Iteration 121/2000, Loss: 0.00022896580048836768\n",
      "Iteration 122/2000, Loss: 0.00030939083080738783\n",
      "Iteration 123/2000, Loss: 0.00019816186977550387\n",
      "Iteration 124/2000, Loss: 0.00022128048294689506\n",
      "Iteration 125/2000, Loss: 0.0002469229802954942\n",
      "Iteration 126/2000, Loss: 0.00028763888985849917\n",
      "Iteration 127/2000, Loss: 0.00018917037232313305\n",
      "Iteration 128/2000, Loss: 0.000292758661089465\n",
      "Iteration 129/2000, Loss: 0.0002173463290091604\n",
      "Iteration 130/2000, Loss: 0.00047070413711480796\n",
      "Iteration 131/2000, Loss: 0.0002155923139071092\n",
      "Iteration 132/2000, Loss: 0.00022432081459555775\n",
      "Iteration 133/2000, Loss: 0.00022833285038359463\n",
      "Iteration 134/2000, Loss: 0.0006366947200149298\n",
      "Iteration 135/2000, Loss: 0.00028852251125499606\n",
      "Iteration 136/2000, Loss: 0.0002168087667087093\n",
      "Iteration 137/2000, Loss: 0.00023622348089702427\n",
      "Iteration 138/2000, Loss: 0.0003011516237165779\n",
      "Iteration 139/2000, Loss: 0.0003886655904352665\n",
      "Iteration 140/2000, Loss: 0.0002070054761134088\n",
      "Iteration 141/2000, Loss: 0.00013766098709311336\n",
      "Iteration 142/2000, Loss: 0.00018815889779943973\n",
      "Iteration 143/2000, Loss: 0.00033207429805770516\n",
      "Iteration 144/2000, Loss: 0.00016893084102775902\n",
      "Iteration 145/2000, Loss: 0.0001858564792200923\n",
      "Iteration 146/2000, Loss: 0.0001516945194453001\n",
      "Iteration 147/2000, Loss: 0.0002078401594189927\n",
      "Iteration 148/2000, Loss: 0.00018486721091903746\n",
      "Iteration 149/2000, Loss: 0.00013440012116916478\n",
      "Iteration 150/2000, Loss: 0.00023669080110266805\n",
      "Iteration 151/2000, Loss: 0.0001535239425720647\n",
      "Iteration 152/2000, Loss: 0.00022397460998035967\n",
      "Iteration 153/2000, Loss: 0.0001828512758947909\n",
      "Iteration 154/2000, Loss: 0.00021641526836901903\n",
      "Iteration 155/2000, Loss: 0.0002816852938849479\n",
      "Iteration 156/2000, Loss: 0.0004895469755865633\n",
      "Iteration 157/2000, Loss: 0.00015362196427304298\n",
      "Iteration 158/2000, Loss: 0.00028419241425581276\n",
      "Iteration 159/2000, Loss: 0.00025582389207556844\n",
      "Iteration 160/2000, Loss: 0.0003505103231873363\n",
      "Iteration 161/2000, Loss: 0.00022645757417194545\n",
      "Iteration 162/2000, Loss: 0.00021677529730368406\n",
      "Iteration 163/2000, Loss: 0.00042143897735513747\n",
      "Iteration 164/2000, Loss: 0.0002635107084643096\n",
      "Iteration 165/2000, Loss: 0.00021362128609325737\n",
      "Iteration 166/2000, Loss: 0.00013346249761525542\n",
      "Iteration 167/2000, Loss: 0.00021111502428539097\n",
      "Iteration 168/2000, Loss: 0.00017322211351711303\n",
      "Iteration 169/2000, Loss: 0.00017761070921551436\n",
      "Iteration 170/2000, Loss: 0.00028795062098652124\n",
      "Iteration 171/2000, Loss: 0.00021945583284832537\n",
      "Iteration 172/2000, Loss: 0.000366920925443992\n",
      "Iteration 173/2000, Loss: 0.0001777327706804499\n",
      "Iteration 174/2000, Loss: 0.00018389053002465516\n",
      "Iteration 175/2000, Loss: 0.0001819756580516696\n",
      "Iteration 176/2000, Loss: 0.00017271535762120038\n",
      "Iteration 177/2000, Loss: 0.00016719555424060673\n",
      "Iteration 178/2000, Loss: 0.00026545391301624477\n",
      "Iteration 179/2000, Loss: 0.00018224670202471316\n",
      "Iteration 180/2000, Loss: 0.00020623079035431147\n",
      "Iteration 181/2000, Loss: 0.0002525275922380388\n",
      "Iteration 182/2000, Loss: 0.00021427021420095116\n",
      "Iteration 183/2000, Loss: 0.0006005933973938227\n",
      "Iteration 184/2000, Loss: 0.0001803581544663757\n",
      "Iteration 185/2000, Loss: 0.0004802912299055606\n",
      "Iteration 186/2000, Loss: 0.00017425589612685144\n",
      "Iteration 187/2000, Loss: 0.00022904339130036533\n",
      "Iteration 188/2000, Loss: 0.00024422627757303417\n",
      "Iteration 189/2000, Loss: 0.0003074778651352972\n",
      "Iteration 190/2000, Loss: 0.0001939158682944253\n",
      "Iteration 191/2000, Loss: 0.0002076770761050284\n",
      "Iteration 192/2000, Loss: 0.0002548092161305249\n",
      "Iteration 193/2000, Loss: 0.00017128579202108085\n",
      "Iteration 194/2000, Loss: 0.0005167280905880034\n",
      "Iteration 195/2000, Loss: 0.00020200999279040843\n",
      "Iteration 196/2000, Loss: 0.00034654230694286525\n",
      "Iteration 197/2000, Loss: 0.0004114533367101103\n",
      "Iteration 198/2000, Loss: 0.00025308565818704665\n",
      "Iteration 199/2000, Loss: 0.000257321895333007\n",
      "Iteration 200/2000, Loss: 0.0002154776157112792\n",
      "Iteration 201/2000, Loss: 0.0002305869129486382\n",
      "Iteration 202/2000, Loss: 0.00012789436732418835\n",
      "Iteration 203/2000, Loss: 0.00032550888136029243\n",
      "Iteration 204/2000, Loss: 0.00019820549641735852\n",
      "Iteration 205/2000, Loss: 0.0002496330125723034\n",
      "Iteration 206/2000, Loss: 0.00023033058096189052\n",
      "Iteration 207/2000, Loss: 0.0004139116208534688\n",
      "Iteration 208/2000, Loss: 0.0002659205056261271\n",
      "Iteration 209/2000, Loss: 0.0002240399771835655\n",
      "Iteration 210/2000, Loss: 0.0004826330696232617\n",
      "Iteration 211/2000, Loss: 0.00019257476378697902\n",
      "Iteration 212/2000, Loss: 0.00024006872263271362\n",
      "Iteration 213/2000, Loss: 0.00035463128006085753\n",
      "Iteration 214/2000, Loss: 0.0002210004604421556\n",
      "Iteration 215/2000, Loss: 0.0006283168913796544\n",
      "Iteration 216/2000, Loss: 0.00037964090006425977\n",
      "Iteration 217/2000, Loss: 0.0002070011687465012\n",
      "Iteration 218/2000, Loss: 0.00017228868091478944\n",
      "Iteration 219/2000, Loss: 0.00018367700977250934\n",
      "Iteration 220/2000, Loss: 0.00034503592178225517\n",
      "Iteration 221/2000, Loss: 0.00029211636865511537\n",
      "Iteration 222/2000, Loss: 0.00040425738552585244\n",
      "Iteration 223/2000, Loss: 0.00020670697267632931\n",
      "Iteration 224/2000, Loss: 0.00035935008781962097\n",
      "Iteration 225/2000, Loss: 0.0003158456820528954\n",
      "Iteration 226/2000, Loss: 0.00019973704183939844\n",
      "Iteration 227/2000, Loss: 0.000383260688977316\n",
      "Iteration 228/2000, Loss: 0.00021647653193213046\n",
      "Iteration 229/2000, Loss: 0.00020997914543841034\n",
      "Iteration 230/2000, Loss: 0.0003191942232660949\n",
      "Iteration 231/2000, Loss: 0.00010095505422214046\n",
      "Iteration 232/2000, Loss: 0.0002411931345704943\n",
      "Iteration 233/2000, Loss: 0.00029891010490246117\n",
      "Iteration 234/2000, Loss: 0.00015693528985138983\n",
      "Iteration 235/2000, Loss: 0.00022751183132641017\n",
      "Iteration 236/2000, Loss: 0.00012435304233804345\n",
      "Iteration 237/2000, Loss: 0.000233140992349945\n",
      "Iteration 238/2000, Loss: 0.00027974226395599544\n",
      "Iteration 239/2000, Loss: 0.00018092298705596477\n",
      "Iteration 240/2000, Loss: 0.00019941564823966473\n",
      "Iteration 241/2000, Loss: 0.00023461355885956436\n",
      "Iteration 242/2000, Loss: 0.0001544102415209636\n",
      "Iteration 243/2000, Loss: 0.0002313182776561007\n",
      "Iteration 244/2000, Loss: 0.00020432588644325733\n",
      "Iteration 245/2000, Loss: 0.00018029716738965362\n",
      "Iteration 246/2000, Loss: 0.00022690767946187407\n",
      "Iteration 247/2000, Loss: 0.00018554652342572808\n",
      "Iteration 248/2000, Loss: 0.00014584025484509766\n",
      "Iteration 249/2000, Loss: 0.0002838679647538811\n",
      "Iteration 250/2000, Loss: 0.00023541873088106513\n",
      "Iteration 251/2000, Loss: 0.00039118947461247444\n",
      "Iteration 252/2000, Loss: 0.00021443009609356523\n",
      "Iteration 253/2000, Loss: 0.00015478229033760726\n",
      "Iteration 254/2000, Loss: 0.00018950350931845605\n",
      "Iteration 255/2000, Loss: 0.00015419967530760914\n",
      "Iteration 256/2000, Loss: 0.000310539297061041\n",
      "Iteration 257/2000, Loss: 0.00010976078192470595\n",
      "Iteration 258/2000, Loss: 0.0001716568658594042\n",
      "Iteration 259/2000, Loss: 0.00014709368406329304\n",
      "Iteration 260/2000, Loss: 0.00020122065325267613\n",
      "Iteration 261/2000, Loss: 0.00015792256454005837\n",
      "Iteration 262/2000, Loss: 0.0001147528673755005\n",
      "Iteration 263/2000, Loss: 0.00016123084060382098\n",
      "Iteration 264/2000, Loss: 0.0001294709654757753\n",
      "Iteration 265/2000, Loss: 0.00016731502546463162\n",
      "Iteration 266/2000, Loss: 0.00020625109027605504\n",
      "Iteration 267/2000, Loss: 0.00026390404673293233\n",
      "Iteration 268/2000, Loss: 0.000247121904976666\n",
      "Iteration 269/2000, Loss: 0.00016298268747050315\n",
      "Iteration 270/2000, Loss: 0.00011833570169983432\n",
      "Iteration 271/2000, Loss: 0.0002730086271185428\n",
      "Iteration 272/2000, Loss: 0.0001448207040084526\n",
      "Iteration 273/2000, Loss: 0.00017092320194933563\n",
      "Iteration 274/2000, Loss: 0.0002891284239012748\n",
      "Iteration 275/2000, Loss: 0.00015102709585335106\n",
      "Iteration 276/2000, Loss: 0.00031936305458657444\n",
      "Iteration 277/2000, Loss: 0.00022624514531344175\n",
      "Iteration 278/2000, Loss: 0.00034186459379270673\n",
      "Iteration 279/2000, Loss: 0.00015299220103770494\n",
      "Iteration 280/2000, Loss: 0.00013353467511478812\n",
      "Iteration 281/2000, Loss: 0.00026286544743925333\n",
      "Iteration 282/2000, Loss: 0.00018538712174631655\n",
      "Iteration 283/2000, Loss: 0.00014710918185301125\n",
      "Iteration 284/2000, Loss: 0.00016770133515819907\n",
      "Iteration 285/2000, Loss: 0.00024055886024143547\n",
      "Iteration 286/2000, Loss: 0.0001818359742173925\n",
      "Iteration 287/2000, Loss: 0.0002613671531435102\n",
      "Iteration 288/2000, Loss: 0.00017779631889425218\n",
      "Iteration 289/2000, Loss: 0.00016336565022356808\n",
      "Iteration 290/2000, Loss: 0.000238904744037427\n",
      "Iteration 291/2000, Loss: 0.0003104871138930321\n",
      "Iteration 292/2000, Loss: 0.0002774111635517329\n",
      "Iteration 293/2000, Loss: 0.00033825638820417225\n",
      "Iteration 294/2000, Loss: 0.00023383655934594572\n",
      "Iteration 295/2000, Loss: 0.00016722780128475279\n",
      "Iteration 296/2000, Loss: 0.0002754503220785409\n",
      "Iteration 297/2000, Loss: 0.00015742248797323555\n",
      "Iteration 298/2000, Loss: 0.0002709091641008854\n",
      "Iteration 299/2000, Loss: 0.00026075495406985283\n",
      "Iteration 300/2000, Loss: 0.0001922679803101346\n",
      "Iteration 301/2000, Loss: 0.0002609085931908339\n",
      "Iteration 302/2000, Loss: 0.00017112470231950283\n",
      "Iteration 303/2000, Loss: 0.0003317898663226515\n",
      "Iteration 304/2000, Loss: 0.00020120634872000664\n",
      "Iteration 305/2000, Loss: 0.00018007479957304895\n",
      "Iteration 306/2000, Loss: 0.00032855948666110635\n",
      "Iteration 307/2000, Loss: 0.00027324739494360983\n",
      "Iteration 308/2000, Loss: 0.00032282265601679683\n",
      "Iteration 309/2000, Loss: 0.00022802736202720553\n",
      "Iteration 310/2000, Loss: 0.0003059983719140291\n",
      "Iteration 311/2000, Loss: 0.00020343170035630465\n",
      "Iteration 312/2000, Loss: 0.00013623469567392021\n",
      "Iteration 313/2000, Loss: 0.00028856148128397763\n",
      "Iteration 314/2000, Loss: 0.00019136305490974337\n",
      "Iteration 315/2000, Loss: 0.00011942751734750345\n",
      "Iteration 316/2000, Loss: 0.00010966896661557257\n",
      "Iteration 317/2000, Loss: 0.00020766997477039695\n",
      "Iteration 318/2000, Loss: 0.00016623837291263044\n",
      "Iteration 319/2000, Loss: 0.00012916646664962173\n",
      "Iteration 320/2000, Loss: 0.0001493810850661248\n",
      "Iteration 321/2000, Loss: 0.0001763569744070992\n",
      "Iteration 322/2000, Loss: 0.000157604314154014\n",
      "Iteration 323/2000, Loss: 0.00018482661107555032\n",
      "Iteration 324/2000, Loss: 0.00012311246246099472\n",
      "Iteration 325/2000, Loss: 0.00016227814194280654\n",
      "Iteration 326/2000, Loss: 0.00020548110478557646\n",
      "Iteration 327/2000, Loss: 0.00011390712461434305\n",
      "Iteration 328/2000, Loss: 0.0002683393831830472\n",
      "Iteration 329/2000, Loss: 0.0002725471858866513\n",
      "Iteration 330/2000, Loss: 0.00016809949011076242\n",
      "Iteration 331/2000, Loss: 0.00019116574549116194\n",
      "Iteration 332/2000, Loss: 0.00021625609952025115\n",
      "Iteration 333/2000, Loss: 0.0006301989778876305\n",
      "Iteration 334/2000, Loss: 0.0002813714381773025\n",
      "Iteration 335/2000, Loss: 0.0003549749089870602\n",
      "Iteration 336/2000, Loss: 0.0001518626813776791\n",
      "Iteration 337/2000, Loss: 0.00017313689750153571\n",
      "Iteration 338/2000, Loss: 0.00023764517391100526\n",
      "Iteration 339/2000, Loss: 0.00017119049152825028\n",
      "Iteration 340/2000, Loss: 0.00017137380200438201\n",
      "Iteration 341/2000, Loss: 0.00014423386892303824\n",
      "Iteration 342/2000, Loss: 0.0001127423529396765\n",
      "Iteration 343/2000, Loss: 0.0001928081619553268\n",
      "Iteration 344/2000, Loss: 0.00021543116599787027\n",
      "Iteration 345/2000, Loss: 0.00026917108334600925\n",
      "Iteration 346/2000, Loss: 0.00014456779172178358\n",
      "Iteration 347/2000, Loss: 0.00022029608953744173\n",
      "Iteration 348/2000, Loss: 0.0002592991222627461\n",
      "Iteration 349/2000, Loss: 0.00019322561274748296\n",
      "Iteration 350/2000, Loss: 0.00030303822131827474\n",
      "Iteration 351/2000, Loss: 0.00023422758386004716\n",
      "Iteration 352/2000, Loss: 0.0003669451689347625\n",
      "Iteration 353/2000, Loss: 0.00015075481496751308\n",
      "Iteration 354/2000, Loss: 0.00025181201635859907\n",
      "Iteration 355/2000, Loss: 0.00023808583500795066\n",
      "Iteration 356/2000, Loss: 0.0002176999842049554\n",
      "Iteration 357/2000, Loss: 0.0002567626070231199\n",
      "Iteration 358/2000, Loss: 0.00013621905236504972\n",
      "Iteration 359/2000, Loss: 0.00024235884484369308\n",
      "Iteration 360/2000, Loss: 0.00018697605992201716\n",
      "Iteration 361/2000, Loss: 0.0001944889227161184\n",
      "Iteration 362/2000, Loss: 0.00021451120846904814\n",
      "Iteration 363/2000, Loss: 0.0004253328370396048\n",
      "Iteration 364/2000, Loss: 0.00017798622138798237\n",
      "Iteration 365/2000, Loss: 0.0001249924534931779\n",
      "Iteration 366/2000, Loss: 0.00025753770023584366\n",
      "Iteration 367/2000, Loss: 0.00014714396093040705\n",
      "Iteration 368/2000, Loss: 0.00017866355483420193\n",
      "Iteration 369/2000, Loss: 0.00015503742906730622\n",
      "Iteration 370/2000, Loss: 0.0002516010426916182\n",
      "Iteration 371/2000, Loss: 0.00015288077702280134\n",
      "Iteration 372/2000, Loss: 0.00013442960334941745\n",
      "Iteration 373/2000, Loss: 0.00025052973069250584\n",
      "Iteration 374/2000, Loss: 0.00014277291484177113\n",
      "Iteration 375/2000, Loss: 0.0002032405318459496\n",
      "Iteration 376/2000, Loss: 0.00020962185226380825\n",
      "Iteration 377/2000, Loss: 0.00016886975208763033\n",
      "Iteration 378/2000, Loss: 0.00013307854533195496\n",
      "Iteration 379/2000, Loss: 0.00014930810721125454\n",
      "Iteration 380/2000, Loss: 0.0001708244817564264\n",
      "Iteration 381/2000, Loss: 0.0002148252970073372\n",
      "Iteration 382/2000, Loss: 0.00014637369895353913\n",
      "Iteration 383/2000, Loss: 0.00018510184600017965\n",
      "Iteration 384/2000, Loss: 0.00010627655865391716\n",
      "Iteration 385/2000, Loss: 0.0001655640808166936\n",
      "Iteration 386/2000, Loss: 0.00016326936020050198\n",
      "Iteration 387/2000, Loss: 0.0001259625278180465\n",
      "Iteration 388/2000, Loss: 0.0001949769357452169\n",
      "Iteration 389/2000, Loss: 0.00010805440979311243\n",
      "Iteration 390/2000, Loss: 0.0001789998059393838\n",
      "Iteration 391/2000, Loss: 0.00013253903307486326\n",
      "Iteration 392/2000, Loss: 0.0001971202582353726\n",
      "Iteration 393/2000, Loss: 0.00015298649668693542\n",
      "Iteration 394/2000, Loss: 0.00016371453239116818\n",
      "Iteration 395/2000, Loss: 0.00019946947577409446\n",
      "Iteration 396/2000, Loss: 0.0001351324317511171\n",
      "Iteration 397/2000, Loss: 0.0004328340000938624\n",
      "Iteration 398/2000, Loss: 0.0001550570596009493\n",
      "Iteration 399/2000, Loss: 0.00020209622744005173\n",
      "Iteration 400/2000, Loss: 0.00014468526933342218\n",
      "Iteration 401/2000, Loss: 0.0002291118580615148\n",
      "Iteration 402/2000, Loss: 0.00030178006272763014\n",
      "Iteration 403/2000, Loss: 0.0001242377475136891\n",
      "Iteration 404/2000, Loss: 0.0002682574850041419\n",
      "Iteration 405/2000, Loss: 0.0001413382706232369\n",
      "Iteration 406/2000, Loss: 0.0001883342192741111\n",
      "Iteration 407/2000, Loss: 0.00017146518803201616\n",
      "Iteration 408/2000, Loss: 0.0002306951064383611\n",
      "Iteration 409/2000, Loss: 0.00015485686890315264\n",
      "Iteration 410/2000, Loss: 0.00011134828673675656\n",
      "Iteration 411/2000, Loss: 0.00016514670278411359\n",
      "Iteration 412/2000, Loss: 0.00016442193009424955\n",
      "Iteration 413/2000, Loss: 0.00022787859779782593\n",
      "Iteration 414/2000, Loss: 0.000162803364219144\n",
      "Iteration 415/2000, Loss: 0.0003776141966227442\n",
      "Iteration 416/2000, Loss: 0.00018025828467216343\n",
      "Iteration 417/2000, Loss: 0.00016887192032299936\n",
      "Iteration 418/2000, Loss: 0.0001165207359008491\n",
      "Iteration 419/2000, Loss: 0.00019185546261724085\n",
      "Iteration 420/2000, Loss: 0.00015708053251728415\n",
      "Iteration 421/2000, Loss: 0.0002447951410431415\n",
      "Iteration 422/2000, Loss: 0.00023768542450852692\n",
      "Iteration 423/2000, Loss: 0.0003601530916057527\n",
      "Iteration 424/2000, Loss: 0.0002232341794297099\n",
      "Iteration 425/2000, Loss: 0.0006301242392510176\n",
      "Iteration 426/2000, Loss: 0.0002723892102949321\n",
      "Iteration 427/2000, Loss: 0.0002198768634116277\n",
      "Iteration 428/2000, Loss: 0.00020951723854523152\n",
      "Iteration 429/2000, Loss: 0.0001374503190163523\n",
      "Iteration 430/2000, Loss: 0.0002060214028460905\n",
      "Iteration 431/2000, Loss: 0.000581692554987967\n",
      "Iteration 432/2000, Loss: 0.0001982525282073766\n",
      "Iteration 433/2000, Loss: 0.00033911789068952203\n",
      "Iteration 434/2000, Loss: 0.00031644600676372647\n",
      "Iteration 435/2000, Loss: 0.00021022670262027532\n",
      "Iteration 436/2000, Loss: 0.00030903134029358625\n",
      "Iteration 437/2000, Loss: 0.000136059446958825\n",
      "Iteration 438/2000, Loss: 0.0003304092970211059\n",
      "Iteration 439/2000, Loss: 0.00031460117315873504\n",
      "Iteration 440/2000, Loss: 0.00019070216512773186\n",
      "Iteration 441/2000, Loss: 0.0002095761738019064\n",
      "Iteration 442/2000, Loss: 0.0003961524344049394\n",
      "Iteration 443/2000, Loss: 0.00019626259745564312\n",
      "Iteration 444/2000, Loss: 0.00016968295676633716\n",
      "Iteration 445/2000, Loss: 0.00013749308709520847\n",
      "Iteration 446/2000, Loss: 0.0003213955496903509\n",
      "Iteration 447/2000, Loss: 0.00013947539264336228\n",
      "Iteration 448/2000, Loss: 0.00017392744484823197\n",
      "Iteration 449/2000, Loss: 0.00022541398357134312\n",
      "Iteration 450/2000, Loss: 0.0002612379321362823\n",
      "Iteration 451/2000, Loss: 0.0002181278687203303\n",
      "Iteration 452/2000, Loss: 0.00018889131024479866\n",
      "Iteration 453/2000, Loss: 0.0001867053215391934\n",
      "Iteration 454/2000, Loss: 0.00011698957678163424\n",
      "Iteration 455/2000, Loss: 0.0002797938941512257\n",
      "Iteration 456/2000, Loss: 0.0001631868799449876\n",
      "Iteration 457/2000, Loss: 0.00021927771740593016\n",
      "Iteration 458/2000, Loss: 0.0001468426053179428\n",
      "Iteration 459/2000, Loss: 0.00014962140994612128\n",
      "Iteration 460/2000, Loss: 0.00018164367065764964\n",
      "Iteration 461/2000, Loss: 0.00022465472284238786\n",
      "Iteration 462/2000, Loss: 0.00013250639312900603\n",
      "Iteration 463/2000, Loss: 0.00013688142644241452\n",
      "Iteration 464/2000, Loss: 0.0001786560460459441\n",
      "Iteration 465/2000, Loss: 0.00030709270504303277\n",
      "Iteration 466/2000, Loss: 0.00017428833234589547\n",
      "Iteration 467/2000, Loss: 0.00021711182489525527\n",
      "Iteration 468/2000, Loss: 0.00015424213779624552\n",
      "Iteration 469/2000, Loss: 0.00015475564578082412\n",
      "Iteration 470/2000, Loss: 0.00037024475750513375\n",
      "Iteration 471/2000, Loss: 0.00018396260566078126\n",
      "Iteration 472/2000, Loss: 0.00013698214024771005\n",
      "Iteration 473/2000, Loss: 0.0003957342996727675\n",
      "Iteration 474/2000, Loss: 0.0002541860449127853\n",
      "Iteration 475/2000, Loss: 0.00020034148474223912\n",
      "Iteration 476/2000, Loss: 0.00014547674800269306\n",
      "Iteration 477/2000, Loss: 0.00018898402049671859\n",
      "Iteration 478/2000, Loss: 0.0002560129505582154\n",
      "Iteration 479/2000, Loss: 0.00021410138288047165\n",
      "Iteration 480/2000, Loss: 0.00018230882415082306\n",
      "Iteration 481/2000, Loss: 0.0002516183885745704\n",
      "Iteration 482/2000, Loss: 0.0001830404216889292\n",
      "Iteration 483/2000, Loss: 0.00024858469259925187\n",
      "Iteration 484/2000, Loss: 0.00023132044589146972\n",
      "Iteration 485/2000, Loss: 0.0002164655743399635\n",
      "Iteration 486/2000, Loss: 0.00021153612760826945\n",
      "Iteration 487/2000, Loss: 0.0002454754139762372\n",
      "Iteration 488/2000, Loss: 0.0002021275577135384\n",
      "Iteration 489/2000, Loss: 0.0002107779000652954\n",
      "Iteration 490/2000, Loss: 0.0004570410819724202\n",
      "Iteration 491/2000, Loss: 0.00030413351487368345\n",
      "Iteration 492/2000, Loss: 0.0001731163210934028\n",
      "Iteration 493/2000, Loss: 0.00018827864550985396\n",
      "Iteration 494/2000, Loss: 0.000151813481352292\n",
      "Iteration 495/2000, Loss: 0.0003153699217364192\n",
      "Iteration 496/2000, Loss: 0.00017537595704197884\n",
      "Iteration 497/2000, Loss: 0.00022256946249399334\n",
      "Iteration 498/2000, Loss: 0.0004986601998098195\n",
      "Iteration 499/2000, Loss: 0.0003444712492637336\n",
      "Iteration 500/2000, Loss: 0.0001679573761066422\n",
      "Iteration 501/2000, Loss: 0.0009179640910588205\n",
      "Iteration 502/2000, Loss: 0.00023910279560368508\n",
      "Iteration 503/2000, Loss: 0.00035756651777774096\n",
      "Iteration 504/2000, Loss: 0.00036875877412967384\n",
      "Iteration 505/2000, Loss: 0.0001479857019148767\n",
      "Iteration 506/2000, Loss: 0.00039374351035803556\n",
      "Iteration 507/2000, Loss: 0.00023496751964557916\n",
      "Iteration 508/2000, Loss: 0.0003082129987888038\n",
      "Iteration 509/2000, Loss: 0.00032434577587991953\n",
      "Iteration 510/2000, Loss: 0.0001849455147748813\n",
      "Iteration 511/2000, Loss: 0.00029545777942985296\n",
      "Iteration 512/2000, Loss: 0.00015935656847432256\n",
      "Iteration 513/2000, Loss: 0.00021650019334629178\n",
      "Iteration 514/2000, Loss: 0.00021105313498992473\n",
      "Iteration 515/2000, Loss: 0.0003730437601916492\n",
      "Iteration 516/2000, Loss: 0.0002097670076182112\n",
      "Iteration 517/2000, Loss: 0.0002026874281000346\n",
      "Iteration 518/2000, Loss: 0.00026705305208452046\n",
      "Iteration 519/2000, Loss: 0.00020612668595276773\n",
      "Iteration 520/2000, Loss: 0.00030130933737382293\n",
      "Iteration 521/2000, Loss: 0.00015579596220050007\n",
      "Iteration 522/2000, Loss: 0.0003022780001629144\n",
      "Iteration 523/2000, Loss: 0.00020954479987267405\n",
      "Iteration 524/2000, Loss: 0.0005994313396513462\n",
      "Iteration 525/2000, Loss: 0.0001743174361763522\n",
      "Iteration 526/2000, Loss: 0.0005856982315890491\n",
      "Iteration 527/2000, Loss: 0.00023907232389319688\n",
      "Iteration 528/2000, Loss: 0.00021374173229560256\n",
      "Iteration 529/2000, Loss: 0.0002711185079533607\n",
      "Iteration 530/2000, Loss: 0.0005157827399671078\n",
      "Iteration 531/2000, Loss: 0.00013336099800653756\n",
      "Iteration 532/2000, Loss: 0.00023099737882148474\n",
      "Iteration 533/2000, Loss: 0.00017051563190761954\n",
      "Iteration 534/2000, Loss: 0.00013418054732028395\n",
      "Iteration 535/2000, Loss: 0.0001708338677417487\n",
      "Iteration 536/2000, Loss: 0.00015609365073032677\n",
      "Iteration 537/2000, Loss: 0.0001193749631056562\n",
      "Iteration 538/2000, Loss: 0.0002939772966783494\n",
      "Iteration 539/2000, Loss: 0.0001570333552081138\n",
      "Iteration 540/2000, Loss: 0.00014013001054991037\n",
      "Iteration 541/2000, Loss: 0.00019015632278751582\n",
      "Iteration 542/2000, Loss: 0.00012846603931393474\n",
      "Iteration 543/2000, Loss: 0.00013721153663937002\n",
      "Iteration 544/2000, Loss: 0.0001338492875220254\n",
      "Iteration 545/2000, Loss: 0.00011415817425586283\n",
      "Iteration 546/2000, Loss: 0.0001610230392543599\n",
      "Iteration 547/2000, Loss: 0.00013240324915386736\n",
      "Iteration 548/2000, Loss: 0.0001357030268991366\n",
      "Iteration 549/2000, Loss: 0.0001251229114132002\n",
      "Iteration 550/2000, Loss: 0.0002119965065503493\n",
      "Iteration 551/2000, Loss: 0.00028922854107804596\n",
      "Iteration 552/2000, Loss: 0.0003095630672760308\n",
      "Iteration 553/2000, Loss: 0.0001662969880271703\n",
      "Iteration 554/2000, Loss: 0.0001227843458764255\n",
      "Iteration 555/2000, Loss: 0.00013912130089011043\n",
      "Iteration 556/2000, Loss: 0.0002229459787486121\n",
      "Iteration 557/2000, Loss: 0.000147572995047085\n",
      "Iteration 558/2000, Loss: 0.0001148013470810838\n",
      "Iteration 559/2000, Loss: 0.00019934470765292645\n",
      "Iteration 560/2000, Loss: 0.0001799719175323844\n",
      "Iteration 561/2000, Loss: 0.00014340164489112794\n",
      "Iteration 562/2000, Loss: 0.00021292777091730386\n",
      "Iteration 563/2000, Loss: 0.00043161443318240345\n",
      "Iteration 564/2000, Loss: 0.00014077780360821635\n",
      "Iteration 565/2000, Loss: 0.00019125838298350573\n",
      "Iteration 566/2000, Loss: 0.0002156480768462643\n",
      "Iteration 567/2000, Loss: 0.00022610764426644892\n",
      "Iteration 568/2000, Loss: 0.00016362206952180713\n",
      "Iteration 569/2000, Loss: 0.00022109680867288262\n",
      "Iteration 570/2000, Loss: 0.00017227620992343873\n",
      "Iteration 571/2000, Loss: 0.00024667527759447694\n",
      "Iteration 572/2000, Loss: 0.00011040011304430664\n",
      "Iteration 573/2000, Loss: 0.0003835570241790265\n",
      "Iteration 574/2000, Loss: 0.0001804458152037114\n",
      "Iteration 575/2000, Loss: 0.0002477256639394909\n",
      "Iteration 576/2000, Loss: 0.00037361306021921337\n",
      "Iteration 577/2000, Loss: 0.00014080721302889287\n",
      "Iteration 578/2000, Loss: 0.00019803018949460238\n",
      "Iteration 579/2000, Loss: 0.0003328708990011364\n",
      "Iteration 580/2000, Loss: 0.0002144985628547147\n",
      "Iteration 581/2000, Loss: 0.00022028274543117732\n",
      "Iteration 582/2000, Loss: 0.0002349522546865046\n",
      "Iteration 583/2000, Loss: 0.00036627930239774287\n",
      "Iteration 584/2000, Loss: 0.0002325563837075606\n",
      "Iteration 585/2000, Loss: 0.0001919368951348588\n",
      "Iteration 586/2000, Loss: 0.00019366714695934206\n",
      "Iteration 587/2000, Loss: 0.0002113171067321673\n",
      "Iteration 588/2000, Loss: 0.00016414883430115879\n",
      "Iteration 589/2000, Loss: 0.0002394561015535146\n",
      "Iteration 590/2000, Loss: 0.00043160858331248164\n",
      "Iteration 591/2000, Loss: 0.00014678445586469024\n",
      "Iteration 592/2000, Loss: 0.0003441887383814901\n",
      "Iteration 593/2000, Loss: 0.00017684866907075047\n",
      "Iteration 594/2000, Loss: 0.0004682537983171642\n",
      "Iteration 595/2000, Loss: 0.00020921962277498096\n",
      "Iteration 596/2000, Loss: 0.00024175495491363108\n",
      "Iteration 597/2000, Loss: 0.0007417159504257143\n",
      "Iteration 598/2000, Loss: 0.00023860893270466477\n",
      "Iteration 599/2000, Loss: 0.0001458935730624944\n",
      "Iteration 600/2000, Loss: 0.0003922826726920903\n",
      "Iteration 601/2000, Loss: 0.00021933336392976344\n",
      "Iteration 602/2000, Loss: 0.00020127386960666627\n",
      "Iteration 603/2000, Loss: 0.00029358791653066874\n",
      "Iteration 604/2000, Loss: 0.0002752804721239954\n",
      "Iteration 605/2000, Loss: 0.00028083514189347625\n",
      "Iteration 606/2000, Loss: 0.0002599733415991068\n",
      "Iteration 607/2000, Loss: 0.0002697922755032778\n",
      "Iteration 608/2000, Loss: 0.00031945458613336086\n",
      "Iteration 609/2000, Loss: 0.00024296728952322155\n",
      "Iteration 610/2000, Loss: 0.00014652943355031312\n",
      "Iteration 611/2000, Loss: 0.00020509395108092576\n",
      "Iteration 612/2000, Loss: 0.00022399138833861798\n",
      "Iteration 613/2000, Loss: 0.00030006031738594174\n",
      "Iteration 614/2000, Loss: 0.0007528418791480362\n",
      "Iteration 615/2000, Loss: 0.00026981867267750204\n",
      "Iteration 616/2000, Loss: 0.0001393562270095572\n",
      "Iteration 617/2000, Loss: 0.00018054705287795514\n",
      "Iteration 618/2000, Loss: 0.00024665286764502525\n",
      "Iteration 619/2000, Loss: 0.00017932364426087588\n",
      "Iteration 620/2000, Loss: 0.0002710214757826179\n",
      "Iteration 621/2000, Loss: 0.0001589557941770181\n",
      "Iteration 622/2000, Loss: 0.00031529981060884893\n",
      "Iteration 623/2000, Loss: 0.0001417598541593179\n",
      "Iteration 624/2000, Loss: 0.00029093094053678215\n",
      "Iteration 625/2000, Loss: 0.0001482977095292881\n",
      "Iteration 626/2000, Loss: 0.0003797454119194299\n",
      "Iteration 627/2000, Loss: 0.00024016204406507313\n",
      "Iteration 628/2000, Loss: 0.00020250263332854956\n",
      "Iteration 629/2000, Loss: 0.0002675004943739623\n",
      "Iteration 630/2000, Loss: 0.00043784378794953227\n",
      "Iteration 631/2000, Loss: 0.0002754352753981948\n",
      "Iteration 632/2000, Loss: 0.0004430482804309577\n",
      "Iteration 633/2000, Loss: 0.0005924624856561422\n",
      "Iteration 634/2000, Loss: 0.00020212405070196837\n",
      "Iteration 635/2000, Loss: 0.0003119043540209532\n",
      "Iteration 636/2000, Loss: 0.0003768320893868804\n",
      "Iteration 637/2000, Loss: 0.000256470637395978\n",
      "Iteration 638/2000, Loss: 0.00016009797400329262\n",
      "Iteration 639/2000, Loss: 0.0002797039633151144\n",
      "Iteration 640/2000, Loss: 0.00016028557729441673\n",
      "Iteration 641/2000, Loss: 0.00022789862123318017\n",
      "Iteration 642/2000, Loss: 0.00020345256780274212\n",
      "Iteration 643/2000, Loss: 0.00041836596210487187\n",
      "Iteration 644/2000, Loss: 0.00020903695258311927\n",
      "Iteration 645/2000, Loss: 0.00021327647846192122\n",
      "Iteration 646/2000, Loss: 0.0003635763714555651\n",
      "Iteration 647/2000, Loss: 0.00020896487694699317\n",
      "Iteration 648/2000, Loss: 0.00024238324840553105\n",
      "Iteration 649/2000, Loss: 0.00022219751554075629\n",
      "Iteration 650/2000, Loss: 0.0009730707388371229\n",
      "Iteration 651/2000, Loss: 0.00014842511154711246\n",
      "Iteration 652/2000, Loss: 0.00033428185270167887\n",
      "Iteration 653/2000, Loss: 0.0002017496299231425\n",
      "Iteration 654/2000, Loss: 0.00026102777337655425\n",
      "Iteration 655/2000, Loss: 0.00022209101007319987\n",
      "Iteration 656/2000, Loss: 0.00016734693781472743\n",
      "Iteration 657/2000, Loss: 0.000242799156694673\n",
      "Iteration 658/2000, Loss: 0.00019348309433553368\n",
      "Iteration 659/2000, Loss: 0.00030257186153903604\n",
      "Iteration 660/2000, Loss: 0.0002899340179283172\n",
      "Iteration 661/2000, Loss: 0.00018832618661690503\n",
      "Iteration 662/2000, Loss: 0.00030841041007079184\n",
      "Iteration 663/2000, Loss: 0.00020031943859066814\n",
      "Iteration 664/2000, Loss: 0.00018169760005548596\n",
      "Iteration 665/2000, Loss: 0.0001896647154353559\n",
      "Iteration 666/2000, Loss: 0.00021579604072030634\n",
      "Iteration 667/2000, Loss: 0.00035901812952943146\n",
      "Iteration 668/2000, Loss: 0.00033335734042339027\n",
      "Iteration 669/2000, Loss: 0.0002494707878213376\n",
      "Iteration 670/2000, Loss: 0.0004415785660967231\n",
      "Iteration 671/2000, Loss: 0.0003759597893804312\n",
      "Iteration 672/2000, Loss: 0.0001918720081448555\n",
      "Iteration 673/2000, Loss: 0.0001668924087425694\n",
      "Iteration 674/2000, Loss: 0.000247958698309958\n",
      "Iteration 675/2000, Loss: 0.00025812495732679963\n",
      "Iteration 676/2000, Loss: 0.0005982102593407035\n",
      "Iteration 677/2000, Loss: 0.00019373251416254789\n",
      "Iteration 678/2000, Loss: 0.0001536406489321962\n",
      "Iteration 679/2000, Loss: 0.00013481297355610877\n",
      "Iteration 680/2000, Loss: 0.00027001037960872054\n",
      "Iteration 681/2000, Loss: 0.00021697807824239135\n",
      "Iteration 682/2000, Loss: 0.0004122242971789092\n",
      "Iteration 683/2000, Loss: 0.000215879685129039\n",
      "Iteration 684/2000, Loss: 0.0002253930870210752\n",
      "Iteration 685/2000, Loss: 0.0001801287871785462\n",
      "Iteration 686/2000, Loss: 0.00018131238175556064\n",
      "Iteration 687/2000, Loss: 0.00021210256090853363\n",
      "Iteration 688/2000, Loss: 0.00020310563559178263\n",
      "Iteration 689/2000, Loss: 0.00014271009422373027\n",
      "Iteration 690/2000, Loss: 0.00017705874051898718\n",
      "Iteration 691/2000, Loss: 0.00012849571066908538\n",
      "Iteration 692/2000, Loss: 0.00014196275151334703\n",
      "Iteration 693/2000, Loss: 0.00022551245638169348\n",
      "Iteration 694/2000, Loss: 0.0002610103983897716\n",
      "Iteration 695/2000, Loss: 0.00016219478857237846\n",
      "Iteration 696/2000, Loss: 0.00026155170053243637\n",
      "Iteration 697/2000, Loss: 0.00020194090029690415\n",
      "Iteration 698/2000, Loss: 0.0001434209116268903\n",
      "Iteration 699/2000, Loss: 0.00011266273213550448\n",
      "Iteration 700/2000, Loss: 0.00035190043854527175\n",
      "Iteration 701/2000, Loss: 0.00019879404862876981\n",
      "Iteration 702/2000, Loss: 0.00013567693531513214\n",
      "Iteration 703/2000, Loss: 0.0003718274529092014\n",
      "Iteration 704/2000, Loss: 0.00038114958442747593\n",
      "Iteration 705/2000, Loss: 0.00018889248894993216\n",
      "Iteration 706/2000, Loss: 0.00021374273637775332\n",
      "Iteration 707/2000, Loss: 0.00019402694306336343\n",
      "Iteration 708/2000, Loss: 0.0004629175236914307\n",
      "Iteration 709/2000, Loss: 0.00027555529959499836\n",
      "Iteration 710/2000, Loss: 0.00021331213065423071\n",
      "Iteration 711/2000, Loss: 0.0004655097145587206\n",
      "Iteration 712/2000, Loss: 0.00038006328395567834\n",
      "Iteration 713/2000, Loss: 0.00022811601229477674\n",
      "Iteration 714/2000, Loss: 0.00024157657753676176\n",
      "Iteration 715/2000, Loss: 0.0001943783281603828\n",
      "Iteration 716/2000, Loss: 0.0005681298789568245\n",
      "Iteration 717/2000, Loss: 0.00024294748436659575\n",
      "Iteration 718/2000, Loss: 0.0001559811644256115\n",
      "Iteration 719/2000, Loss: 0.00017456298519391567\n",
      "Iteration 720/2000, Loss: 0.00019829606753773987\n",
      "Iteration 721/2000, Loss: 0.00017057226796168834\n",
      "Iteration 722/2000, Loss: 0.0001601469557499513\n",
      "Iteration 723/2000, Loss: 0.0002415323251625523\n",
      "Iteration 724/2000, Loss: 0.00013890348782297224\n",
      "Iteration 725/2000, Loss: 0.0003011551743838936\n",
      "Iteration 726/2000, Loss: 0.00020760999177582562\n",
      "Iteration 727/2000, Loss: 0.0002010668977163732\n",
      "Iteration 728/2000, Loss: 0.00025467443629167974\n",
      "Iteration 729/2000, Loss: 0.00014054655912332237\n",
      "Iteration 730/2000, Loss: 0.0001336434215772897\n",
      "Iteration 731/2000, Loss: 0.0002366094122407958\n",
      "Iteration 732/2000, Loss: 0.00023569035693071783\n",
      "Iteration 733/2000, Loss: 0.00025784698664210737\n",
      "Iteration 734/2000, Loss: 0.00014163766172714531\n",
      "Iteration 735/2000, Loss: 0.00021656829630956054\n",
      "Iteration 736/2000, Loss: 0.00015565750072710216\n",
      "Iteration 737/2000, Loss: 0.00021079936414025724\n",
      "Iteration 738/2000, Loss: 0.00013332650996744633\n",
      "Iteration 739/2000, Loss: 0.00019666936714202166\n",
      "Iteration 740/2000, Loss: 0.00016870138642843813\n",
      "Iteration 741/2000, Loss: 0.00019263404828961939\n",
      "Iteration 742/2000, Loss: 0.00011895997886313125\n",
      "Iteration 743/2000, Loss: 0.00012466036423575133\n",
      "Iteration 744/2000, Loss: 0.00019989782595075667\n",
      "Iteration 745/2000, Loss: 0.00020434928592294455\n",
      "Iteration 746/2000, Loss: 0.00021902918524574488\n",
      "Iteration 747/2000, Loss: 0.00022606720449402928\n",
      "Iteration 748/2000, Loss: 0.00014219865261111408\n",
      "Iteration 749/2000, Loss: 0.00021861071581952274\n",
      "Iteration 750/2000, Loss: 0.00017539529653731734\n",
      "Iteration 751/2000, Loss: 0.00017313876014668494\n",
      "Iteration 752/2000, Loss: 0.0001784552150638774\n",
      "Iteration 753/2000, Loss: 0.0001159836319857277\n",
      "Iteration 754/2000, Loss: 0.0002286576054757461\n",
      "Iteration 755/2000, Loss: 0.00013252825010567904\n",
      "Iteration 756/2000, Loss: 0.00017895815835800022\n",
      "Iteration 757/2000, Loss: 0.00016203217091970146\n",
      "Iteration 758/2000, Loss: 0.0002776903274934739\n",
      "Iteration 759/2000, Loss: 0.00018233328592032194\n",
      "Iteration 760/2000, Loss: 0.00014822102093603462\n",
      "Iteration 761/2000, Loss: 0.0003602081269491464\n",
      "Iteration 762/2000, Loss: 0.00016573796165175736\n",
      "Iteration 763/2000, Loss: 9.383758879266679e-05\n",
      "Iteration 764/2000, Loss: 0.0015561467735096812\n",
      "Iteration 765/2000, Loss: 0.00026442736270837486\n",
      "Iteration 766/2000, Loss: 0.00040650396840646863\n",
      "Iteration 767/2000, Loss: 0.00018967865617014468\n",
      "Iteration 768/2000, Loss: 0.00023858516942709684\n",
      "Iteration 769/2000, Loss: 0.00035152799682691693\n",
      "Iteration 770/2000, Loss: 0.00022360125149134547\n",
      "Iteration 771/2000, Loss: 0.0002798186324071139\n",
      "Iteration 772/2000, Loss: 0.00023326849623117596\n",
      "Iteration 773/2000, Loss: 0.00021755309717264026\n",
      "Iteration 774/2000, Loss: 0.0002555633254814893\n",
      "Iteration 775/2000, Loss: 0.0002643399639055133\n",
      "Iteration 776/2000, Loss: 0.00017206261691171676\n",
      "Iteration 777/2000, Loss: 0.00013406250218395144\n",
      "Iteration 778/2000, Loss: 0.00028586125699803233\n",
      "Iteration 779/2000, Loss: 0.00011945408914471045\n",
      "Iteration 780/2000, Loss: 0.00030540736042894423\n",
      "Iteration 781/2000, Loss: 0.0001983571273740381\n",
      "Iteration 782/2000, Loss: 9.915274131344631e-05\n",
      "Iteration 783/2000, Loss: 0.00025190855376422405\n",
      "Iteration 784/2000, Loss: 0.00043153733713552356\n",
      "Iteration 785/2000, Loss: 0.00012732509640045464\n",
      "Iteration 786/2000, Loss: 0.00013744014722760767\n",
      "Iteration 787/2000, Loss: 0.00016759944264777005\n",
      "Iteration 788/2000, Loss: 0.00022049174003768712\n",
      "Iteration 789/2000, Loss: 0.00021695776376873255\n",
      "Iteration 790/2000, Loss: 0.00026706972857937217\n",
      "Iteration 791/2000, Loss: 0.00022232666378840804\n",
      "Iteration 792/2000, Loss: 0.00042225400102324784\n",
      "Iteration 793/2000, Loss: 0.00021609020768664777\n",
      "Iteration 794/2000, Loss: 0.0003602677315939218\n",
      "Iteration 795/2000, Loss: 0.00018915586406365037\n",
      "Iteration 796/2000, Loss: 0.00029878379427827895\n",
      "Iteration 797/2000, Loss: 0.000316308083711192\n",
      "Iteration 798/2000, Loss: 0.00026232749223709106\n",
      "Iteration 799/2000, Loss: 0.0001806857908377424\n",
      "Iteration 800/2000, Loss: 0.00022277631796896458\n",
      "Iteration 801/2000, Loss: 0.00026597900432534516\n",
      "Iteration 802/2000, Loss: 0.00026742435875348747\n",
      "Iteration 803/2000, Loss: 0.00032993213972076774\n",
      "Iteration 804/2000, Loss: 0.00019578849605750293\n",
      "Iteration 805/2000, Loss: 0.0001586306025274098\n",
      "Iteration 806/2000, Loss: 0.0001838366879383102\n",
      "Iteration 807/2000, Loss: 0.00017786049284040928\n",
      "Iteration 808/2000, Loss: 0.00030450461781583726\n",
      "Iteration 809/2000, Loss: 0.00020348417456261814\n",
      "Iteration 810/2000, Loss: 0.00015494019316975027\n",
      "Iteration 811/2000, Loss: 0.00016042152128648013\n",
      "Iteration 812/2000, Loss: 0.0009135178406722844\n",
      "Iteration 813/2000, Loss: 0.0002570140059106052\n",
      "Iteration 814/2000, Loss: 0.00022336264373734593\n",
      "Iteration 815/2000, Loss: 0.0002556534600444138\n",
      "Iteration 816/2000, Loss: 0.00014489886234514415\n",
      "Iteration 817/2000, Loss: 0.0002297666360391304\n",
      "Iteration 818/2000, Loss: 0.0002548324700910598\n",
      "Iteration 819/2000, Loss: 0.00014846227713860571\n",
      "Iteration 820/2000, Loss: 0.00014583105803467333\n",
      "Iteration 821/2000, Loss: 0.00024343753466382623\n",
      "Iteration 822/2000, Loss: 0.0002061843842966482\n",
      "Iteration 823/2000, Loss: 0.00025089632254093885\n",
      "Iteration 824/2000, Loss: 0.0002942603314295411\n",
      "Iteration 825/2000, Loss: 0.0001407229865435511\n",
      "Iteration 826/2000, Loss: 0.0003279316006228328\n",
      "Iteration 827/2000, Loss: 0.00019387993961572647\n",
      "Iteration 828/2000, Loss: 0.000231687372433953\n",
      "Iteration 829/2000, Loss: 0.00014152395306155086\n",
      "Iteration 830/2000, Loss: 0.00017851135635282844\n",
      "Iteration 831/2000, Loss: 0.0002051529154414311\n",
      "Iteration 832/2000, Loss: 0.0002739572082646191\n",
      "Iteration 833/2000, Loss: 0.0002356523764319718\n",
      "Iteration 834/2000, Loss: 0.00011989691120106727\n",
      "Iteration 835/2000, Loss: 0.00020692095858976245\n",
      "Iteration 836/2000, Loss: 0.00029595004161819816\n",
      "Iteration 837/2000, Loss: 0.0002336694742552936\n",
      "Iteration 838/2000, Loss: 0.00018273055320605636\n",
      "Iteration 839/2000, Loss: 0.00021690728317480534\n",
      "Iteration 840/2000, Loss: 0.00012164594954811037\n",
      "Iteration 841/2000, Loss: 0.0002531490463297814\n",
      "Iteration 842/2000, Loss: 0.00023817889450583607\n",
      "Iteration 843/2000, Loss: 0.00016471937124151736\n",
      "Iteration 844/2000, Loss: 0.00020754725846927613\n",
      "Iteration 845/2000, Loss: 0.00019500839698594064\n",
      "Iteration 846/2000, Loss: 0.00025868412922136486\n",
      "Iteration 847/2000, Loss: 0.0001545338163850829\n",
      "Iteration 848/2000, Loss: 0.00012605683878064156\n",
      "Iteration 849/2000, Loss: 0.00022709470067638904\n",
      "Iteration 850/2000, Loss: 0.0004163467383477837\n",
      "Iteration 851/2000, Loss: 0.00018684571841731668\n",
      "Iteration 852/2000, Loss: 0.0002494164800737053\n",
      "Iteration 853/2000, Loss: 0.00018362038827035576\n",
      "Iteration 854/2000, Loss: 0.00032126763835549355\n",
      "Iteration 855/2000, Loss: 0.00021284594549797475\n",
      "Iteration 856/2000, Loss: 0.000278824707493186\n",
      "Iteration 857/2000, Loss: 0.00036708792322315276\n",
      "Iteration 858/2000, Loss: 0.0002861639950424433\n",
      "Iteration 859/2000, Loss: 0.0003720184613484889\n",
      "Iteration 860/2000, Loss: 0.0004104251856915653\n",
      "Iteration 861/2000, Loss: 0.00023665704065933824\n",
      "Iteration 862/2000, Loss: 0.000307959271594882\n",
      "Iteration 863/2000, Loss: 0.00039665948133915663\n",
      "Iteration 864/2000, Loss: 0.00032767857192084193\n",
      "Iteration 865/2000, Loss: 0.00021141707838978618\n",
      "Iteration 866/2000, Loss: 0.00027174458955414593\n",
      "Iteration 867/2000, Loss: 0.00017488189041614532\n",
      "Iteration 868/2000, Loss: 0.00038594595389440656\n",
      "Iteration 869/2000, Loss: 0.00026990968035534024\n",
      "Iteration 870/2000, Loss: 0.0003052917600143701\n",
      "Iteration 871/2000, Loss: 0.0001440152555005625\n",
      "Iteration 872/2000, Loss: 0.0002617595309857279\n",
      "Iteration 873/2000, Loss: 0.0002891424810513854\n",
      "Iteration 874/2000, Loss: 0.0001813998242141679\n",
      "Iteration 875/2000, Loss: 0.00024006010789889842\n",
      "Iteration 876/2000, Loss: 9.246546687791124e-05\n",
      "Iteration 877/2000, Loss: 0.0002307768154423684\n",
      "Iteration 878/2000, Loss: 0.00016745117318350822\n",
      "Iteration 879/2000, Loss: 0.00017575733363628387\n",
      "Iteration 880/2000, Loss: 0.0006210538558661938\n",
      "Iteration 881/2000, Loss: 0.00018145966168958694\n",
      "Iteration 882/2000, Loss: 0.00033905779127962887\n",
      "Iteration 883/2000, Loss: 0.00042778567876666784\n",
      "Iteration 884/2000, Loss: 0.00022428772354032844\n",
      "Iteration 885/2000, Loss: 0.000609423266723752\n",
      "Iteration 886/2000, Loss: 0.00024209781258832663\n",
      "Iteration 887/2000, Loss: 0.0005931381019763649\n",
      "Iteration 888/2000, Loss: 0.00043882502359338105\n",
      "Iteration 889/2000, Loss: 0.0003116692241746932\n",
      "Iteration 890/2000, Loss: 0.00035176228266209364\n",
      "Iteration 891/2000, Loss: 0.0002989929635077715\n",
      "Iteration 892/2000, Loss: 0.0004455028392840177\n",
      "Iteration 893/2000, Loss: 0.00021764020493719727\n",
      "Iteration 894/2000, Loss: 0.00032238822313956916\n",
      "Iteration 895/2000, Loss: 0.00030803357367403805\n",
      "Iteration 896/2000, Loss: 0.00018851696222554892\n",
      "Iteration 897/2000, Loss: 0.0002847307187039405\n",
      "Iteration 898/2000, Loss: 0.00015536324644926935\n",
      "Iteration 899/2000, Loss: 0.0004219206457491964\n",
      "Iteration 900/2000, Loss: 0.00022521558275911957\n",
      "Iteration 901/2000, Loss: 0.00020894675981253386\n",
      "Iteration 902/2000, Loss: 0.0001561063836561516\n",
      "Iteration 903/2000, Loss: 0.00010501751967240125\n",
      "Iteration 904/2000, Loss: 0.00035175809171050787\n",
      "Iteration 905/2000, Loss: 0.000590137264225632\n",
      "Iteration 906/2000, Loss: 0.00015430412895511836\n",
      "Iteration 907/2000, Loss: 0.00019672082271426916\n",
      "Iteration 908/2000, Loss: 0.0001575524074723944\n",
      "Iteration 909/2000, Loss: 0.0006275391788221896\n",
      "Iteration 910/2000, Loss: 0.00019751446961890906\n",
      "Iteration 911/2000, Loss: 0.00020020222291350365\n",
      "Iteration 912/2000, Loss: 0.00021667213877663016\n",
      "Iteration 913/2000, Loss: 0.00016806903295218945\n",
      "Iteration 914/2000, Loss: 0.00030524356407113373\n",
      "Iteration 915/2000, Loss: 0.00012850873463321477\n",
      "Iteration 916/2000, Loss: 0.00027746372506953776\n",
      "Iteration 917/2000, Loss: 0.00014196326083038002\n",
      "Iteration 918/2000, Loss: 0.0002033227647189051\n",
      "Iteration 919/2000, Loss: 0.00021708669373765588\n",
      "Iteration 920/2000, Loss: 0.00018253953021485358\n",
      "Iteration 921/2000, Loss: 0.00026792887365445495\n",
      "Iteration 922/2000, Loss: 0.00015574244025629014\n",
      "Iteration 923/2000, Loss: 0.0001495105680078268\n",
      "Iteration 924/2000, Loss: 0.0003390388155821711\n",
      "Iteration 925/2000, Loss: 0.00020666704222094268\n",
      "Iteration 926/2000, Loss: 0.00025635736528784037\n",
      "Iteration 927/2000, Loss: 0.0001845373335527256\n",
      "Iteration 928/2000, Loss: 0.0004310032818466425\n",
      "Iteration 929/2000, Loss: 0.00016736125689931214\n",
      "Iteration 930/2000, Loss: 0.00029142427956685424\n",
      "Iteration 931/2000, Loss: 0.0003872480592690408\n",
      "Iteration 932/2000, Loss: 0.00017578787810634822\n",
      "Iteration 933/2000, Loss: 0.00023260073794517666\n",
      "Iteration 934/2000, Loss: 0.00022864338825456798\n",
      "Iteration 935/2000, Loss: 0.0004042151849716902\n",
      "Iteration 936/2000, Loss: 0.00036802716203965247\n",
      "Iteration 937/2000, Loss: 0.0003760401741601527\n",
      "Iteration 938/2000, Loss: 0.0003837536205537617\n",
      "Iteration 939/2000, Loss: 0.00029171694768592715\n",
      "Iteration 940/2000, Loss: 0.00023304324713535607\n",
      "Iteration 941/2000, Loss: 0.0001892728469101712\n",
      "Iteration 942/2000, Loss: 0.0001644379663048312\n",
      "Iteration 943/2000, Loss: 0.0001702577283140272\n",
      "Iteration 944/2000, Loss: 0.0002401527890469879\n",
      "Iteration 945/2000, Loss: 0.00032536950311623514\n",
      "Iteration 946/2000, Loss: 0.00020134553778916597\n",
      "Iteration 947/2000, Loss: 0.0004142243997193873\n",
      "Iteration 948/2000, Loss: 0.00010035902232630178\n",
      "Iteration 949/2000, Loss: 0.000430345069617033\n",
      "Iteration 950/2000, Loss: 0.00036523534799925983\n",
      "Iteration 951/2000, Loss: 0.0001429070543963462\n",
      "Iteration 952/2000, Loss: 0.00039704469963908195\n",
      "Iteration 953/2000, Loss: 0.0001956317137228325\n",
      "Iteration 954/2000, Loss: 0.00020608196791727096\n",
      "Iteration 955/2000, Loss: 0.0002086325694108382\n",
      "Iteration 956/2000, Loss: 0.0002113099762937054\n",
      "Iteration 957/2000, Loss: 0.00022991473088040948\n",
      "Iteration 958/2000, Loss: 0.0003042149473913014\n",
      "Iteration 959/2000, Loss: 0.0001059106580214575\n",
      "Iteration 960/2000, Loss: 0.00010895766899921\n",
      "Iteration 961/2000, Loss: 0.000422536744736135\n",
      "Iteration 962/2000, Loss: 0.0001481837098253891\n",
      "Iteration 963/2000, Loss: 0.0002960395941045135\n",
      "Iteration 964/2000, Loss: 0.00018325637211091816\n",
      "Iteration 965/2000, Loss: 0.00014692773402202874\n",
      "Iteration 966/2000, Loss: 0.00019627240544650704\n",
      "Iteration 967/2000, Loss: 0.00021623578504659235\n",
      "Iteration 968/2000, Loss: 0.00020153634250164032\n",
      "Iteration 969/2000, Loss: 0.00018005965102929622\n",
      "Iteration 970/2000, Loss: 0.00016579760995227844\n",
      "Iteration 971/2000, Loss: 0.0001715811959002167\n",
      "Iteration 972/2000, Loss: 0.00034064511419273913\n",
      "Iteration 973/2000, Loss: 0.00024673607549630105\n",
      "Iteration 974/2000, Loss: 0.00011589504720177501\n",
      "Iteration 975/2000, Loss: 0.00020509009482339025\n",
      "Iteration 976/2000, Loss: 0.00037012185202911496\n",
      "Iteration 977/2000, Loss: 0.00016226887237280607\n",
      "Iteration 978/2000, Loss: 0.0002780119830276817\n",
      "Iteration 979/2000, Loss: 0.00019206159049645066\n",
      "Iteration 980/2000, Loss: 0.00028288981411606073\n",
      "Iteration 981/2000, Loss: 0.0002525456075090915\n",
      "Iteration 982/2000, Loss: 0.0003122106718365103\n",
      "Iteration 983/2000, Loss: 0.00022275798255577683\n",
      "Iteration 984/2000, Loss: 0.00023279500601347536\n",
      "Iteration 985/2000, Loss: 0.0002451399923302233\n",
      "Iteration 986/2000, Loss: 0.00023519678507000208\n",
      "Iteration 987/2000, Loss: 0.000152224994963035\n",
      "Iteration 988/2000, Loss: 0.00014171679504215717\n",
      "Iteration 989/2000, Loss: 0.00030255643650889397\n",
      "Iteration 990/2000, Loss: 0.0002753542794380337\n",
      "Iteration 991/2000, Loss: 0.0001766356872394681\n",
      "Iteration 992/2000, Loss: 0.0001913995511131361\n",
      "Iteration 993/2000, Loss: 0.00012690722360275686\n",
      "Iteration 994/2000, Loss: 0.00012486489140428603\n",
      "Iteration 995/2000, Loss: 0.0002494552463758737\n",
      "Iteration 996/2000, Loss: 0.0001503155945101753\n",
      "Iteration 997/2000, Loss: 0.00015510203957092017\n",
      "Iteration 998/2000, Loss: 0.0001600295363459736\n",
      "Iteration 999/2000, Loss: 0.00016616194625385106\n",
      "Iteration 1000/2000, Loss: 0.00012959999730810523\n",
      "Iteration 1001/2000, Loss: 0.00019889559189323336\n",
      "Iteration 1002/2000, Loss: 0.00011773049482144415\n",
      "Iteration 1003/2000, Loss: 0.0003046196943614632\n",
      "Iteration 1004/2000, Loss: 0.000273340119747445\n",
      "Iteration 1005/2000, Loss: 0.0003659257781691849\n",
      "Iteration 1006/2000, Loss: 0.00023666786728426814\n",
      "Iteration 1007/2000, Loss: 0.00033372078905813396\n",
      "Iteration 1008/2000, Loss: 0.0002897102967835963\n",
      "Iteration 1009/2000, Loss: 0.00014873182226438075\n",
      "Iteration 1010/2000, Loss: 0.00035783901694230735\n",
      "Iteration 1011/2000, Loss: 0.00035939994268119335\n",
      "Iteration 1012/2000, Loss: 0.0002978435659315437\n",
      "Iteration 1013/2000, Loss: 0.00019226181029807776\n",
      "Iteration 1014/2000, Loss: 0.0002348247799091041\n",
      "Iteration 1015/2000, Loss: 0.000251789519097656\n",
      "Iteration 1016/2000, Loss: 0.0001723339082673192\n",
      "Iteration 1017/2000, Loss: 0.0003517475270200521\n",
      "Iteration 1018/2000, Loss: 0.0002743690274655819\n",
      "Iteration 1019/2000, Loss: 0.0002674237184692174\n",
      "Iteration 1020/2000, Loss: 0.00023636269907001406\n",
      "Iteration 1021/2000, Loss: 0.00017541834677103907\n",
      "Iteration 1022/2000, Loss: 0.0002757163892965764\n",
      "Iteration 1023/2000, Loss: 0.0003298597875982523\n",
      "Iteration 1024/2000, Loss: 0.0002730810665525496\n",
      "Iteration 1025/2000, Loss: 0.0005085775628685951\n",
      "Iteration 1026/2000, Loss: 0.00019371505186427385\n",
      "Iteration 1027/2000, Loss: 0.00027443389990366995\n",
      "Iteration 1028/2000, Loss: 0.00013268471229821444\n",
      "Iteration 1029/2000, Loss: 0.0003105460782535374\n",
      "Iteration 1030/2000, Loss: 0.00017597975966054946\n",
      "Iteration 1031/2000, Loss: 0.00023104395950213075\n",
      "Iteration 1032/2000, Loss: 0.0002010124735534191\n",
      "Iteration 1033/2000, Loss: 0.000117665265861433\n",
      "Iteration 1034/2000, Loss: 0.00018047483172267675\n",
      "Iteration 1035/2000, Loss: 0.0005377334891818464\n",
      "Iteration 1036/2000, Loss: 0.00026583202998153865\n",
      "Iteration 1037/2000, Loss: 0.00018051006190944463\n",
      "Iteration 1038/2000, Loss: 0.00011230822565266863\n",
      "Iteration 1039/2000, Loss: 0.0002878222439903766\n",
      "Iteration 1040/2000, Loss: 0.0004398403107188642\n",
      "Iteration 1041/2000, Loss: 0.00022497955069411546\n",
      "Iteration 1042/2000, Loss: 0.0003500530438032001\n",
      "Iteration 1043/2000, Loss: 0.000380138837499544\n",
      "Iteration 1044/2000, Loss: 0.0001886339159682393\n",
      "Iteration 1045/2000, Loss: 0.0002562945010140538\n",
      "Iteration 1046/2000, Loss: 0.0001726567861624062\n",
      "Iteration 1047/2000, Loss: 0.0002842741960193962\n",
      "Iteration 1048/2000, Loss: 0.0002887506561819464\n",
      "Iteration 1049/2000, Loss: 0.00016242438869085163\n",
      "Iteration 1050/2000, Loss: 0.0001723115856293589\n",
      "Iteration 1051/2000, Loss: 0.00013752072118222713\n",
      "Iteration 1052/2000, Loss: 0.00014975586964283139\n",
      "Iteration 1053/2000, Loss: 0.00013153351028449833\n",
      "Iteration 1054/2000, Loss: 0.0001597194786882028\n",
      "Iteration 1055/2000, Loss: 0.00012068039359292015\n",
      "Iteration 1056/2000, Loss: 0.00011360736971255392\n",
      "Iteration 1057/2000, Loss: 0.00014007037680130452\n",
      "Iteration 1058/2000, Loss: 0.0005952026112936437\n",
      "Iteration 1059/2000, Loss: 0.00012100459571229294\n",
      "Iteration 1060/2000, Loss: 0.00015995105786714703\n",
      "Iteration 1061/2000, Loss: 9.500730811851099e-05\n",
      "Iteration 1062/2000, Loss: 0.00034493819111958146\n",
      "Iteration 1063/2000, Loss: 0.00010773537360364571\n",
      "Iteration 1064/2000, Loss: 0.00011794477904913947\n",
      "Iteration 1065/2000, Loss: 0.0001987702416954562\n",
      "Iteration 1066/2000, Loss: 0.0002269287797389552\n",
      "Iteration 1067/2000, Loss: 0.00017350907728541642\n",
      "Iteration 1068/2000, Loss: 0.00010815343557624146\n",
      "Iteration 1069/2000, Loss: 0.00047275066026486456\n",
      "Iteration 1070/2000, Loss: 0.0001518446224508807\n",
      "Iteration 1071/2000, Loss: 0.00012193364818813279\n",
      "Iteration 1072/2000, Loss: 0.00031120108906179667\n",
      "Iteration 1073/2000, Loss: 0.00025000900495797396\n",
      "Iteration 1074/2000, Loss: 0.00019752288062591106\n",
      "Iteration 1075/2000, Loss: 0.00012843147851526737\n",
      "Iteration 1076/2000, Loss: 0.00017264681810047477\n",
      "Iteration 1077/2000, Loss: 0.00021041273430455476\n",
      "Iteration 1078/2000, Loss: 0.00017084699356928468\n",
      "Iteration 1079/2000, Loss: 0.00020265206694602966\n",
      "Iteration 1080/2000, Loss: 0.0001498361671110615\n",
      "Iteration 1081/2000, Loss: 0.0003267265565227717\n",
      "Iteration 1082/2000, Loss: 0.00027593239792622626\n",
      "Iteration 1083/2000, Loss: 0.00033849015017040074\n",
      "Iteration 1084/2000, Loss: 0.00020894015324302018\n",
      "Iteration 1085/2000, Loss: 0.0003277983341831714\n",
      "Iteration 1086/2000, Loss: 0.000125205609947443\n",
      "Iteration 1087/2000, Loss: 0.00018472484953235835\n",
      "Iteration 1088/2000, Loss: 0.00011086237645940855\n",
      "Iteration 1089/2000, Loss: 0.0001842937635956332\n",
      "Iteration 1090/2000, Loss: 0.00025056343292817473\n",
      "Iteration 1091/2000, Loss: 0.000172667860169895\n",
      "Iteration 1092/2000, Loss: 0.00017466941790189594\n",
      "Iteration 1093/2000, Loss: 0.00025276344968006015\n",
      "Iteration 1094/2000, Loss: 0.00036323670065030456\n",
      "Iteration 1095/2000, Loss: 0.0001207232489832677\n",
      "Iteration 1096/2000, Loss: 0.0003607890394050628\n",
      "Iteration 1097/2000, Loss: 0.00020331113773863763\n",
      "Iteration 1098/2000, Loss: 0.001039469731040299\n",
      "Iteration 1099/2000, Loss: 0.00031880231108516455\n",
      "Iteration 1100/2000, Loss: 0.00032358651515096426\n",
      "Iteration 1101/2000, Loss: 0.0002732225984800607\n",
      "Iteration 1102/2000, Loss: 0.00014536414528265595\n",
      "Iteration 1103/2000, Loss: 0.00020573739311657846\n",
      "Iteration 1104/2000, Loss: 0.00018475281831342727\n",
      "Iteration 1105/2000, Loss: 0.0002167580387322232\n",
      "Iteration 1106/2000, Loss: 0.00029128347523510456\n",
      "Iteration 1107/2000, Loss: 0.00023048229923006147\n",
      "Iteration 1108/2000, Loss: 0.0002253079874208197\n",
      "Iteration 1109/2000, Loss: 0.0001437091559637338\n",
      "Iteration 1110/2000, Loss: 0.00021639511396642774\n",
      "Iteration 1111/2000, Loss: 0.00015173686551861465\n",
      "Iteration 1112/2000, Loss: 0.0001982209796551615\n",
      "Iteration 1113/2000, Loss: 0.00020516072981990874\n",
      "Iteration 1114/2000, Loss: 0.0002673796843737364\n",
      "Iteration 1115/2000, Loss: 0.00017131683125626296\n",
      "Iteration 1116/2000, Loss: 0.00023570365738123655\n",
      "Iteration 1117/2000, Loss: 0.0004025913658551872\n",
      "Iteration 1118/2000, Loss: 0.0016941953217610717\n",
      "Iteration 1119/2000, Loss: 0.00023481700918637216\n",
      "Iteration 1120/2000, Loss: 0.00018416343664284796\n",
      "Iteration 1121/2000, Loss: 0.0005010229651816189\n",
      "Iteration 1122/2000, Loss: 0.00018315673514734954\n",
      "Iteration 1123/2000, Loss: 0.0002349869319004938\n",
      "Iteration 1124/2000, Loss: 0.00043247180292382836\n",
      "Iteration 1125/2000, Loss: 0.0001892520085675642\n",
      "Iteration 1126/2000, Loss: 0.0002343612868571654\n",
      "Iteration 1127/2000, Loss: 0.00023196350957732648\n",
      "Iteration 1128/2000, Loss: 0.00036365340929478407\n",
      "Iteration 1129/2000, Loss: 0.00030719395726919174\n",
      "Iteration 1130/2000, Loss: 0.00047891822759993374\n",
      "Iteration 1131/2000, Loss: 0.0003287926083430648\n",
      "Iteration 1132/2000, Loss: 0.0003223208768758923\n",
      "Iteration 1133/2000, Loss: 0.0002073163486784324\n",
      "Iteration 1134/2000, Loss: 0.00012637703912332654\n",
      "Iteration 1135/2000, Loss: 0.00014785230450797826\n",
      "Iteration 1136/2000, Loss: 0.00034082375350408256\n",
      "Iteration 1137/2000, Loss: 0.00022057486057747155\n",
      "Iteration 1138/2000, Loss: 0.0001558353251311928\n",
      "Iteration 1139/2000, Loss: 0.00024862485588528216\n",
      "Iteration 1140/2000, Loss: 0.00015923322644084692\n",
      "Iteration 1141/2000, Loss: 0.0001957061031134799\n",
      "Iteration 1142/2000, Loss: 0.0002608391805551946\n",
      "Iteration 1143/2000, Loss: 0.0002259642060380429\n",
      "Iteration 1144/2000, Loss: 0.00022141427325550467\n",
      "Iteration 1145/2000, Loss: 0.00033251085551455617\n",
      "Iteration 1146/2000, Loss: 0.00022669923782814294\n",
      "Iteration 1147/2000, Loss: 0.0002982054138556123\n",
      "Iteration 1148/2000, Loss: 0.000215644744457677\n",
      "Iteration 1149/2000, Loss: 0.000133111811010167\n",
      "Iteration 1150/2000, Loss: 0.0001950002188095823\n",
      "Iteration 1151/2000, Loss: 0.0004257572290953249\n",
      "Iteration 1152/2000, Loss: 0.00015807923045940697\n",
      "Iteration 1153/2000, Loss: 0.00024596464936621487\n",
      "Iteration 1154/2000, Loss: 0.00014883311814628541\n",
      "Iteration 1155/2000, Loss: 0.0013470100238919258\n",
      "Iteration 1156/2000, Loss: 0.00021613226272165775\n",
      "Iteration 1157/2000, Loss: 0.00020035043417010456\n",
      "Iteration 1158/2000, Loss: 0.00035075057530775666\n",
      "Iteration 1159/2000, Loss: 0.00036277962499298155\n",
      "Iteration 1160/2000, Loss: 0.0002532368525862694\n",
      "Iteration 1161/2000, Loss: 0.00019532788428477943\n",
      "Iteration 1162/2000, Loss: 0.0002462737902533263\n",
      "Iteration 1163/2000, Loss: 0.0002867410075850785\n",
      "Iteration 1164/2000, Loss: 0.00014079657557886094\n",
      "Iteration 1165/2000, Loss: 0.0002439618547214195\n",
      "Iteration 1166/2000, Loss: 0.00019206841534469277\n",
      "Iteration 1167/2000, Loss: 0.00019353380776010454\n",
      "Iteration 1168/2000, Loss: 0.00027579496963880956\n",
      "Iteration 1169/2000, Loss: 0.0001950523437699303\n",
      "Iteration 1170/2000, Loss: 0.0001813645358197391\n",
      "Iteration 1171/2000, Loss: 0.0001867328683147207\n",
      "Iteration 1172/2000, Loss: 0.00036343312240205705\n",
      "Iteration 1173/2000, Loss: 0.00018102377362083644\n",
      "Iteration 1174/2000, Loss: 0.00013740274880547076\n",
      "Iteration 1175/2000, Loss: 0.00016213624621741474\n",
      "Iteration 1176/2000, Loss: 0.00022144817921798676\n",
      "Iteration 1177/2000, Loss: 0.0001541721576359123\n",
      "Iteration 1178/2000, Loss: 0.0001903936208691448\n",
      "Iteration 1179/2000, Loss: 0.00020857513300143182\n",
      "Iteration 1180/2000, Loss: 0.0001944729738170281\n",
      "Iteration 1181/2000, Loss: 0.0002971711219288409\n",
      "Iteration 1182/2000, Loss: 0.00013705961464438587\n",
      "Iteration 1183/2000, Loss: 0.00021914152603130788\n",
      "Iteration 1184/2000, Loss: 0.0002075598167721182\n",
      "Iteration 1185/2000, Loss: 0.00019366540072951466\n",
      "Iteration 1186/2000, Loss: 0.00018424636800773442\n",
      "Iteration 1187/2000, Loss: 0.0001579845993546769\n",
      "Iteration 1188/2000, Loss: 0.00017275002028327435\n",
      "Iteration 1189/2000, Loss: 0.00011107278987765312\n",
      "Iteration 1190/2000, Loss: 0.00028906206716783345\n",
      "Iteration 1191/2000, Loss: 0.0001587581937201321\n",
      "Iteration 1192/2000, Loss: 0.00018991343677043915\n",
      "Iteration 1193/2000, Loss: 0.00016286014579236507\n",
      "Iteration 1194/2000, Loss: 0.00013366315397433937\n",
      "Iteration 1195/2000, Loss: 0.00018377437663730234\n",
      "Iteration 1196/2000, Loss: 0.00016052351566031575\n",
      "Iteration 1197/2000, Loss: 0.00010633967031026259\n",
      "Iteration 1198/2000, Loss: 0.00015988154336810112\n",
      "Iteration 1199/2000, Loss: 0.00017492867482360452\n",
      "Iteration 1200/2000, Loss: 0.0001397131272824481\n",
      "Iteration 1201/2000, Loss: 0.0001614264474483207\n",
      "Iteration 1202/2000, Loss: 0.00014393910532817245\n",
      "Iteration 1203/2000, Loss: 0.00016277535178232938\n",
      "Iteration 1204/2000, Loss: 0.00018774734053295106\n",
      "Iteration 1205/2000, Loss: 0.00021000187552999705\n",
      "Iteration 1206/2000, Loss: 0.00019468872051220387\n",
      "Iteration 1207/2000, Loss: 0.00035891568404622376\n",
      "Iteration 1208/2000, Loss: 0.00019667968444991857\n",
      "Iteration 1209/2000, Loss: 0.0001536007912363857\n",
      "Iteration 1210/2000, Loss: 0.0005644281627610326\n",
      "Iteration 1211/2000, Loss: 0.00015327274741139263\n",
      "Iteration 1212/2000, Loss: 0.0001349771482637152\n",
      "Iteration 1213/2000, Loss: 0.00022117275511845946\n",
      "Iteration 1214/2000, Loss: 0.00015709515719208866\n",
      "Iteration 1215/2000, Loss: 0.00010052013385575265\n",
      "Iteration 1216/2000, Loss: 0.0004906479152850807\n",
      "Iteration 1217/2000, Loss: 0.00015184906078502536\n",
      "Iteration 1218/2000, Loss: 0.00022125561372376978\n",
      "Iteration 1219/2000, Loss: 0.0002341175131732598\n",
      "Iteration 1220/2000, Loss: 0.00013242474233265966\n",
      "Iteration 1221/2000, Loss: 0.00030139292357489467\n",
      "Iteration 1222/2000, Loss: 0.0001695754035608843\n",
      "Iteration 1223/2000, Loss: 0.0001628353347769007\n",
      "Iteration 1224/2000, Loss: 0.00016341386071871966\n",
      "Iteration 1225/2000, Loss: 0.00013307588233146816\n",
      "Iteration 1226/2000, Loss: 0.00017176943947561085\n",
      "Iteration 1227/2000, Loss: 0.000175940222106874\n",
      "Iteration 1228/2000, Loss: 0.00011829012510133907\n",
      "Iteration 1229/2000, Loss: 0.00012078302825102583\n",
      "Iteration 1230/2000, Loss: 0.00015346615691669285\n",
      "Iteration 1231/2000, Loss: 0.0001243788719875738\n",
      "Iteration 1232/2000, Loss: 0.00011817407357739285\n",
      "Iteration 1233/2000, Loss: 0.00015838856052141637\n",
      "Iteration 1234/2000, Loss: 0.00017012159514706582\n",
      "Iteration 1235/2000, Loss: 0.0001261792640434578\n",
      "Iteration 1236/2000, Loss: 0.0001734206744004041\n",
      "Iteration 1237/2000, Loss: 0.00014263586490415037\n",
      "Iteration 1238/2000, Loss: 0.00014283331984188408\n",
      "Iteration 1239/2000, Loss: 0.00011832089512608945\n",
      "Iteration 1240/2000, Loss: 0.00020046114514116198\n",
      "Iteration 1241/2000, Loss: 0.0001697023690212518\n",
      "Iteration 1242/2000, Loss: 0.00018961320165544748\n",
      "Iteration 1243/2000, Loss: 0.00012775263166986406\n",
      "Iteration 1244/2000, Loss: 0.00012697170313913375\n",
      "Iteration 1245/2000, Loss: 0.00013146332639735192\n",
      "Iteration 1246/2000, Loss: 0.0001295311376452446\n",
      "Iteration 1247/2000, Loss: 0.000188115329365246\n",
      "Iteration 1248/2000, Loss: 0.0004894874873571098\n",
      "Iteration 1249/2000, Loss: 0.00016331448568962514\n",
      "Iteration 1250/2000, Loss: 0.000355816213414073\n",
      "Iteration 1251/2000, Loss: 0.0002720905758906156\n",
      "Iteration 1252/2000, Loss: 0.00016574334586039186\n",
      "Iteration 1253/2000, Loss: 0.0002359862846788019\n",
      "Iteration 1254/2000, Loss: 0.0003590109699871391\n",
      "Iteration 1255/2000, Loss: 0.00011654147238004953\n",
      "Iteration 1256/2000, Loss: 0.0001614695502212271\n",
      "Iteration 1257/2000, Loss: 0.00013142774696461856\n",
      "Iteration 1258/2000, Loss: 0.00016771424270700663\n",
      "Iteration 1259/2000, Loss: 0.00016615296772215515\n",
      "Iteration 1260/2000, Loss: 0.00012995243014302105\n",
      "Iteration 1261/2000, Loss: 0.00023031607270240784\n",
      "Iteration 1262/2000, Loss: 0.00012500879529397935\n",
      "Iteration 1263/2000, Loss: 0.0003088803787250072\n",
      "Iteration 1264/2000, Loss: 0.0002490026818122715\n",
      "Iteration 1265/2000, Loss: 0.00022233696654438972\n",
      "Iteration 1266/2000, Loss: 0.0003383094444870949\n",
      "Iteration 1267/2000, Loss: 0.00011211646778974682\n",
      "Iteration 1268/2000, Loss: 0.0003570422704797238\n",
      "Iteration 1269/2000, Loss: 0.00017730210674926639\n",
      "Iteration 1270/2000, Loss: 0.00025325894239358604\n",
      "Iteration 1271/2000, Loss: 0.0005961960996501148\n",
      "Iteration 1272/2000, Loss: 0.00015048994100652635\n",
      "Iteration 1273/2000, Loss: 0.00040449853986501694\n",
      "Iteration 1274/2000, Loss: 0.00017369062697980553\n",
      "Iteration 1275/2000, Loss: 0.0003039892762899399\n",
      "Iteration 1276/2000, Loss: 0.000490916077978909\n",
      "Iteration 1277/2000, Loss: 9.214586316375062e-05\n",
      "Iteration 1278/2000, Loss: 0.0003428329073358327\n",
      "Iteration 1279/2000, Loss: 0.00019395301933400333\n",
      "Iteration 1280/2000, Loss: 0.0002734181471168995\n",
      "Iteration 1281/2000, Loss: 0.00022621158859692514\n",
      "Iteration 1282/2000, Loss: 0.00013078782649245113\n",
      "Iteration 1283/2000, Loss: 0.000299044098937884\n",
      "Iteration 1284/2000, Loss: 0.00012538411829154938\n",
      "Iteration 1285/2000, Loss: 0.00020407095144037157\n",
      "Iteration 1286/2000, Loss: 0.00018270286091137677\n",
      "Iteration 1287/2000, Loss: 0.0002164248435292393\n",
      "Iteration 1288/2000, Loss: 0.0003063702897634357\n",
      "Iteration 1289/2000, Loss: 0.00010975717304972932\n",
      "Iteration 1290/2000, Loss: 0.00022685894509777427\n",
      "Iteration 1291/2000, Loss: 0.0003975500585511327\n",
      "Iteration 1292/2000, Loss: 0.0004001818597316742\n",
      "Iteration 1293/2000, Loss: 0.0004463148070499301\n",
      "Iteration 1294/2000, Loss: 0.0002278600150020793\n",
      "Iteration 1295/2000, Loss: 0.00010331559315090999\n",
      "Iteration 1296/2000, Loss: 0.0001684387243585661\n",
      "Iteration 1297/2000, Loss: 0.00020553049398586154\n",
      "Iteration 1298/2000, Loss: 0.00025102883228100836\n",
      "Iteration 1299/2000, Loss: 0.0001840204349718988\n",
      "Iteration 1300/2000, Loss: 0.0001455084275221452\n",
      "Iteration 1301/2000, Loss: 0.0002651381364557892\n",
      "Iteration 1302/2000, Loss: 0.00011968190665356815\n",
      "Iteration 1303/2000, Loss: 0.00038666476029902697\n",
      "Iteration 1304/2000, Loss: 0.00010977463534800336\n",
      "Iteration 1305/2000, Loss: 0.0002135266549885273\n",
      "Iteration 1306/2000, Loss: 0.0001866535603767261\n",
      "Iteration 1307/2000, Loss: 0.0002768213744275272\n",
      "Iteration 1308/2000, Loss: 0.00025919327163137496\n",
      "Iteration 1309/2000, Loss: 0.00014321430353447795\n",
      "Iteration 1310/2000, Loss: 0.00012484772014431655\n",
      "Iteration 1311/2000, Loss: 0.00016892209532670677\n",
      "Iteration 1312/2000, Loss: 0.0002690787077881396\n",
      "Iteration 1313/2000, Loss: 0.00016055922606028616\n",
      "Iteration 1314/2000, Loss: 0.0001463264925405383\n",
      "Iteration 1315/2000, Loss: 0.0001515610929345712\n",
      "Iteration 1316/2000, Loss: 0.00021248254051897675\n",
      "Iteration 1317/2000, Loss: 0.00024211798154283315\n",
      "Iteration 1318/2000, Loss: 0.00017447126447223127\n",
      "Iteration 1319/2000, Loss: 0.0002760938659776002\n",
      "Iteration 1320/2000, Loss: 0.00012154514843132347\n",
      "Iteration 1321/2000, Loss: 0.00018986951909027994\n",
      "Iteration 1322/2000, Loss: 0.00013785733608528972\n",
      "Iteration 1323/2000, Loss: 0.00028521273634396493\n",
      "Iteration 1324/2000, Loss: 0.00021211175771895796\n",
      "Iteration 1325/2000, Loss: 0.00018650184210855514\n",
      "Iteration 1326/2000, Loss: 0.00029898673528805375\n",
      "Iteration 1327/2000, Loss: 0.0001362268376396969\n",
      "Iteration 1328/2000, Loss: 0.00019590849115047604\n",
      "Iteration 1329/2000, Loss: 0.00013982976088300347\n",
      "Iteration 1330/2000, Loss: 0.00019418618467170745\n",
      "Iteration 1331/2000, Loss: 0.0002717415045481175\n",
      "Iteration 1332/2000, Loss: 0.00016681768465787172\n",
      "Iteration 1333/2000, Loss: 0.0004259698907844722\n",
      "Iteration 1334/2000, Loss: 0.00018107143114320934\n",
      "Iteration 1335/2000, Loss: 0.00012272587628103793\n",
      "Iteration 1336/2000, Loss: 0.0001560382661409676\n",
      "Iteration 1337/2000, Loss: 0.000519425084348768\n",
      "Iteration 1338/2000, Loss: 0.00025809305952861905\n",
      "Iteration 1339/2000, Loss: 0.00017538080282974988\n",
      "Iteration 1340/2000, Loss: 0.00014690171519760042\n",
      "Iteration 1341/2000, Loss: 0.00017596221005078405\n",
      "Iteration 1342/2000, Loss: 0.0001861077471403405\n",
      "Iteration 1343/2000, Loss: 0.00019726550090126693\n",
      "Iteration 1344/2000, Loss: 0.0001837526506278664\n",
      "Iteration 1345/2000, Loss: 0.0001801277103368193\n",
      "Iteration 1346/2000, Loss: 0.00012717567733488977\n",
      "Iteration 1347/2000, Loss: 0.0002566102775745094\n",
      "Iteration 1348/2000, Loss: 0.00020500487880781293\n",
      "Iteration 1349/2000, Loss: 0.00019535851606633514\n",
      "Iteration 1350/2000, Loss: 0.0001607229351066053\n",
      "Iteration 1351/2000, Loss: 0.00011301873018965125\n",
      "Iteration 1352/2000, Loss: 0.0002119975833920762\n",
      "Iteration 1353/2000, Loss: 0.00014003316755406559\n",
      "Iteration 1354/2000, Loss: 0.00025445796200074255\n",
      "Iteration 1355/2000, Loss: 0.00022004351194482297\n",
      "Iteration 1356/2000, Loss: 0.00010803138138726354\n",
      "Iteration 1357/2000, Loss: 0.0002048766182269901\n",
      "Iteration 1358/2000, Loss: 0.0001937858760356903\n",
      "Iteration 1359/2000, Loss: 0.00021126499632373452\n",
      "Iteration 1360/2000, Loss: 0.00013496172323357314\n",
      "Iteration 1361/2000, Loss: 0.0001948753633769229\n",
      "Iteration 1362/2000, Loss: 0.0002693234127946198\n",
      "Iteration 1363/2000, Loss: 0.0003126614319626242\n",
      "Iteration 1364/2000, Loss: 0.00011299981997581199\n",
      "Iteration 1365/2000, Loss: 0.00018506425840314478\n",
      "Iteration 1366/2000, Loss: 0.000167464604601264\n",
      "Iteration 1367/2000, Loss: 0.00013074181333649904\n",
      "Iteration 1368/2000, Loss: 0.00030075982795096934\n",
      "Iteration 1369/2000, Loss: 0.0005072443746030331\n",
      "Iteration 1370/2000, Loss: 0.0003943484334740788\n",
      "Iteration 1371/2000, Loss: 0.00016054935986176133\n",
      "Iteration 1372/2000, Loss: 0.00015563893248327076\n",
      "Iteration 1373/2000, Loss: 0.0003036212583538145\n",
      "Iteration 1374/2000, Loss: 0.00017293255950789899\n",
      "Iteration 1375/2000, Loss: 0.00021458735864143819\n",
      "Iteration 1376/2000, Loss: 0.00015209840785246342\n",
      "Iteration 1377/2000, Loss: 0.00025363184977322817\n",
      "Iteration 1378/2000, Loss: 0.0002273066493216902\n",
      "Iteration 1379/2000, Loss: 0.00016098193009383976\n",
      "Iteration 1380/2000, Loss: 0.0008402614621445537\n",
      "Iteration 1381/2000, Loss: 0.00021705045946873724\n",
      "Iteration 1382/2000, Loss: 0.00025439527234993875\n",
      "Iteration 1383/2000, Loss: 0.0001779919839464128\n",
      "Iteration 1384/2000, Loss: 0.0002468848251737654\n",
      "Iteration 1385/2000, Loss: 0.00021560823370236903\n",
      "Iteration 1386/2000, Loss: 0.0006013544625602663\n",
      "Iteration 1387/2000, Loss: 0.00017360178753733635\n",
      "Iteration 1388/2000, Loss: 0.00038421936915256083\n",
      "Iteration 1389/2000, Loss: 0.0009465618059039116\n",
      "Iteration 1390/2000, Loss: 0.00013119191862642765\n",
      "Iteration 1391/2000, Loss: 0.00018310871382709593\n",
      "Iteration 1392/2000, Loss: 0.00018911984807346016\n",
      "Iteration 1393/2000, Loss: 0.0002606503840070218\n",
      "Iteration 1394/2000, Loss: 0.00032392676803283393\n",
      "Iteration 1395/2000, Loss: 0.00025465546059422195\n",
      "Iteration 1396/2000, Loss: 0.0002122061705449596\n",
      "Iteration 1397/2000, Loss: 0.00013823814515490085\n",
      "Iteration 1398/2000, Loss: 0.0004239854752086103\n",
      "Iteration 1399/2000, Loss: 0.0002492509374860674\n",
      "Iteration 1400/2000, Loss: 0.000240930647123605\n",
      "Iteration 1401/2000, Loss: 0.000171548206708394\n",
      "Iteration 1402/2000, Loss: 0.00034947003587149084\n",
      "Iteration 1403/2000, Loss: 0.00012883084127679467\n",
      "Iteration 1404/2000, Loss: 0.00019522437651176006\n",
      "Iteration 1405/2000, Loss: 0.00015042370068840683\n",
      "Iteration 1406/2000, Loss: 0.0004292528028599918\n",
      "Iteration 1407/2000, Loss: 0.00020124425645917654\n",
      "Iteration 1408/2000, Loss: 0.0001586081925779581\n",
      "Iteration 1409/2000, Loss: 0.00022381376766134053\n",
      "Iteration 1410/2000, Loss: 0.00014460917736869305\n",
      "Iteration 1411/2000, Loss: 0.0002256712905364111\n",
      "Iteration 1412/2000, Loss: 0.00019875190628226846\n",
      "Iteration 1413/2000, Loss: 0.0002492820203769952\n",
      "Iteration 1414/2000, Loss: 0.00029576433007605374\n",
      "Iteration 1415/2000, Loss: 0.000178332717041485\n",
      "Iteration 1416/2000, Loss: 0.00023823986703064293\n",
      "Iteration 1417/2000, Loss: 0.00018324272241443396\n",
      "Iteration 1418/2000, Loss: 0.00014098623069003224\n",
      "Iteration 1419/2000, Loss: 0.00018533412367105484\n",
      "Iteration 1420/2000, Loss: 0.0001792112016119063\n",
      "Iteration 1421/2000, Loss: 0.001035751891322434\n",
      "Iteration 1422/2000, Loss: 0.00021844076400157064\n",
      "Iteration 1423/2000, Loss: 0.0004664383304771036\n",
      "Iteration 1424/2000, Loss: 0.00022325704048853368\n",
      "Iteration 1425/2000, Loss: 0.00023185400641523302\n",
      "Iteration 1426/2000, Loss: 0.00025173116591759026\n",
      "Iteration 1427/2000, Loss: 0.00019824999617412686\n",
      "Iteration 1428/2000, Loss: 0.00017903803382068872\n",
      "Iteration 1429/2000, Loss: 0.0002891381736844778\n",
      "Iteration 1430/2000, Loss: 0.00020608541672118008\n",
      "Iteration 1431/2000, Loss: 0.00014192135131452233\n",
      "Iteration 1432/2000, Loss: 0.00034490309190005064\n",
      "Iteration 1433/2000, Loss: 0.0001789237285265699\n",
      "Iteration 1434/2000, Loss: 0.00018966339121107012\n",
      "Iteration 1435/2000, Loss: 0.00016258243704214692\n",
      "Iteration 1436/2000, Loss: 0.00018072368402499706\n",
      "Iteration 1437/2000, Loss: 0.0004511323058977723\n",
      "Iteration 1438/2000, Loss: 0.0003074336564168334\n",
      "Iteration 1439/2000, Loss: 0.00023827533004805446\n",
      "Iteration 1440/2000, Loss: 9.967210644390434e-05\n",
      "Iteration 1441/2000, Loss: 0.00019289874762762338\n",
      "Iteration 1442/2000, Loss: 0.00014204111357685179\n",
      "Iteration 1443/2000, Loss: 0.00015844452718738467\n",
      "Iteration 1444/2000, Loss: 0.00014397841005120426\n",
      "Iteration 1445/2000, Loss: 0.00023230828810483217\n",
      "Iteration 1446/2000, Loss: 0.0003286016872152686\n",
      "Iteration 1447/2000, Loss: 0.00014209974324330688\n",
      "Iteration 1448/2000, Loss: 0.00020810356363654137\n",
      "Iteration 1449/2000, Loss: 0.00024559840676374733\n",
      "Iteration 1450/2000, Loss: 0.0001438898325432092\n",
      "Iteration 1451/2000, Loss: 0.00021821408881805837\n",
      "Iteration 1452/2000, Loss: 0.0003473644610494375\n",
      "Iteration 1453/2000, Loss: 0.00026345904916524887\n",
      "Iteration 1454/2000, Loss: 0.00023779072216711938\n",
      "Iteration 1455/2000, Loss: 0.00030125907505862415\n",
      "Iteration 1456/2000, Loss: 0.00023163006699178368\n",
      "Iteration 1457/2000, Loss: 0.00020521532860584557\n",
      "Iteration 1458/2000, Loss: 0.00023159899865277112\n",
      "Iteration 1459/2000, Loss: 0.00015637917385902256\n",
      "Iteration 1460/2000, Loss: 0.00033033706131391227\n",
      "Iteration 1461/2000, Loss: 0.00027326346025802195\n",
      "Iteration 1462/2000, Loss: 0.0003226179105695337\n",
      "Iteration 1463/2000, Loss: 0.00040836248081177473\n",
      "Iteration 1464/2000, Loss: 0.00017232976097147912\n",
      "Iteration 1465/2000, Loss: 0.0006501251482404768\n",
      "Iteration 1466/2000, Loss: 0.0001639779075048864\n",
      "Iteration 1467/2000, Loss: 0.00032311424729414284\n",
      "Iteration 1468/2000, Loss: 0.00035079577355645597\n",
      "Iteration 1469/2000, Loss: 0.00024085673794616014\n",
      "Iteration 1470/2000, Loss: 0.0004594063793774694\n",
      "Iteration 1471/2000, Loss: 0.00018302829994354397\n",
      "Iteration 1472/2000, Loss: 0.00026062593678943813\n",
      "Iteration 1473/2000, Loss: 0.00030191068071871996\n",
      "Iteration 1474/2000, Loss: 0.00017026426212396473\n",
      "Iteration 1475/2000, Loss: 0.0003414859820622951\n",
      "Iteration 1476/2000, Loss: 0.0002954198280349374\n",
      "Iteration 1477/2000, Loss: 0.0002650861570145935\n",
      "Iteration 1478/2000, Loss: 0.00022612929751630872\n",
      "Iteration 1479/2000, Loss: 0.00019894412253051996\n",
      "Iteration 1480/2000, Loss: 0.0003434298268985003\n",
      "Iteration 1481/2000, Loss: 0.0003298935480415821\n",
      "Iteration 1482/2000, Loss: 0.00046929073869250715\n",
      "Iteration 1483/2000, Loss: 0.0002468374732416123\n",
      "Iteration 1484/2000, Loss: 0.00027116655837744474\n",
      "Iteration 1485/2000, Loss: 0.0004047211550641805\n",
      "Iteration 1486/2000, Loss: 0.00018678548804018646\n",
      "Iteration 1487/2000, Loss: 0.0003021167649421841\n",
      "Iteration 1488/2000, Loss: 0.0002362606319366023\n",
      "Iteration 1489/2000, Loss: 0.00038342937477864325\n",
      "Iteration 1490/2000, Loss: 0.00020786772074643523\n",
      "Iteration 1491/2000, Loss: 0.00020006146223749965\n",
      "Iteration 1492/2000, Loss: 0.0004211177001707256\n",
      "Iteration 1493/2000, Loss: 0.0002165028126910329\n",
      "Iteration 1494/2000, Loss: 0.00023825254174880683\n",
      "Iteration 1495/2000, Loss: 0.00024265670799650252\n",
      "Iteration 1496/2000, Loss: 0.00024072571250144392\n",
      "Iteration 1497/2000, Loss: 0.00022298803378362209\n",
      "Iteration 1498/2000, Loss: 0.0003677271888591349\n",
      "Iteration 1499/2000, Loss: 0.00010340289736632258\n",
      "Iteration 1500/2000, Loss: 0.00032125841244123876\n",
      "Iteration 1501/2000, Loss: 0.00022720375272911042\n",
      "Iteration 1502/2000, Loss: 0.00022414256818592548\n",
      "Iteration 1503/2000, Loss: 0.00030549848452210426\n",
      "Iteration 1504/2000, Loss: 0.00019926423556171358\n",
      "Iteration 1505/2000, Loss: 0.000473933934699744\n",
      "Iteration 1506/2000, Loss: 0.0002530681958887726\n",
      "Iteration 1507/2000, Loss: 0.00024323373509105295\n",
      "Iteration 1508/2000, Loss: 0.00044393903226591647\n",
      "Iteration 1509/2000, Loss: 0.00010355582344345748\n",
      "Iteration 1510/2000, Loss: 0.0003558313474059105\n",
      "Iteration 1511/2000, Loss: 0.00022933831496629864\n",
      "Iteration 1512/2000, Loss: 0.0004285119066480547\n",
      "Iteration 1513/2000, Loss: 0.00026819974300451577\n",
      "Iteration 1514/2000, Loss: 0.00020255663548596203\n",
      "Iteration 1515/2000, Loss: 0.00029475046903826296\n",
      "Iteration 1516/2000, Loss: 0.000252943835221231\n",
      "Iteration 1517/2000, Loss: 0.0005118162371218204\n",
      "Iteration 1518/2000, Loss: 0.00019675206567626446\n",
      "Iteration 1519/2000, Loss: 0.0006447004852816463\n",
      "Iteration 1520/2000, Loss: 0.0002751785796135664\n",
      "Iteration 1521/2000, Loss: 0.0002932455681730062\n",
      "Iteration 1522/2000, Loss: 0.00048625803901813924\n",
      "Iteration 1523/2000, Loss: 0.00016216265794355422\n",
      "Iteration 1524/2000, Loss: 0.000646317726932466\n",
      "Iteration 1525/2000, Loss: 0.00015201566566247493\n",
      "Iteration 1526/2000, Loss: 0.00035119373933412135\n",
      "Iteration 1527/2000, Loss: 0.00031793126254342496\n",
      "Iteration 1528/2000, Loss: 0.00037506193621084094\n",
      "Iteration 1529/2000, Loss: 0.0004958655335940421\n",
      "Iteration 1530/2000, Loss: 0.00014259629824664444\n",
      "Iteration 1531/2000, Loss: 0.00037934540887363255\n",
      "Iteration 1532/2000, Loss: 0.00022310180065687746\n",
      "Iteration 1533/2000, Loss: 0.0004446457023732364\n",
      "Iteration 1534/2000, Loss: 0.0001907586702145636\n",
      "Iteration 1535/2000, Loss: 0.0003087116638198495\n",
      "Iteration 1536/2000, Loss: 0.00018012645887210965\n",
      "Iteration 1537/2000, Loss: 0.00032516138162463903\n",
      "Iteration 1538/2000, Loss: 0.0003099911264143884\n",
      "Iteration 1539/2000, Loss: 0.0002679819881450385\n",
      "Iteration 1540/2000, Loss: 0.0003425161703489721\n",
      "Iteration 1541/2000, Loss: 0.0001345503842458129\n",
      "Iteration 1542/2000, Loss: 0.00024336116621270776\n",
      "Iteration 1543/2000, Loss: 0.0002058634563582018\n",
      "Iteration 1544/2000, Loss: 0.00023647279886063188\n",
      "Iteration 1545/2000, Loss: 0.00020410808792803437\n",
      "Iteration 1546/2000, Loss: 0.00026045861886814237\n",
      "Iteration 1547/2000, Loss: 0.00024195780861191452\n",
      "Iteration 1548/2000, Loss: 0.00023075082572177052\n",
      "Iteration 1549/2000, Loss: 0.00048700065235607326\n",
      "Iteration 1550/2000, Loss: 0.0001611965853953734\n",
      "Iteration 1551/2000, Loss: 0.000477213179692626\n",
      "Iteration 1552/2000, Loss: 0.00023289222735911608\n",
      "Iteration 1553/2000, Loss: 0.0003172194992657751\n",
      "Iteration 1554/2000, Loss: 0.0003446048067416996\n",
      "Iteration 1555/2000, Loss: 0.00017006369307637215\n",
      "Iteration 1556/2000, Loss: 0.00023745684302411973\n",
      "Iteration 1557/2000, Loss: 0.00024449502234347165\n",
      "Iteration 1558/2000, Loss: 0.00023368356050923467\n",
      "Iteration 1559/2000, Loss: 0.0002046175068244338\n",
      "Iteration 1560/2000, Loss: 0.0003259534714743495\n",
      "Iteration 1561/2000, Loss: 0.00019457584130577743\n",
      "Iteration 1562/2000, Loss: 0.0002658444282133132\n",
      "Iteration 1563/2000, Loss: 0.0002707570674829185\n",
      "Iteration 1564/2000, Loss: 0.00020560965640470386\n",
      "Iteration 1565/2000, Loss: 0.0002287412207806483\n",
      "Iteration 1566/2000, Loss: 0.00022946267563384026\n",
      "Iteration 1567/2000, Loss: 0.0003330324252601713\n",
      "Iteration 1568/2000, Loss: 0.00019775940745603293\n",
      "Iteration 1569/2000, Loss: 0.0003546732768882066\n",
      "Iteration 1570/2000, Loss: 0.0002486845478415489\n",
      "Iteration 1571/2000, Loss: 0.00016592464817222208\n",
      "Iteration 1572/2000, Loss: 0.000270959542831406\n",
      "Iteration 1573/2000, Loss: 0.0002159939322154969\n",
      "Iteration 1574/2000, Loss: 0.00022334074310492724\n",
      "Iteration 1575/2000, Loss: 0.00032569895847700536\n",
      "Iteration 1576/2000, Loss: 0.00024235543969552964\n",
      "Iteration 1577/2000, Loss: 0.0002465774887241423\n",
      "Iteration 1578/2000, Loss: 0.00017141389253083616\n",
      "Iteration 1579/2000, Loss: 0.0005204603658057749\n",
      "Iteration 1580/2000, Loss: 0.00011392206943128258\n",
      "Iteration 1581/2000, Loss: 0.00016796319687273353\n",
      "Iteration 1582/2000, Loss: 0.00017498183296993375\n",
      "Iteration 1583/2000, Loss: 0.000180487273610197\n",
      "Iteration 1584/2000, Loss: 0.00023017439525574446\n",
      "Iteration 1585/2000, Loss: 0.00011291661212453619\n",
      "Iteration 1586/2000, Loss: 0.00020363093062769622\n",
      "Iteration 1587/2000, Loss: 0.0002805572876241058\n",
      "Iteration 1588/2000, Loss: 0.00019717375107575208\n",
      "Iteration 1589/2000, Loss: 0.00018566517974250019\n",
      "Iteration 1590/2000, Loss: 0.00019484836957417428\n",
      "Iteration 1591/2000, Loss: 0.00012743879051413387\n",
      "Iteration 1592/2000, Loss: 0.00012907896598335356\n",
      "Iteration 1593/2000, Loss: 0.00020292405679356307\n",
      "Iteration 1594/2000, Loss: 0.00036540644941851497\n",
      "Iteration 1595/2000, Loss: 0.00015341448306571692\n",
      "Iteration 1596/2000, Loss: 0.00023056077770888805\n",
      "Iteration 1597/2000, Loss: 0.00013140254304744303\n",
      "Iteration 1598/2000, Loss: 0.00014422340609598905\n",
      "Iteration 1599/2000, Loss: 0.0001899101334856823\n",
      "Iteration 1600/2000, Loss: 0.0003840424178633839\n",
      "Iteration 1601/2000, Loss: 0.0001782309846021235\n",
      "Iteration 1602/2000, Loss: 0.0002148203202523291\n",
      "Iteration 1603/2000, Loss: 0.00019502524810377508\n",
      "Iteration 1604/2000, Loss: 0.00015563369379378855\n",
      "Iteration 1605/2000, Loss: 0.0002894670469686389\n",
      "Iteration 1606/2000, Loss: 0.0001902057119878009\n",
      "Iteration 1607/2000, Loss: 0.00018594296125229448\n",
      "Iteration 1608/2000, Loss: 0.00019598493236117065\n",
      "Iteration 1609/2000, Loss: 0.00019315769895911217\n",
      "Iteration 1610/2000, Loss: 0.00012543084449134767\n",
      "Iteration 1611/2000, Loss: 0.00019689867622219026\n",
      "Iteration 1612/2000, Loss: 0.00015753135085105896\n",
      "Iteration 1613/2000, Loss: 0.00015132591943256557\n",
      "Iteration 1614/2000, Loss: 0.00011673278640955687\n",
      "Iteration 1615/2000, Loss: 8.850648737279698e-05\n",
      "Iteration 1616/2000, Loss: 0.00018474302487447858\n",
      "Iteration 1617/2000, Loss: 0.00015503256872761995\n",
      "Iteration 1618/2000, Loss: 0.00023032452736515552\n",
      "Iteration 1619/2000, Loss: 0.00026818038895726204\n",
      "Iteration 1620/2000, Loss: 0.0003538302844390273\n",
      "Iteration 1621/2000, Loss: 0.000503028801176697\n",
      "Iteration 1622/2000, Loss: 0.00038182124262675643\n",
      "Iteration 1623/2000, Loss: 0.00015621290367562324\n",
      "Iteration 1624/2000, Loss: 0.00015958416042849422\n",
      "Iteration 1625/2000, Loss: 0.00015653454465791583\n",
      "Iteration 1626/2000, Loss: 0.00018545318744145334\n",
      "Iteration 1627/2000, Loss: 0.00024490407668054104\n",
      "Iteration 1628/2000, Loss: 0.00018397886015009135\n",
      "Iteration 1629/2000, Loss: 0.00028370239306241274\n",
      "Iteration 1630/2000, Loss: 0.00015918849385343492\n",
      "Iteration 1631/2000, Loss: 0.00015069589426275343\n",
      "Iteration 1632/2000, Loss: 0.0002725609519984573\n",
      "Iteration 1633/2000, Loss: 0.00021304338588379323\n",
      "Iteration 1634/2000, Loss: 0.00025660006213001907\n",
      "Iteration 1635/2000, Loss: 0.00013937705080024898\n",
      "Iteration 1636/2000, Loss: 0.00025771124637685716\n",
      "Iteration 1637/2000, Loss: 0.0006863056914880872\n",
      "Iteration 1638/2000, Loss: 0.0001870661071734503\n",
      "Iteration 1639/2000, Loss: 0.00033134783734567463\n",
      "Iteration 1640/2000, Loss: 0.00021626829402521253\n",
      "Iteration 1641/2000, Loss: 0.0002427649451419711\n",
      "Iteration 1642/2000, Loss: 0.0001555322523927316\n",
      "Iteration 1643/2000, Loss: 0.00024819528334774077\n",
      "Iteration 1644/2000, Loss: 0.00013283337466418743\n",
      "Iteration 1645/2000, Loss: 0.0007208210881799459\n",
      "Iteration 1646/2000, Loss: 0.00014353045844472945\n",
      "Iteration 1647/2000, Loss: 0.0005786087713204324\n",
      "Iteration 1648/2000, Loss: 0.00016270916967187077\n",
      "Iteration 1649/2000, Loss: 0.00015724725381005555\n",
      "Iteration 1650/2000, Loss: 0.00018021484720520675\n",
      "Iteration 1651/2000, Loss: 0.00015654179151169956\n",
      "Iteration 1652/2000, Loss: 0.0002892865159083158\n",
      "Iteration 1653/2000, Loss: 0.00013481572386808693\n",
      "Iteration 1654/2000, Loss: 0.0002452536718919873\n",
      "Iteration 1655/2000, Loss: 0.00016566895646974444\n",
      "Iteration 1656/2000, Loss: 0.00041175016667693853\n",
      "Iteration 1657/2000, Loss: 0.00023479449737351388\n",
      "Iteration 1658/2000, Loss: 0.00015312249888665974\n",
      "Iteration 1659/2000, Loss: 0.00013183458941057324\n",
      "Iteration 1660/2000, Loss: 0.00020697727450169623\n",
      "Iteration 1661/2000, Loss: 0.00016795798728708178\n",
      "Iteration 1662/2000, Loss: 0.00028358050622045994\n",
      "Iteration 1663/2000, Loss: 0.00015413052460644394\n",
      "Iteration 1664/2000, Loss: 0.00018230271234642714\n",
      "Iteration 1665/2000, Loss: 0.00016611606406513602\n",
      "Iteration 1666/2000, Loss: 0.00015763136616442353\n",
      "Iteration 1667/2000, Loss: 0.0002619686129037291\n",
      "Iteration 1668/2000, Loss: 0.0002379462239332497\n",
      "Iteration 1669/2000, Loss: 0.00022019846073817462\n",
      "Iteration 1670/2000, Loss: 0.000279782572761178\n",
      "Iteration 1671/2000, Loss: 0.00030101355514489114\n",
      "Iteration 1672/2000, Loss: 0.0001337887515546754\n",
      "Iteration 1673/2000, Loss: 0.0002796334447339177\n",
      "Iteration 1674/2000, Loss: 0.0004656935343518853\n",
      "Iteration 1675/2000, Loss: 0.00017834392201621085\n",
      "Iteration 1676/2000, Loss: 0.000270646414719522\n",
      "Iteration 1677/2000, Loss: 0.00015079630247782916\n",
      "Iteration 1678/2000, Loss: 0.0004978434299118817\n",
      "Iteration 1679/2000, Loss: 0.0002432290348224342\n",
      "Iteration 1680/2000, Loss: 0.00026377421454526484\n",
      "Iteration 1681/2000, Loss: 0.00044849334517493844\n",
      "Iteration 1682/2000, Loss: 0.0007788882940076292\n",
      "Iteration 1683/2000, Loss: 0.0002851801400538534\n",
      "Iteration 1684/2000, Loss: 0.0004500132636167109\n",
      "Iteration 1685/2000, Loss: 0.0004790499806404114\n",
      "Iteration 1686/2000, Loss: 0.0002343666274100542\n",
      "Iteration 1687/2000, Loss: 0.00024114956613630056\n",
      "Iteration 1688/2000, Loss: 0.0003868417697958648\n",
      "Iteration 1689/2000, Loss: 0.00032527875737287104\n",
      "Iteration 1690/2000, Loss: 0.00022080630878917873\n",
      "Iteration 1691/2000, Loss: 0.00021210995328146964\n",
      "Iteration 1692/2000, Loss: 0.00015771418111398816\n",
      "Iteration 1693/2000, Loss: 0.00020425939874257892\n",
      "Iteration 1694/2000, Loss: 0.00015291583258658648\n",
      "Iteration 1695/2000, Loss: 0.0003169250558130443\n",
      "Iteration 1696/2000, Loss: 0.00021336032659746706\n",
      "Iteration 1697/2000, Loss: 0.0003370228805579245\n",
      "Iteration 1698/2000, Loss: 0.00028766097966581583\n",
      "Iteration 1699/2000, Loss: 0.00019765383331105113\n",
      "Iteration 1700/2000, Loss: 0.0003168457478750497\n",
      "Iteration 1701/2000, Loss: 0.00022791056835558265\n",
      "Iteration 1702/2000, Loss: 0.0002072947972919792\n",
      "Iteration 1703/2000, Loss: 0.00018873349472414702\n",
      "Iteration 1704/2000, Loss: 0.00016054208390414715\n",
      "Iteration 1705/2000, Loss: 0.000193000742001459\n",
      "Iteration 1706/2000, Loss: 0.00010400432074675336\n",
      "Iteration 1707/2000, Loss: 0.00014575237582903355\n",
      "Iteration 1708/2000, Loss: 0.00036269775591790676\n",
      "Iteration 1709/2000, Loss: 0.0001800604077288881\n",
      "Iteration 1710/2000, Loss: 0.00018446697504259646\n",
      "Iteration 1711/2000, Loss: 0.00018322895630262792\n",
      "Iteration 1712/2000, Loss: 0.00010526641563046724\n",
      "Iteration 1713/2000, Loss: 0.00014594478125218302\n",
      "Iteration 1714/2000, Loss: 0.0002599841682240367\n",
      "Iteration 1715/2000, Loss: 0.00010845828364836052\n",
      "Iteration 1716/2000, Loss: 0.00014226188068278134\n",
      "Iteration 1717/2000, Loss: 0.0001653866347623989\n",
      "Iteration 1718/2000, Loss: 0.00015534958220086992\n",
      "Iteration 1719/2000, Loss: 0.00013202040281612426\n",
      "Iteration 1720/2000, Loss: 0.00016882042109500617\n",
      "Iteration 1721/2000, Loss: 0.00020175160898361355\n",
      "Iteration 1722/2000, Loss: 0.00016305592725984752\n",
      "Iteration 1723/2000, Loss: 0.0001730418880470097\n",
      "Iteration 1724/2000, Loss: 0.0002047020388999954\n",
      "Iteration 1725/2000, Loss: 0.00014357244072016329\n",
      "Iteration 1726/2000, Loss: 0.00039279877091757953\n",
      "Iteration 1727/2000, Loss: 0.00017361741629429162\n",
      "Iteration 1728/2000, Loss: 0.000366891355952248\n",
      "Iteration 1729/2000, Loss: 0.00019828483345918357\n",
      "Iteration 1730/2000, Loss: 0.00033595479908399284\n",
      "Iteration 1731/2000, Loss: 0.00021079805446788669\n",
      "Iteration 1732/2000, Loss: 0.0002491423801984638\n",
      "Iteration 1733/2000, Loss: 0.00029897165950387716\n",
      "Iteration 1734/2000, Loss: 0.00018872761575039476\n",
      "Iteration 1735/2000, Loss: 0.0001493734889663756\n",
      "Iteration 1736/2000, Loss: 0.000327649584505707\n",
      "Iteration 1737/2000, Loss: 0.00012593674182426184\n",
      "Iteration 1738/2000, Loss: 0.00015329952293541282\n",
      "Iteration 1739/2000, Loss: 0.00019792259263340384\n",
      "Iteration 1740/2000, Loss: 0.00019418849842622876\n",
      "Iteration 1741/2000, Loss: 0.00011727712262654677\n",
      "Iteration 1742/2000, Loss: 0.00017064429994206876\n",
      "Iteration 1743/2000, Loss: 0.0001931581209646538\n",
      "Iteration 1744/2000, Loss: 0.00014801029465161264\n",
      "Iteration 1745/2000, Loss: 0.00020068018056917936\n",
      "Iteration 1746/2000, Loss: 0.00025504970108158886\n",
      "Iteration 1747/2000, Loss: 0.00039598086732439697\n",
      "Iteration 1748/2000, Loss: 0.00015563896158710122\n",
      "Iteration 1749/2000, Loss: 0.00013735861284658313\n",
      "Iteration 1750/2000, Loss: 0.0002372969320276752\n",
      "Iteration 1751/2000, Loss: 0.0002561854780651629\n",
      "Iteration 1752/2000, Loss: 0.00023940493701957166\n",
      "Iteration 1753/2000, Loss: 0.00017335492884740233\n",
      "Iteration 1754/2000, Loss: 0.0001862664648797363\n",
      "Iteration 1755/2000, Loss: 0.000377373507944867\n",
      "Iteration 1756/2000, Loss: 0.00020588579354807734\n",
      "Iteration 1757/2000, Loss: 0.00018248158448841423\n",
      "Iteration 1758/2000, Loss: 0.0003611751599237323\n",
      "Iteration 1759/2000, Loss: 0.00017285000649280846\n",
      "Iteration 1760/2000, Loss: 0.00018097854626830667\n",
      "Iteration 1761/2000, Loss: 0.00032116190413944423\n",
      "Iteration 1762/2000, Loss: 0.00010937311890302226\n",
      "Iteration 1763/2000, Loss: 0.000672890164423734\n",
      "Iteration 1764/2000, Loss: 0.0002963879960589111\n",
      "Iteration 1765/2000, Loss: 0.00017300713807344437\n",
      "Iteration 1766/2000, Loss: 0.0002842231187969446\n",
      "Iteration 1767/2000, Loss: 0.00015575491124764085\n",
      "Iteration 1768/2000, Loss: 0.0001678515982348472\n",
      "Iteration 1769/2000, Loss: 0.0001469739800086245\n",
      "Iteration 1770/2000, Loss: 0.0003480748273432255\n",
      "Iteration 1771/2000, Loss: 0.00016929265984799713\n",
      "Iteration 1772/2000, Loss: 0.0002361796796321869\n",
      "Iteration 1773/2000, Loss: 0.00024358573136851192\n",
      "Iteration 1774/2000, Loss: 0.00014405837282538414\n",
      "Iteration 1775/2000, Loss: 0.0003812063077930361\n",
      "Iteration 1776/2000, Loss: 0.00013423252676147968\n",
      "Iteration 1777/2000, Loss: 0.00027816553483717144\n",
      "Iteration 1778/2000, Loss: 0.00027411096380092204\n",
      "Iteration 1779/2000, Loss: 0.0007490696152672172\n",
      "Iteration 1780/2000, Loss: 0.00020933191990479827\n",
      "Iteration 1781/2000, Loss: 0.00022098679619375616\n",
      "Iteration 1782/2000, Loss: 0.00025468377862125635\n",
      "Iteration 1783/2000, Loss: 0.00012576922017615288\n",
      "Iteration 1784/2000, Loss: 0.00018257142801303416\n",
      "Iteration 1785/2000, Loss: 0.00019016060105059296\n",
      "Iteration 1786/2000, Loss: 0.000253128819167614\n",
      "Iteration 1787/2000, Loss: 0.00017097282398026437\n",
      "Iteration 1788/2000, Loss: 0.00016099192725960165\n",
      "Iteration 1789/2000, Loss: 0.00018159198225475848\n",
      "Iteration 1790/2000, Loss: 0.0003688449796754867\n",
      "Iteration 1791/2000, Loss: 0.00013956488692201674\n",
      "Iteration 1792/2000, Loss: 0.00015820295084267855\n",
      "Iteration 1793/2000, Loss: 0.0001961881062015891\n",
      "Iteration 1794/2000, Loss: 0.0003067395300604403\n",
      "Iteration 1795/2000, Loss: 0.0001215937954839319\n",
      "Iteration 1796/2000, Loss: 0.00014224763435777277\n",
      "Iteration 1797/2000, Loss: 0.0001924384559970349\n",
      "Iteration 1798/2000, Loss: 0.00019938881450798362\n",
      "Iteration 1799/2000, Loss: 0.00020662863971665502\n",
      "Iteration 1800/2000, Loss: 0.0003888268256559968\n",
      "Iteration 1801/2000, Loss: 0.0001302832824876532\n",
      "Iteration 1802/2000, Loss: 0.00012414339289534837\n",
      "Iteration 1803/2000, Loss: 0.00021188869141042233\n",
      "Iteration 1804/2000, Loss: 0.0002804890973493457\n",
      "Iteration 1805/2000, Loss: 0.00020842108642682433\n",
      "Iteration 1806/2000, Loss: 0.0003846038307528943\n",
      "Iteration 1807/2000, Loss: 0.0003804869775194675\n",
      "Iteration 1808/2000, Loss: 0.0005601374432444572\n",
      "Iteration 1809/2000, Loss: 0.00046281309914775193\n",
      "Iteration 1810/2000, Loss: 0.0001248955522896722\n",
      "Iteration 1811/2000, Loss: 0.0004887934192083776\n",
      "Iteration 1812/2000, Loss: 0.00025384913897141814\n",
      "Iteration 1813/2000, Loss: 0.0001844187790993601\n",
      "Iteration 1814/2000, Loss: 0.00028012777329422534\n",
      "Iteration 1815/2000, Loss: 0.00010824494529515505\n",
      "Iteration 1816/2000, Loss: 0.0002481413830537349\n",
      "Iteration 1817/2000, Loss: 0.00013224607391748577\n",
      "Iteration 1818/2000, Loss: 0.0007029061089269817\n",
      "Iteration 1819/2000, Loss: 0.00022385218471754342\n",
      "Iteration 1820/2000, Loss: 0.00020666737691499293\n",
      "Iteration 1821/2000, Loss: 0.00023689579393249005\n",
      "Iteration 1822/2000, Loss: 0.00014279417518991977\n",
      "Iteration 1823/2000, Loss: 0.0002979758137371391\n",
      "Iteration 1824/2000, Loss: 0.0002608432259876281\n",
      "Iteration 1825/2000, Loss: 0.00015082793834153563\n",
      "Iteration 1826/2000, Loss: 0.00031377034611068666\n",
      "Iteration 1827/2000, Loss: 0.00011316912423353642\n",
      "Iteration 1828/2000, Loss: 0.00026726789656095207\n",
      "Iteration 1829/2000, Loss: 0.0002447566366754472\n",
      "Iteration 1830/2000, Loss: 0.00010340123117202893\n",
      "Iteration 1831/2000, Loss: 0.00018785451538860798\n",
      "Iteration 1832/2000, Loss: 0.00020708245574496686\n",
      "Iteration 1833/2000, Loss: 0.00017164139717351645\n",
      "Iteration 1834/2000, Loss: 0.00015736749628558755\n",
      "Iteration 1835/2000, Loss: 0.00015378031821455806\n",
      "Iteration 1836/2000, Loss: 0.00014849762374069542\n",
      "Iteration 1837/2000, Loss: 0.00014931605255696923\n",
      "Iteration 1838/2000, Loss: 0.00014008961443323642\n",
      "Iteration 1839/2000, Loss: 0.00022550419089384377\n",
      "Iteration 1840/2000, Loss: 0.00017913186457008123\n",
      "Iteration 1841/2000, Loss: 0.0003355811059009284\n",
      "Iteration 1842/2000, Loss: 0.00013770145596936345\n",
      "Iteration 1843/2000, Loss: 0.00018705838010646403\n",
      "Iteration 1844/2000, Loss: 0.0001275011309189722\n",
      "Iteration 1845/2000, Loss: 0.0002210392995039001\n",
      "Iteration 1846/2000, Loss: 0.0001500430516898632\n",
      "Iteration 1847/2000, Loss: 0.000148685576277785\n",
      "Iteration 1848/2000, Loss: 0.0002376986521994695\n",
      "Iteration 1849/2000, Loss: 0.00022854653070680797\n",
      "Iteration 1850/2000, Loss: 0.00020829295681323856\n",
      "Iteration 1851/2000, Loss: 0.00015600942424498498\n",
      "Iteration 1852/2000, Loss: 0.00022295732924249023\n",
      "Iteration 1853/2000, Loss: 0.00017217922140844166\n",
      "Iteration 1854/2000, Loss: 0.00011436185741331428\n",
      "Iteration 1855/2000, Loss: 0.00019237014930695295\n",
      "Iteration 1856/2000, Loss: 0.00023200623400043696\n",
      "Iteration 1857/2000, Loss: 0.00018782757979352027\n",
      "Iteration 1858/2000, Loss: 0.00017476231732871383\n",
      "Iteration 1859/2000, Loss: 0.0002622208558022976\n",
      "Iteration 1860/2000, Loss: 0.0002051942137768492\n",
      "Iteration 1861/2000, Loss: 0.00018040301802102476\n",
      "Iteration 1862/2000, Loss: 0.00018438964616507292\n",
      "Iteration 1863/2000, Loss: 0.0002203437325078994\n",
      "Iteration 1864/2000, Loss: 0.00016568427963647991\n",
      "Iteration 1865/2000, Loss: 0.00023529167810920626\n",
      "Iteration 1866/2000, Loss: 0.00016552332090213895\n",
      "Iteration 1867/2000, Loss: 0.00029717633151449263\n",
      "Iteration 1868/2000, Loss: 0.00019775769033003598\n",
      "Iteration 1869/2000, Loss: 0.0002110079221893102\n",
      "Iteration 1870/2000, Loss: 0.00019535550381988287\n",
      "Iteration 1871/2000, Loss: 0.00019464695651549846\n",
      "Iteration 1872/2000, Loss: 0.00030179883469827473\n",
      "Iteration 1873/2000, Loss: 0.00023285570205189288\n",
      "Iteration 1874/2000, Loss: 0.00024140877940226346\n",
      "Iteration 1875/2000, Loss: 0.00018163870845455676\n",
      "Iteration 1876/2000, Loss: 0.00028225884307175875\n",
      "Iteration 1877/2000, Loss: 0.000138803428853862\n",
      "Iteration 1878/2000, Loss: 0.0002682521881069988\n",
      "Iteration 1879/2000, Loss: 0.00018597900634631515\n",
      "Iteration 1880/2000, Loss: 0.00022081795032136142\n",
      "Iteration 1881/2000, Loss: 0.0003200844512321055\n",
      "Iteration 1882/2000, Loss: 0.00023786463134456426\n",
      "Iteration 1883/2000, Loss: 0.00034488417441025376\n",
      "Iteration 1884/2000, Loss: 0.000406251143431291\n",
      "Iteration 1885/2000, Loss: 0.00023922235413920134\n",
      "Iteration 1886/2000, Loss: 0.0003295590286143124\n",
      "Iteration 1887/2000, Loss: 0.0003308198938611895\n",
      "Iteration 1888/2000, Loss: 0.00013500772183761\n",
      "Iteration 1889/2000, Loss: 0.0005351369618438184\n",
      "Iteration 1890/2000, Loss: 0.00021480541909113526\n",
      "Iteration 1891/2000, Loss: 0.00020194011449348181\n",
      "Iteration 1892/2000, Loss: 0.00020442233653739095\n",
      "Iteration 1893/2000, Loss: 0.0002736335445661098\n",
      "Iteration 1894/2000, Loss: 0.00014170704525895417\n",
      "Iteration 1895/2000, Loss: 0.00014689350791741163\n",
      "Iteration 1896/2000, Loss: 0.00021917105186730623\n",
      "Iteration 1897/2000, Loss: 0.0002438938827253878\n",
      "Iteration 1898/2000, Loss: 0.00014377161278389394\n",
      "Iteration 1899/2000, Loss: 0.00024973240215331316\n",
      "Iteration 1900/2000, Loss: 0.0001615781948203221\n",
      "Iteration 1901/2000, Loss: 0.0003080948954448104\n",
      "Iteration 1902/2000, Loss: 0.00030686112586408854\n",
      "Iteration 1903/2000, Loss: 0.00024254478921648115\n",
      "Iteration 1904/2000, Loss: 0.00038159111863933504\n",
      "Iteration 1905/2000, Loss: 0.00013473119179252535\n",
      "Iteration 1906/2000, Loss: 0.00020891742315143347\n",
      "Iteration 1907/2000, Loss: 0.00011459930101409554\n",
      "Iteration 1908/2000, Loss: 0.00016918897745199502\n",
      "Iteration 1909/2000, Loss: 0.00014626930351369083\n",
      "Iteration 1910/2000, Loss: 0.0002689227112568915\n",
      "Iteration 1911/2000, Loss: 0.00030555386911146343\n",
      "Iteration 1912/2000, Loss: 0.0001827429950935766\n",
      "Iteration 1913/2000, Loss: 0.00011495294893393293\n",
      "Iteration 1914/2000, Loss: 0.00016335058899130672\n",
      "Iteration 1915/2000, Loss: 0.00021382937848102301\n",
      "Iteration 1916/2000, Loss: 0.0001745374029269442\n",
      "Iteration 1917/2000, Loss: 0.00023234394029714167\n",
      "Iteration 1918/2000, Loss: 0.00017935024516191334\n",
      "Iteration 1919/2000, Loss: 0.0007034615846350789\n",
      "Iteration 1920/2000, Loss: 0.00019177995272912085\n",
      "Iteration 1921/2000, Loss: 0.00019163230899721384\n",
      "Iteration 1922/2000, Loss: 0.00019600750238168985\n",
      "Iteration 1923/2000, Loss: 0.00023450069420505315\n",
      "Iteration 1924/2000, Loss: 0.00022390257799997926\n",
      "Iteration 1925/2000, Loss: 0.0001411714038113132\n",
      "Iteration 1926/2000, Loss: 0.00017621464212425053\n",
      "Iteration 1927/2000, Loss: 0.0003805972228292376\n",
      "Iteration 1928/2000, Loss: 0.0003190833958797157\n",
      "Iteration 1929/2000, Loss: 0.00023676517594140023\n",
      "Iteration 1930/2000, Loss: 0.00025820755399763584\n",
      "Iteration 1931/2000, Loss: 0.00015741074457764626\n",
      "Iteration 1932/2000, Loss: 0.0002262024354422465\n",
      "Iteration 1933/2000, Loss: 0.00015115199494175613\n",
      "Iteration 1934/2000, Loss: 0.00017794854647945613\n",
      "Iteration 1935/2000, Loss: 0.0001949143479578197\n",
      "Iteration 1936/2000, Loss: 0.00016491091810166836\n",
      "Iteration 1937/2000, Loss: 0.0002537011750973761\n",
      "Iteration 1938/2000, Loss: 0.0001557688956381753\n",
      "Iteration 1939/2000, Loss: 0.0002638164150994271\n",
      "Iteration 1940/2000, Loss: 0.00015669144340790808\n",
      "Iteration 1941/2000, Loss: 0.0001164195200544782\n",
      "Iteration 1942/2000, Loss: 0.00037894840352237225\n",
      "Iteration 1943/2000, Loss: 0.00018638958863448352\n",
      "Iteration 1944/2000, Loss: 0.0003154902660753578\n",
      "Iteration 1945/2000, Loss: 0.00017757050227373838\n",
      "Iteration 1946/2000, Loss: 0.00013901929196435958\n",
      "Iteration 1947/2000, Loss: 0.0002454087371006608\n",
      "Iteration 1948/2000, Loss: 0.00022374119726009667\n",
      "Iteration 1949/2000, Loss: 0.00020541205594781786\n",
      "Iteration 1950/2000, Loss: 0.0001950588048202917\n",
      "Iteration 1951/2000, Loss: 0.00028078301693312824\n",
      "Iteration 1952/2000, Loss: 0.00029807467944920063\n",
      "Iteration 1953/2000, Loss: 0.0001451915450161323\n",
      "Iteration 1954/2000, Loss: 0.00017751450650393963\n",
      "Iteration 1955/2000, Loss: 0.00021874434605706483\n",
      "Iteration 1956/2000, Loss: 0.00020460209634620696\n",
      "Iteration 1957/2000, Loss: 0.00024162809131667018\n",
      "Iteration 1958/2000, Loss: 0.0001480107894167304\n",
      "Iteration 1959/2000, Loss: 0.00017845985712483525\n",
      "Iteration 1960/2000, Loss: 0.00023870506265666336\n",
      "Iteration 1961/2000, Loss: 0.00015406173770315945\n",
      "Iteration 1962/2000, Loss: 0.00023453471658285707\n",
      "Iteration 1963/2000, Loss: 0.00030892458744347095\n",
      "Iteration 1964/2000, Loss: 0.0002643764892127365\n",
      "Iteration 1965/2000, Loss: 0.00042366093839518726\n",
      "Iteration 1966/2000, Loss: 0.0001776631106622517\n",
      "Iteration 1967/2000, Loss: 0.0002534438972361386\n",
      "Iteration 1968/2000, Loss: 0.0004192403284832835\n",
      "Iteration 1969/2000, Loss: 0.0004851600097026676\n",
      "Iteration 1970/2000, Loss: 0.0002470877079758793\n",
      "Iteration 1971/2000, Loss: 0.00028613294125534594\n",
      "Iteration 1972/2000, Loss: 0.0006490082014352083\n",
      "Iteration 1973/2000, Loss: 0.0003073924162890762\n",
      "Iteration 1974/2000, Loss: 0.000513496866915375\n",
      "Iteration 1975/2000, Loss: 0.0001467602705815807\n",
      "Iteration 1976/2000, Loss: 0.0004988489090465009\n",
      "Iteration 1977/2000, Loss: 0.00025484152138233185\n",
      "Iteration 1978/2000, Loss: 0.0002864697598852217\n",
      "Iteration 1979/2000, Loss: 0.000310788513161242\n",
      "Iteration 1980/2000, Loss: 0.00018224948144052178\n",
      "Iteration 1981/2000, Loss: 0.00036887999158352613\n",
      "Iteration 1982/2000, Loss: 0.0003080792957916856\n",
      "Iteration 1983/2000, Loss: 0.0002113030495820567\n",
      "Iteration 1984/2000, Loss: 0.00021364985150285065\n",
      "Iteration 1985/2000, Loss: 0.0002160239964723587\n",
      "Iteration 1986/2000, Loss: 0.00021532316168304533\n",
      "Iteration 1987/2000, Loss: 0.0001325033517787233\n",
      "Iteration 1988/2000, Loss: 0.00024327440769411623\n",
      "Iteration 1989/2000, Loss: 0.00028012823895551264\n",
      "Iteration 1990/2000, Loss: 0.0002652271359693259\n",
      "Iteration 1991/2000, Loss: 0.00015688994608353823\n",
      "Iteration 1992/2000, Loss: 0.0002749464474618435\n",
      "Iteration 1993/2000, Loss: 0.000214200874324888\n",
      "Iteration 1994/2000, Loss: 0.0001473806332796812\n",
      "Iteration 1995/2000, Loss: 0.00011567072942852974\n",
      "Iteration 1996/2000, Loss: 0.00023225744371302426\n",
      "Iteration 1997/2000, Loss: 0.0001001752752927132\n",
      "Iteration 1998/2000, Loss: 0.00016276801761705428\n",
      "Iteration 1999/2000, Loss: 0.00013632535410579294\n",
      "Iteration 2000/2000, Loss: 0.00037136286846362054\n",
      "Model weights saved after training and testing with linewidth 200000.0 Hz.\n",
      "\n",
      "\n",
      "Testing with linewidth: 200000.0 Hz and Distance: 2000.0 km\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Testing MSE - Linewidth: 200000.0, Link Distance: 2000.0, Original: 0.005361279938369989, Neural Network: 0.008571930229663849\n",
      "\n",
      "Training with linewidth: 200000.0 Hz and Distance: 4000.0 km\n",
      "Iteration 1/2000, Loss: 0.0002436168142594397\n",
      "Iteration 2/2000, Loss: 0.00013005040818825364\n",
      "Iteration 3/2000, Loss: 0.00018373304919805378\n",
      "Iteration 4/2000, Loss: 0.00010932541044894606\n",
      "Iteration 5/2000, Loss: 0.0001385761279379949\n",
      "Iteration 6/2000, Loss: 0.00030823747511021793\n",
      "Iteration 7/2000, Loss: 0.0002057521342067048\n",
      "Iteration 8/2000, Loss: 0.00018214200099464506\n",
      "Iteration 9/2000, Loss: 0.00026847576373256743\n",
      "Iteration 10/2000, Loss: 0.00017313768330495805\n",
      "Iteration 11/2000, Loss: 0.0002583146560937166\n",
      "Iteration 12/2000, Loss: 0.00028100088820792735\n",
      "Iteration 13/2000, Loss: 0.00018106159404851496\n",
      "Iteration 14/2000, Loss: 0.00025881262263283134\n",
      "Iteration 15/2000, Loss: 0.00015558140876237303\n",
      "Iteration 16/2000, Loss: 0.00022706229356117547\n",
      "Iteration 17/2000, Loss: 0.0002754713059403002\n",
      "Iteration 18/2000, Loss: 0.0012053720420226455\n",
      "Iteration 19/2000, Loss: 0.00028222528635524213\n",
      "Iteration 20/2000, Loss: 0.0002914315846282989\n",
      "Iteration 21/2000, Loss: 0.00024014707014430314\n",
      "Iteration 22/2000, Loss: 0.000616776873357594\n",
      "Iteration 23/2000, Loss: 0.0012730236630886793\n",
      "Iteration 24/2000, Loss: 0.0004035296442452818\n",
      "Iteration 25/2000, Loss: 0.0008807191625237465\n",
      "Iteration 26/2000, Loss: 0.0005191801465116441\n",
      "Iteration 27/2000, Loss: 0.0005141611327417195\n",
      "Iteration 28/2000, Loss: 0.0007085819961503148\n",
      "Iteration 29/2000, Loss: 0.00047351320972666144\n",
      "Iteration 30/2000, Loss: 0.0006624875823035836\n",
      "Iteration 31/2000, Loss: 0.0002982053847517818\n",
      "Iteration 32/2000, Loss: 0.0006246569682843983\n",
      "Iteration 33/2000, Loss: 0.0011278513120487332\n",
      "Iteration 34/2000, Loss: 0.0005158991552889347\n",
      "Iteration 35/2000, Loss: 0.00015219014312606305\n",
      "Iteration 36/2000, Loss: 0.0005699152825400233\n",
      "Iteration 37/2000, Loss: 0.00019636373326648027\n",
      "Iteration 38/2000, Loss: 0.0004957228666171432\n",
      "Iteration 39/2000, Loss: 0.00032574491342529655\n",
      "Iteration 40/2000, Loss: 0.00020486766879912466\n",
      "Iteration 41/2000, Loss: 0.00025069850380532444\n",
      "Iteration 42/2000, Loss: 0.00021853136422578245\n",
      "Iteration 43/2000, Loss: 0.0002633616968523711\n",
      "Iteration 44/2000, Loss: 0.00021255519823171198\n",
      "Iteration 45/2000, Loss: 0.00027777798823080957\n",
      "Iteration 46/2000, Loss: 0.00024535536067560315\n",
      "Iteration 47/2000, Loss: 0.00028251492767594755\n",
      "Iteration 48/2000, Loss: 0.00013525583199225366\n",
      "Iteration 49/2000, Loss: 0.00027968076756224036\n",
      "Iteration 50/2000, Loss: 0.00015688726853113621\n",
      "Iteration 51/2000, Loss: 0.00021845188166480511\n",
      "Iteration 52/2000, Loss: 0.00023594913363922387\n",
      "Iteration 53/2000, Loss: 0.00042577256681397557\n",
      "Iteration 54/2000, Loss: 0.00017403936362825334\n",
      "Iteration 55/2000, Loss: 0.0003356500237714499\n",
      "Iteration 56/2000, Loss: 0.00019232454360462725\n",
      "Iteration 57/2000, Loss: 0.0005340709467418492\n",
      "Iteration 58/2000, Loss: 0.00024460506392642856\n",
      "Iteration 59/2000, Loss: 0.00034630941809155047\n",
      "Iteration 60/2000, Loss: 0.00025462015764787793\n",
      "Iteration 61/2000, Loss: 0.0004377896257210523\n",
      "Iteration 62/2000, Loss: 0.0002023439883487299\n",
      "Iteration 63/2000, Loss: 0.00040329352486878633\n",
      "Iteration 64/2000, Loss: 0.0002330984134459868\n",
      "Iteration 65/2000, Loss: 0.000268626055913046\n",
      "Iteration 66/2000, Loss: 0.00030261464416980743\n",
      "Iteration 67/2000, Loss: 0.0002026632137130946\n",
      "Iteration 68/2000, Loss: 0.00021114369155839086\n",
      "Iteration 69/2000, Loss: 0.00029022249509580433\n",
      "Iteration 70/2000, Loss: 0.0005206319037824869\n",
      "Iteration 71/2000, Loss: 0.0002292777062393725\n",
      "Iteration 72/2000, Loss: 0.0004628337046597153\n",
      "Iteration 73/2000, Loss: 0.0003292242472525686\n",
      "Iteration 74/2000, Loss: 0.0006010941579006612\n",
      "Iteration 75/2000, Loss: 0.00038830051198601723\n",
      "Iteration 76/2000, Loss: 0.0005830744048580527\n",
      "Iteration 77/2000, Loss: 0.0006721352692693472\n",
      "Iteration 78/2000, Loss: 0.0001568877778481692\n",
      "Iteration 79/2000, Loss: 0.0004509440914262086\n",
      "Iteration 80/2000, Loss: 0.00013719085836783051\n",
      "Iteration 81/2000, Loss: 0.0002982961304951459\n",
      "Iteration 82/2000, Loss: 0.00010822463082149625\n",
      "Iteration 83/2000, Loss: 0.0002687437809072435\n",
      "Iteration 84/2000, Loss: 0.00013553741155192256\n",
      "Iteration 85/2000, Loss: 0.00022955007443670183\n",
      "Iteration 86/2000, Loss: 0.00019561336375772953\n",
      "Iteration 87/2000, Loss: 0.00023868604330345988\n",
      "Iteration 88/2000, Loss: 0.000137873794301413\n",
      "Iteration 89/2000, Loss: 0.000331126939272508\n",
      "Iteration 90/2000, Loss: 0.0001634987856959924\n",
      "Iteration 91/2000, Loss: 0.0002994514361489564\n",
      "Iteration 92/2000, Loss: 0.0002894553472287953\n",
      "Iteration 93/2000, Loss: 0.00027036963729187846\n",
      "Iteration 94/2000, Loss: 0.00014434687909670174\n",
      "Iteration 95/2000, Loss: 0.0003435724647715688\n",
      "Iteration 96/2000, Loss: 0.00046781040146015584\n",
      "Iteration 97/2000, Loss: 0.00019372519454918802\n",
      "Iteration 98/2000, Loss: 0.0002924134605564177\n",
      "Iteration 99/2000, Loss: 0.00015585735673084855\n",
      "Iteration 100/2000, Loss: 0.00026952647021971643\n",
      "Iteration 101/2000, Loss: 0.00020301654876675457\n",
      "Iteration 102/2000, Loss: 0.00032456699409522116\n",
      "Iteration 103/2000, Loss: 0.00020964565919712186\n",
      "Iteration 104/2000, Loss: 0.00019089283887296915\n",
      "Iteration 105/2000, Loss: 0.00043934545828960836\n",
      "Iteration 106/2000, Loss: 0.00026451676967553794\n",
      "Iteration 107/2000, Loss: 0.00016461611085105687\n",
      "Iteration 108/2000, Loss: 0.00014249923697207123\n",
      "Iteration 109/2000, Loss: 0.00012867132318206131\n",
      "Iteration 110/2000, Loss: 0.00014450136222876608\n",
      "Iteration 111/2000, Loss: 0.00013187300646677613\n",
      "Iteration 112/2000, Loss: 0.00012287654681131244\n",
      "Iteration 113/2000, Loss: 0.0001478353951824829\n",
      "Iteration 114/2000, Loss: 0.00015966701903380454\n",
      "Iteration 115/2000, Loss: 0.00016525301907677203\n",
      "Iteration 116/2000, Loss: 0.00015830779739189893\n",
      "Iteration 117/2000, Loss: 0.0001729364594211802\n",
      "Iteration 118/2000, Loss: 0.00027422772836871445\n",
      "Iteration 119/2000, Loss: 0.00015556275320705026\n",
      "Iteration 120/2000, Loss: 0.00011025714775314555\n",
      "Iteration 121/2000, Loss: 0.00012091670942027122\n",
      "Iteration 122/2000, Loss: 9.921263699652627e-05\n",
      "Iteration 123/2000, Loss: 0.00013488130935002118\n",
      "Iteration 124/2000, Loss: 0.0001474311575293541\n",
      "Iteration 125/2000, Loss: 9.595684969099239e-05\n",
      "Iteration 126/2000, Loss: 0.00031680765096098185\n",
      "Iteration 127/2000, Loss: 0.00020247678912710398\n",
      "Iteration 128/2000, Loss: 0.00024590076645836234\n",
      "Iteration 129/2000, Loss: 0.00014959732652641833\n",
      "Iteration 130/2000, Loss: 0.00014782407379243523\n",
      "Iteration 131/2000, Loss: 9.915290138451383e-05\n",
      "Iteration 132/2000, Loss: 0.00014228461077436805\n",
      "Iteration 133/2000, Loss: 0.0002960943093057722\n",
      "Iteration 134/2000, Loss: 0.00036359461955726147\n",
      "Iteration 135/2000, Loss: 0.0001595987268956378\n",
      "Iteration 136/2000, Loss: 0.000140265969093889\n",
      "Iteration 137/2000, Loss: 0.00026761446497403085\n",
      "Iteration 138/2000, Loss: 0.00028317951364442706\n",
      "Iteration 139/2000, Loss: 0.0002635603887028992\n",
      "Iteration 140/2000, Loss: 0.0003667269484139979\n",
      "Iteration 141/2000, Loss: 0.00016053352737799287\n",
      "Iteration 142/2000, Loss: 0.0009687235578894615\n",
      "Iteration 143/2000, Loss: 0.00018484570318832994\n",
      "Iteration 144/2000, Loss: 0.00025680652470327914\n",
      "Iteration 145/2000, Loss: 0.00017714213754516095\n",
      "Iteration 146/2000, Loss: 0.0002013186167459935\n",
      "Iteration 147/2000, Loss: 0.00024111468519549817\n",
      "Iteration 148/2000, Loss: 0.00018095340055879205\n",
      "Iteration 149/2000, Loss: 0.000170327301020734\n",
      "Iteration 150/2000, Loss: 0.00021475915855262429\n",
      "Iteration 151/2000, Loss: 0.00018890686624217778\n",
      "Iteration 152/2000, Loss: 0.00016827305080369115\n",
      "Iteration 153/2000, Loss: 0.0002665766514837742\n",
      "Iteration 154/2000, Loss: 0.00019666705338750035\n",
      "Iteration 155/2000, Loss: 0.00019935972522944212\n",
      "Iteration 156/2000, Loss: 0.0004426647210493684\n",
      "Iteration 157/2000, Loss: 0.00019373251416254789\n",
      "Iteration 158/2000, Loss: 0.00042240775655955076\n",
      "Iteration 159/2000, Loss: 0.0001769594382494688\n",
      "Iteration 160/2000, Loss: 0.00019592506578192115\n",
      "Iteration 161/2000, Loss: 0.00015235444880090654\n",
      "Iteration 162/2000, Loss: 0.00017091170593630522\n",
      "Iteration 163/2000, Loss: 0.00021899490093346685\n",
      "Iteration 164/2000, Loss: 0.00012969558883924037\n",
      "Iteration 165/2000, Loss: 0.00021588275558315217\n",
      "Iteration 166/2000, Loss: 0.0002636101853568107\n",
      "Iteration 167/2000, Loss: 0.00023883838730398566\n",
      "Iteration 168/2000, Loss: 0.00018923904281109571\n",
      "Iteration 169/2000, Loss: 0.00024545646738260984\n",
      "Iteration 170/2000, Loss: 0.0001375885185552761\n",
      "Iteration 171/2000, Loss: 0.0004204557917546481\n",
      "Iteration 172/2000, Loss: 0.00019955683092121035\n",
      "Iteration 173/2000, Loss: 0.00023585856251884252\n",
      "Iteration 174/2000, Loss: 0.0003300560056231916\n",
      "Iteration 175/2000, Loss: 0.00016012514242902398\n",
      "Iteration 176/2000, Loss: 0.00039771286537870765\n",
      "Iteration 177/2000, Loss: 0.00021060292783658952\n",
      "Iteration 178/2000, Loss: 0.00033045076997950673\n",
      "Iteration 179/2000, Loss: 0.0001845268561737612\n",
      "Iteration 180/2000, Loss: 0.0002844490809366107\n",
      "Iteration 181/2000, Loss: 0.0002861197281163186\n",
      "Iteration 182/2000, Loss: 0.00033270069980062544\n",
      "Iteration 183/2000, Loss: 0.00029653849196620286\n",
      "Iteration 184/2000, Loss: 0.0002850910241249949\n",
      "Iteration 185/2000, Loss: 0.0003119209432043135\n",
      "Iteration 186/2000, Loss: 0.00018173636635765433\n",
      "Iteration 187/2000, Loss: 0.00016262334247585386\n",
      "Iteration 188/2000, Loss: 0.00023252243408933282\n",
      "Iteration 189/2000, Loss: 0.0001918285561259836\n",
      "Iteration 190/2000, Loss: 0.00024641677737236023\n",
      "Iteration 191/2000, Loss: 0.00028558121994137764\n",
      "Iteration 192/2000, Loss: 0.0003277636133134365\n",
      "Iteration 193/2000, Loss: 0.00032423139782622457\n",
      "Iteration 194/2000, Loss: 0.00015373284986708313\n",
      "Iteration 195/2000, Loss: 0.0004828150849789381\n",
      "Iteration 196/2000, Loss: 0.00018745912529993802\n",
      "Iteration 197/2000, Loss: 0.0002009488525800407\n",
      "Iteration 198/2000, Loss: 0.0001532145543023944\n",
      "Iteration 199/2000, Loss: 0.0002641830069478601\n",
      "Iteration 200/2000, Loss: 0.0002409525914117694\n",
      "Iteration 201/2000, Loss: 0.0002973279042635113\n",
      "Iteration 202/2000, Loss: 0.00011239010927965865\n",
      "Iteration 203/2000, Loss: 0.00022162336972542107\n",
      "Iteration 204/2000, Loss: 0.00020206894259899855\n",
      "Iteration 205/2000, Loss: 0.0003422748704906553\n",
      "Iteration 206/2000, Loss: 0.0005332185537554324\n",
      "Iteration 207/2000, Loss: 0.00021040735009592026\n",
      "Iteration 208/2000, Loss: 0.00023131260240916163\n",
      "Iteration 209/2000, Loss: 0.00024221945204772055\n",
      "Iteration 210/2000, Loss: 0.00026323870406486094\n",
      "Iteration 211/2000, Loss: 0.0003823175502475351\n",
      "Iteration 212/2000, Loss: 0.0001549796579638496\n",
      "Iteration 213/2000, Loss: 0.00030293737654574215\n",
      "Iteration 214/2000, Loss: 0.00030537063139490783\n",
      "Iteration 215/2000, Loss: 0.0009468853240832686\n",
      "Iteration 216/2000, Loss: 0.00030548442737199366\n",
      "Iteration 217/2000, Loss: 0.0002359888021601364\n",
      "Iteration 218/2000, Loss: 0.0005073690554127097\n",
      "Iteration 219/2000, Loss: 0.00019011099357157946\n",
      "Iteration 220/2000, Loss: 0.00023326273367274553\n",
      "Iteration 221/2000, Loss: 0.00016314128879457712\n",
      "Iteration 222/2000, Loss: 0.0001674255181569606\n",
      "Iteration 223/2000, Loss: 0.0001753260294208303\n",
      "Iteration 224/2000, Loss: 0.0002258235908811912\n",
      "Iteration 225/2000, Loss: 0.00022579162032343447\n",
      "Iteration 226/2000, Loss: 0.00020768602553289384\n",
      "Iteration 227/2000, Loss: 0.00031699854298494756\n",
      "Iteration 228/2000, Loss: 0.0002799039357341826\n",
      "Iteration 229/2000, Loss: 0.0002968891058117151\n",
      "Iteration 230/2000, Loss: 0.0002455950598232448\n",
      "Iteration 231/2000, Loss: 0.00028276618104428053\n",
      "Iteration 232/2000, Loss: 0.0004014389996882528\n",
      "Iteration 233/2000, Loss: 0.0002784819225780666\n",
      "Iteration 234/2000, Loss: 0.00019581986998673528\n",
      "Iteration 235/2000, Loss: 0.0004641300765797496\n",
      "Iteration 236/2000, Loss: 0.00013732445950154215\n",
      "Iteration 237/2000, Loss: 0.00025024093338288367\n",
      "Iteration 238/2000, Loss: 0.00027370438328944147\n",
      "Iteration 239/2000, Loss: 0.00022737756080459803\n",
      "Iteration 240/2000, Loss: 0.0005308893159963191\n",
      "Iteration 241/2000, Loss: 0.00015166834054980427\n",
      "Iteration 242/2000, Loss: 0.00016962242079898715\n",
      "Iteration 243/2000, Loss: 0.00011762866779463366\n",
      "Iteration 244/2000, Loss: 0.00015987289953045547\n",
      "Iteration 245/2000, Loss: 9.371762280352414e-05\n",
      "Iteration 246/2000, Loss: 0.00018944461771752685\n",
      "Iteration 247/2000, Loss: 0.00026548709138296545\n",
      "Iteration 248/2000, Loss: 0.0003197645128238946\n",
      "Iteration 249/2000, Loss: 0.00022722828725818545\n",
      "Iteration 250/2000, Loss: 0.0003160728083457798\n",
      "Iteration 251/2000, Loss: 0.0002127316693076864\n",
      "Iteration 252/2000, Loss: 0.0002942713035736233\n",
      "Iteration 253/2000, Loss: 0.00020173702796455473\n",
      "Iteration 254/2000, Loss: 0.0003945829812437296\n",
      "Iteration 255/2000, Loss: 0.00024299319193232805\n",
      "Iteration 256/2000, Loss: 0.00018274328613188118\n",
      "Iteration 257/2000, Loss: 0.00018207043467555195\n",
      "Iteration 258/2000, Loss: 0.00021712761372327805\n",
      "Iteration 259/2000, Loss: 0.0003731571196112782\n",
      "Iteration 260/2000, Loss: 0.00016194740601349622\n",
      "Iteration 261/2000, Loss: 0.00016038015019148588\n",
      "Iteration 262/2000, Loss: 0.00016366741328965873\n",
      "Iteration 263/2000, Loss: 0.000335689983330667\n",
      "Iteration 264/2000, Loss: 0.00020264041086193174\n",
      "Iteration 265/2000, Loss: 0.00028083421057090163\n",
      "Iteration 266/2000, Loss: 0.00019398100266698748\n",
      "Iteration 267/2000, Loss: 0.00018697195628192276\n",
      "Iteration 268/2000, Loss: 0.000152150314534083\n",
      "Iteration 269/2000, Loss: 0.0003773010103031993\n",
      "Iteration 270/2000, Loss: 0.00024613033747300506\n",
      "Iteration 271/2000, Loss: 0.00031845193007029593\n",
      "Iteration 272/2000, Loss: 0.00013302704610396177\n",
      "Iteration 273/2000, Loss: 0.0002716937742661685\n",
      "Iteration 274/2000, Loss: 0.00014915606880094856\n",
      "Iteration 275/2000, Loss: 0.00013095728354528546\n",
      "Iteration 276/2000, Loss: 0.0002345291431993246\n",
      "Iteration 277/2000, Loss: 0.00025039855972863734\n",
      "Iteration 278/2000, Loss: 0.0001456940663047135\n",
      "Iteration 279/2000, Loss: 0.00019845445058308542\n",
      "Iteration 280/2000, Loss: 0.00020019662042614073\n",
      "Iteration 281/2000, Loss: 0.0003028642968274653\n",
      "Iteration 282/2000, Loss: 0.00019878910097759217\n",
      "Iteration 283/2000, Loss: 0.00022722926223650575\n",
      "Iteration 284/2000, Loss: 0.00017530328477732837\n",
      "Iteration 285/2000, Loss: 0.00024771742755547166\n",
      "Iteration 286/2000, Loss: 0.0002460500691086054\n",
      "Iteration 287/2000, Loss: 0.0002404869010206312\n",
      "Iteration 288/2000, Loss: 0.00023461644013877958\n",
      "Iteration 289/2000, Loss: 0.00035674948594532907\n",
      "Iteration 290/2000, Loss: 0.00041268201312050223\n",
      "Iteration 291/2000, Loss: 0.00018958281725645065\n",
      "Iteration 292/2000, Loss: 0.0003988870303146541\n",
      "Iteration 293/2000, Loss: 0.00037364198942668736\n",
      "Iteration 294/2000, Loss: 0.00041341446922160685\n",
      "Iteration 295/2000, Loss: 0.00038216132088564336\n",
      "Iteration 296/2000, Loss: 0.0003366570163052529\n",
      "Iteration 297/2000, Loss: 0.0004040837229695171\n",
      "Iteration 298/2000, Loss: 0.00019957643235102296\n",
      "Iteration 299/2000, Loss: 0.00025647252914495766\n",
      "Iteration 300/2000, Loss: 0.00013873979332856834\n",
      "Iteration 301/2000, Loss: 0.0002952931390609592\n",
      "Iteration 302/2000, Loss: 0.00014757475582882762\n",
      "Iteration 303/2000, Loss: 0.00017201577429659665\n",
      "Iteration 304/2000, Loss: 0.00029494447517208755\n",
      "Iteration 305/2000, Loss: 0.00020690838573500514\n",
      "Iteration 306/2000, Loss: 0.0002114766393788159\n",
      "Iteration 307/2000, Loss: 0.00017572098295204341\n",
      "Iteration 308/2000, Loss: 0.00015827159222681075\n",
      "Iteration 309/2000, Loss: 0.0003192489384673536\n",
      "Iteration 310/2000, Loss: 0.00020423793466761708\n",
      "Iteration 311/2000, Loss: 0.00033297311165370047\n",
      "Iteration 312/2000, Loss: 0.0006468539941124618\n",
      "Iteration 313/2000, Loss: 0.00013322460290510207\n",
      "Iteration 314/2000, Loss: 0.0003067632205784321\n",
      "Iteration 315/2000, Loss: 0.00021203210053499788\n",
      "Iteration 316/2000, Loss: 0.00026250025257468224\n",
      "Iteration 317/2000, Loss: 0.00014219980221241713\n",
      "Iteration 318/2000, Loss: 0.00015405533486045897\n",
      "Iteration 319/2000, Loss: 0.00024136602587532252\n",
      "Iteration 320/2000, Loss: 0.00017547882453072816\n",
      "Iteration 321/2000, Loss: 9.610594861442223e-05\n",
      "Iteration 322/2000, Loss: 0.0003585444937925786\n",
      "Iteration 323/2000, Loss: 0.0002020142273977399\n",
      "Iteration 324/2000, Loss: 0.00015466463810298592\n",
      "Iteration 325/2000, Loss: 0.00012293309555388987\n",
      "Iteration 326/2000, Loss: 0.00010747015767265111\n",
      "Iteration 327/2000, Loss: 0.00021237731561996043\n",
      "Iteration 328/2000, Loss: 0.00013895012671127915\n",
      "Iteration 329/2000, Loss: 0.0001223478466272354\n",
      "Iteration 330/2000, Loss: 0.00014592145453207195\n",
      "Iteration 331/2000, Loss: 0.000226081014261581\n",
      "Iteration 332/2000, Loss: 0.00012220513599459082\n",
      "Iteration 333/2000, Loss: 0.00020179181592538953\n",
      "Iteration 334/2000, Loss: 0.00013364369806367904\n",
      "Iteration 335/2000, Loss: 0.0001762002648320049\n",
      "Iteration 336/2000, Loss: 0.00019462911586742848\n",
      "Iteration 337/2000, Loss: 0.00011700944742187858\n",
      "Iteration 338/2000, Loss: 0.00014017641660757363\n",
      "Iteration 339/2000, Loss: 0.0002608563518151641\n",
      "Iteration 340/2000, Loss: 0.000315999670419842\n",
      "Iteration 341/2000, Loss: 0.0006526568322442472\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Train\")\n",
    "iterations = 2000\n",
    "linewidths = [100e3, 200e3, 300e3, 400e3, 500e3, 750e3, 1000e3]\n",
    "link_distances = [1e3, 2e3, 4e3, 5e3]\n",
    "test_results = pipeline.train_and_test(iterations=iterations, linewidths = linewidths, link_distances=link_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd63a62e-a26c-4fac-a786-88735fb84a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Start Test\")\n",
    "# trained_model = load_model('nn_model')\n",
    "# pipeline.nn_equalise.load_weights('final_model_weights.h5')\n",
    "# original_mse, nn_mse, tx_symbols_arr, rx_symbols_arr, rx_symbols_nn_arr = pipeline.test(trained_model, num_symbols=100)\n",
    "# print(f\"Original MSE: {original_mse.numpy()}, Neural Network MSE: {nn_mse.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebb68b-d267-41d0-ba70-c4e72efea44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_io = PlotInputOutput(rx_symbols_arr, tx_symbols_arr)\n",
    "# plot_io.plot_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ec2ae-977d-4d6a-b0b0-eb476ea9c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_ionn = PlotInputOutput(rx_symbols_nn_arr, tx_symbols_arr)\n",
    "# plot_ionn.plot_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f2c24b-61ea-4d36-b567-d9a5282687c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = SNRVsLinewidthPlotterNN(pipeline, linewidths, link_distances)\n",
    "plotter.plot(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e62ba-a587-4a0e-944e-732a74fe7a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
